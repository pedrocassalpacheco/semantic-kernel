{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOy6oqXF7l3X"
      },
      "source": [
        "# **RAG with Semantic Kernal using Astra DB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WjI3itCpCS"
      },
      "source": [
        "This an example of building a Retrieval Augmented Generation (RAG) application efficiently by configuring Semantic Kernal using Astra DB MemoryStore. You just need to folllow and execute those steps below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2bHZGUWVSqE"
      },
      "source": [
        "### Install libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJEVPMLq1944",
        "outputId": "f54ad942-42fc-4108-f517-e6ab9dedd527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q semantic-kernel\n",
        "%pip install -q ragstack-ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3M7fv6yWZLG"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0RON-Xa2AJp"
      },
      "outputs": [],
      "source": [
        "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
        "from semantic_kernel.prompt_template.input_variable import InputVariable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaPoDGlGW4JJ"
      },
      "source": [
        "### Load ENV Variables\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import dotenv_values\n",
        "env = dotenv_values(\".env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G6RH5I9pWQi"
      },
      "source": [
        "### Initialize Semantic Kernel and Add Chat & Embedding services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "fku-UzEb9vPp",
        "outputId": "947a55c8-0fdc-486e-e57c-3e923be2a824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added OpenAI Chat Service\n",
            "Added OpenAI Embedding Service\n"
          ]
        }
      ],
      "source": [
        "import semantic_kernel as sk\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    OpenAIChatCompletion,\n",
        "    OpenAITextEmbedding\n",
        ")\n",
        "\n",
        "CHAT_COMPLETION_MODEL = \"gpt-3.5-turbo\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Semantic Kernel\n",
        "kernel = sk.Kernel()\n",
        "\n",
        "#A dd OpenAI chat_completion service\n",
        "kernel.add_service(\n",
        "    OpenAIChatCompletion(\n",
        "        service_id=\"completion\",\n",
        "        ai_model_id=CHAT_COMPLETION_MODEL,\n",
        "        api_key=env[\"OPEN_AI_API_KEY\"]\n",
        "    )\n",
        ")\n",
        "print(\"Added OpenAI Chat Service\")\n",
        "\n",
        "# Add OpenAI text_embedding service\n",
        "kernel.add_service(\n",
        "    OpenAITextEmbedding(\n",
        "        service_id=\"embedding\",\n",
        "        ai_model_id=\"text-embedding-ada-002\",\n",
        "        api_key=env[\"OPEN_AI_API_KEY\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Added OpenAI Embedding Service\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVigdyHpxGbL"
      },
      "source": [
        "# **Register Astra DB memory in Semantic kernal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "9U_IJnGpHtPK",
        "outputId": "505e69e3-4cea-487d-f418-b5878748bf7a"
      },
      "outputs": [],
      "source": [
        "from semantic_kernel.connectors.memory.astradb import AstraDBMemoryStore\n",
        "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
        "from semantic_kernel.memory.memory_store_base import MemoryStoreBase\n",
        "from semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n",
        "\n",
        "# Create Astra memory store\n",
        "\n",
        "store = AstraDBMemoryStore(env[\"ASTRA_DB_TOKEN\"], \n",
        "                           env[\"ASTRA_DB_ID\"], \n",
        "                           env[\"ASTRA_REGION\"], \n",
        "                           env[\"KEYSPACE\"], \n",
        "                           int(env[\"EMBEDDING_DIMENSION\"]), \n",
        "                           env[\"SIMILARITY\"])\n",
        "\n",
        "\n",
        "# Register Astra memory in Semantic Kernal Memory\n",
        "memory = SemanticTextMemory(storage=store, embeddings_generator=kernel.get_service(\"embedding\"))\n",
        "kernel.import_plugin_from_object(TextMemoryPlugin(memory), \"TextMemoryPluginACDB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document loading and embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Multiple definitions in dictionary at byte 0x3d9cb for key /Rotate\n",
            "Multiple definitions in dictionary at byte 0x3da82 for key /Rotate\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving vectors to  documents\n",
            "Processing  ./data/2403.06995.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='arXiv:2403.06995v1  [cs.AI]  3 Mar 2024EXACT ALGORITHMS AND HEURISTICS FOR CAPACITATED\\nCOVERING SALESMAN PROBLEMS\\nLucas Porto Maziero, Fábio Luiz Usberti, Celso Cavellucci\\nInstitute of Computing\\nUniversidade Estadual de Campinas\\nCampinas\\n{lucas.maziero, fusberti, celsocv}@ic.unicamp.br\\nABSTRACT\\nThis paper introduces the Capacitated Covering Salesman Pr oblem (CCSP), approaching the notion\\nof service by coverage in capacitated vehicle routing probl ems. In CCSP, locations where vehicles\\ncan transit are provided, some of which have customers with d emands. The objective is to ser-\\nvice customers through a ﬂeet of vehicles based in a depot, mi nimizing the total distance traversed\\nby the vehicles. CCSP is unique in the sense that customers, t o be serviced, do not need to be\\nvisited by a vehicle. Instead, they can be serviced if they ar e within a coverage area of the vehi-\\ncle. This assumption is motivated by applications in which s ome customers are unreachable (e.g.,\\nforbidden access to vehicles) or visiting every customer is impractical. In this work, optimization\\nmethodologies are proposed for the CCSP based on ILP (Intege r Linear Programming) and BRKGA\\n(Biased Random-Key Genetic Algorithm) metaheuristic. Com putational experiments conducted on\\na benchmark of instances for the CCSP evaluate the performan ce of the methodologies with respect\\nto primal bounds. Furthermore, our ILP formulation is exten ded in order to create a novel MILP\\n(Mixed Integer Linear Programming) for the Multi-Depot Cov ering Tour Vehicle Routing Problem\\n(MDCTVRP). Computational experiments show that the extend ed MILP formulation outperformed\\nthe previous state-of-the-art exact approach with respect to optimality gaps. In particular, optimal\\nsolutions were obtained for several previously unsolved in stances.\\nKeywords Covering routing problems ·Integer linear programming ·Metaheuristic·Matheuristic\\n1 Introduction\\nThe Capacitated Vehicle Routing Problem (CVRP), initially proposed by Dantzig and Ramser [1], is one of the most\\nwell-known problems in combinatorial optimization. The go al of CVRP is to service the demands of a set of customers\\nthrough a set of vehicles located in a depot, minimizing the t otal distance travelled. Each vehicle must depart and retur n\\nto the depot, and cannot service more than its capacity [2].\\nThe CVRP encompasses many variants with restrictions of tim e constraints, resources availability, and even customers\\naccessibility, for example, regions of difﬁcult means of en try to vehicles [3, 4]. The latter can be addressed by con-\\nsidering service by covering. A notion by which a customer ca n be serviced remotely as long as the customer is in\\nthe covering range of the vehicle. For example, in Figure 1 cu stomersbanddare within the covering range of cus-\\ntomera. Also, customers aandecan be remotely serviced by vertex c, if there is enough remaining capacity in the\\ncorresponding vehicle.\\nThe ﬁrst problem using the concept of servicing by coverage i s the Covering Salesman Problem (CSP), by Current\\nand Schilling [5], stated as follows. Given an undirected gr aph with cost attributed to the edges, the objective is to\\ndetermine a minimum cost cycle such that every vertex out of t he cycle is covered by at least one vertex in the cycle.\\nThe CSP generalizes the Travelling Salesman Problem (TSP) [ 6] in the case where each vertex only covers itself, from\\nwhich follows that CSP is NP-hard.' metadata={'source': './data/2403.06995.pdf', 'page': 0}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nFigure 1: Example of covering ranges.\\nGeneralizations of the CSP were investigated in literature . Golden et al. [7] proposed a generalization of the CSP in\\nwhich each vertex has a covering demand referring to the numb er of times it must be covered by the tour. Also, each\\nvertex has a ﬁxed cost that incurs from visiting it. The autho rs developed a heuristic with local search that explores\\nexchange, removal, and insertion neighborhoods.\\nGendreau et al. [8] investigated the Covering Tour Problem ( CTP), a problem where the vertices are categorized by\\nthose that can be visited V, must be visited T⊆V, and cannot be visited W. The goal of the CTP is to obtain a\\nminimum cost Hamiltonian cycle over a set of vertices S⊆Vcontaining all vertices in Tand no vertices in W, and\\neach vertex of Wis covered by at least one vertex in S. Exact and heuristic methodologies were proposed to solve t he\\nproblem.\\nHachicha et al. [9] introduced the Multi-Vehicle Covering R outing Problem ( m-CTP). It generalizes the CTP in the\\nsense that there are multiple vehicles, and each route canno t exceed predeﬁned length and number of vertices. The\\nm-CTP was used as the basis to formulate a problem of locating d istribution centers for humanitarian aid in disaster\\nareas [10]. Methodologies to solve the m-CTP include branch-and-cut [11], column generation [12], branch-and-price\\n[13], constructive heuristics [9], evolutionary metaheur istic [11], variable neighborhood descent[14].\\nAllahyari et al. [4] proposed the the Multi-Depot Covering T our Vehicle Routing Problem (MDCTVRP). The MD-\\nCTVRP is a combination of the Multi-Depot Vehicle Routing Pr oblem (MDVRP) [15] and CSP. In the MDCTVRP,\\nthe demand of each customer can be served either by visiting t he customer directly or by covering, i.e, the customer\\nlocation is within a covering range of at least one visited cu stomer. The authors developed two mixed integer program-\\nming formulations and a hybrid metaheuristic, combining Gr eedy Randomized Adaptive Search Procedure (GRASP),\\nIterated Local Search (ILS) and Simulated Annealing (SA).\\nIt is worth noticing that the CSP does not have a multi-vehicl e variant, as does the CTP. This work ﬁlls this gaps by\\nproposing the Capacitated Covering Salesman Problem (CCSP ), a NP-hard problem generalizing both the CVRP and\\nthe CSP. Vertices with non-negative demands must be covered by a set of capacitated vehicles, based at the depot. The\\ngoal is to ﬁnd a minimum cost set of vehicle routes servicing a ll the demands. The CCSP represents a straightforward\\nextension of the CSP where the service employed by the vehicl es comes at a limited supply. At the same time, the\\nCCSP generalizes the CVRP since covering provides an additi onal way to service each demand. It is worth pointing\\nout the differences between m-CTP and CCSP:\\n• CCSP considers demands on the vertices;\\n•m-CTP forces some vertices to be visited ( T⊆V);\\n•m-CTP constrains the routes by their lengths and number of ver tices, while in CCSP the vehicle is capacitated\\nby the amount of serviced demand.\\n• CCSP is a natural generalization of the CSP and the m-CTP generalizes the CTP.\\nOur contributions Two combinatorial optimization problems, the CSP and the VR P, are combined into a general\\nframework to address routing problems with multiple vehicl es and limited capacity in the context of service by cov-\\nering. Mathematical formulations, using integer linear pr ogramming, are provided to represent these problems as a\\nCCSP. The complexity of solving these problems optimally as ks for heuristic methodologies to tackle large instances\\nthat arise from real applications. This work answers this de mand by proposing a biased random-keys genetic algo-\\nrithm to solve the CCSP, and a matheuristic to intensify the s earch. Furthermore, we extended our ILP to solve the\\n2' metadata={'source': './data/2403.06995.pdf', 'page': 1}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nMDCTVRP and conducted computational experiments on a bench mark of instances, comparing our formulation with\\nthe state-of-the-art exact methodology from literature. T he proposed formulation outperformed the previous approac h\\nwith respect to optimality gaps. Moreover, optimal solutio ns were proven for several previously unsolved instances.\\nThe CCSP and MDCTVRP share common concepts of covering and ve hicle capacity. Consequently, both can be\\nmodeled in a similar manner concerning serving remotely cus tomers and demands served by the vehicle. Nevertheless,\\nnotable distinctions exist between these problems. In CCSP , a vehicle is not obligated to serve a customer during its\\nvisit, unlike in MDCTVRP, where the vehicle is required to se rve a customer during each visit. Additionally, in\\nCCSP there are customers with no speciﬁed demand, providing an opportunity for vehicles to use them to serve other\\ncustomers remotely which have demand. In contrast, in MDCTV RP every customer is associated with a speciﬁc\\ndemand.\\nThis paper is organized as follows. Section 2 formally deﬁne s the CCSP and MDCTVRP, presenting ILP formulations\\nfor the CCSP and a MILP formulation for the MDCTVRP. Section 3 describes the BRKGA, and the intra-route and\\ninter-route intensiﬁcation procedures. In Section 4, comp utational experiments are conducted on a representative se t\\nof instances, and results are analyzed and discussed. Secti on 5 gives the concluding remarks.\\n2 Mathematical Formulations\\n2.1 Models for the CCSP\\nConsider a complete undirected graph G(V,E), where each vertex v∈Vhas a demand dv, each edge e∈Ehas a\\nmetric cost ce, and a depot vertex is denoted by v0. LetV0=V\\\\{v0}andVd={v∈V:dv>0}. There are M\\nhomogeneous vehicles with capacity Qthat must service all vertices with positive demand.\\nFor each vertex v∈V,C(v)is the set of vertices that covers vandD(v)is the set of vertices that are covered by v. It\\nis assumed that v∈C(v)andv∈D(v),∀v∈V.\\nA route is a nonempty subset R⊆Eof edges for which the induced subgraph G[R]is a simple cycle containing v0.\\nThe goal of CCSP is to ﬁnd Mroutes of minimum cost with the following constraints:\\n• each vertex is visited no more than once;\\n• each demand dv:v∈Vdis serviced by a route R, which implies in vor some vertex in C(v)being visited\\nbyR, and the demand dvbeing deducted from the capacity of the vehicle;\\n• the total demand serviced by any vehicle must not exceed its capacityQ.\\nFigure 2 shows an optimal solution for a CCSP instance. Route s are depicted with black lines; the blue triangle is v0;\\nred squares are vertices with positive demand; green points are visited vertices with no demand; arrows show which\\nroute serviced each demand.\\nThe following ILP formulation CCSP 1is proposed for the CCSP. We denote δ(v)as the edge cut-set of vertex v, and\\nδ(S)the edge cut-set of a subset S⊆V. The formulation includes the following decision variable s:xe∈Z+gives\\nthe number of times edge e∈Eis traversed; yv∈{0,1}denotes if vertex v∈Vis visited (1) or not (0); zuv∈{0,1}\\nshows if vertex u∈Vdis serviced through vertex v∈C(u)(1) or not (0); K∈Z+is the number of vehicles.\\n3' metadata={'source': './data/2403.06995.pdf', 'page': 2}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\n 0 100 200 300 400 500 600 700 800 900 1000\\n 0 100 200 300 400 500 600 700 800 900 1000\\nFigure 2: Optimal solution for the CCSP instance X-n115-w11-c7.\\n(CCSP 1)\\nMIN∑\\ne∈Ecexe, (1)\\nsubject to\\n∑\\ne∈δ(v0)xe= 2K, (2)\\n∑\\ne∈δ(v)xe= 2yv ∀v∈V0, (3)\\n∑\\nv∈C(u)yv⩾1 ∀u∈Vd, (4)\\nzuv⩽yv ∀u∈Vd,∀v∈C(u), (5)\\n∑\\nv∈C(u)zuv= 1 ∀u∈Vd, (6)\\n∑\\ne∈δ(S)xe⩾2\\nQ∑\\nu∈Vd∑\\nv∈(S∩C(u))duzuv ∀S⊆V0, (7)\\nxe∈{0,1} ∀ e /∈δ(v0), (8)\\nxe∈{0,1,2} ∀ e∈δ(v0), (9)\\nyv∈{0,1} ∀ v∈V0, (10)\\nzuv∈{0,1} ∀ u∈Vd,∀v∈C(u), (11)\\nK∈Z+. (12)\\n4' metadata={'source': './data/2403.06995.pdf', 'page': 3}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nThe objective function ( 1) minimizes the total cost of the routes. Constraints ( 2) state that the vertex depot is visited\\nby allKroutes. Constraints ( 3) ensure that the number of edges incident to a vertex v∈V0is2ifvis visited or 0\\notherwise. Constraints ( 4) impose that each vertex in Vdmust be covered by at least one route. Constraints ( 5) state\\nthat if a vertex u∈Vdis serviced by a vertex v∈C(u), thenvis visited. Constraints ( 6) ensure that every vertex\\nu∈Vdis serviced by a vertex v∈C(u). Constraints ( 7) impose both the connectivity and the vehicle capacity by\\nforcing into the solution a sufﬁcient number of edges to each subset of vertices.\\nA second formulation, denominated CCSP 2, can be derived by eliminating variables ythrough variable substitution\\nusing constraints ( 3).\\n(CCSP 2)\\nMIN∑\\ne∈Ecexe,\\nsubject to\\n∑\\ne∈δ(v)xe⩾2zuv ∀v∈V0,∀u∈Vd, (13)\\n∑\\ne∈δ(v)xe⩽2∑\\nu∈Vdzuv ∀v∈V0, (14)\\n∑\\ne∈δ(v)xe⩽2 ∀v∈V0, (15)\\n(2),(6),(7),(8),(9),(11),(12).\\nConstraints ( 13), (14), and (15) impose the correct number of edges incident to a vertex v∈V0(2if visited or 0\\notherwise).\\nPreliminary experiments have shown that, even though the CCSP 2has fewer variables than the CCSP 1, the overall\\nquality of the upper and lower bounds obtained by CCSP 1is better than CCSP 2. Therefore, only the CCSP 1\\nformulation will be considered in the computational experi ments.\\n2.2 Models for a Multi-depot Variant\\nThe Multi-Depot Covering Tour Vehicle Routing Problem (MDC TVRP), proposed by Allahyari et al. [4], is deﬁned\\nnext. Given a directed graph G= (N,A), with vertices N=Nc∪Nd, and arcs A. Each customer i∈Nc=\\n{1,2,...,nc}has a positive demand di. SetNd={1,...,nd}contains the depots. Each arc (i,j)∈Ahas a positive\\ntraversing cost cij. Each customer has to be covered by a route. Set C(v)represents the vertices that covers v. A cost\\nc′\\nij>0is attributed for servicing customer ithroughj. A set of identical vehicles P={1,2,...,p}is available, and\\nQis the vehicle capacity. Each depot k∈Ndhas a limited capacity H. Finally, to each depot is attributed a unique\\nsetPk={1,...,p k}of vehicles. The objective of MDCTVRP is to ﬁnd a minimum cost set of routes, such that all\\ndemands are covered, the vehicles and depots capacities are satisﬁed, and each vehicle starts and ends its route in the\\nsame depot.\\nBorrowing ideas from model CCSP 1and from the ﬂow-based formulation by Allahyari et al. [4], w e propose a\\nnew MILP formulation for the MDCTVRP. The formulation, henc eforth denominated MDCTVRP m, includes the\\nfollowing decision variables: xijdenotes if arc (i,j)∈Ais traversed ( 1) or not (0);yvrepresents if vertex v∈Nc\\nis visited ( 1) or not (0);zuvshows if vertex u∈Ncis serviced by vertex v∈Nc(1) or not (0);fijgives the vehicle\\nload while traversing arc (i,j). We represent SPas the set containing all simple paths connecting depots. Sp eciﬁcally,\\nSP(st)∈SPdenotes the set of all simple paths between depots sandt.\\n5' metadata={'source': './data/2403.06995.pdf', 'page': 4}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\n(MDCTVRP m)\\nMIN∑\\n(i,j)∈Acijxij+∑\\ni∈Nc∑\\nj∈Ncc′\\nijzij, (16)\\nsubject to\\n∑\\nj∈Ncxjk=∑\\nj∈Ncxkj ∀k∈Nd, (17)\\n∑\\nj∈Ncxkj⩽|Pk| ∀ k∈Nd, (18)\\n∑\\nj∈Nxjv=∑\\nj∈Nxvj=yv ∀v∈Nc, (19)\\n∑\\nv∈C(u)yv⩾1 ∀u∈Nc, (20)\\nzuv⩽yv ∀u∈Nc,∀v∈C(u), (21)\\n∑\\nj∈Nxvj⩽zvv ∀v∈Nc, (22)\\n∑\\nv∈C(u)zuv= 1 ∀u∈Nc, (23)\\n∑\\n(i,j)∈SP(st)xij⩽⏐⏐⏐SP(st)⏐⏐⏐−1 ∀s,t∈Nd,s̸=t,∀SP(st)∈SP, (24)\\n∑\\nj∈Nfji=∑\\nj∈Ncdjzji+∑\\nj∈Nfij ∀i∈Nc, (25)\\n∑\\ni∈Ncfik= 0 ∀k∈Nd, (26)\\nfij⩽(Q−di)xij ∀(i,j)∈A:i∈N,j∈Nc, (27)\\ndjxij⩽fij ∀(i,j)∈A:i∈N,j∈Nc, (28)\\n∑\\ni∈Ncfki⩽H ∀k∈Nd, (29)\\nxi,j∈{0,1} ∀ (i,j)∈A, (30)\\nyv∈{0,1} ∀ v∈Nc, (31)\\nzuv∈{0,1} ∀ u∈Nc,∀v∈C(u), (32)\\nfij∈R+∀(i,j)∈A. (33)\\nThe objective function ( 16) minimizes the total cost of the routes and allocations cost s. For each depot k∈Nd,\\nconstraints ( 17) impose that the number of vehicles arriving kmust be equal to the number of vehicles leaving k.\\nConstraints ( 18) bound the amount of vehicles arriving each depot. For each c ustomerv∈Nc, constraints ( 19)\\nstate that the number of arcs arriving and leaving vis1ifvis visited, or 0otherwise. Constraints ( 20) impose that\\neach vertex in Ncmust be covered by at least one route. Constraints ( 21) state that a vertex u∈Nccan only be\\nserviced through a vertex v∈C(u)ifvis visited. Constraints ( 22) require that if a customer v∈Ncis visited by a\\nvehicle, then its demand is serviced by itself. Constraints (23) ensure that every vertex u∈Ncis serviced by a vertex\\nv∈C(u). Constraints ( 24) prevent simple paths between depots, forcing each route to start and end in the same depot.\\nConstraints ( 25) impose the ﬂow conservation on each customer i. Constraints ( 26) ensure that the vehicle load is zero\\nwhen returning to the depot. Constraints ( 27) and constraints ( 28) bound the vehicle load when traversing arc (i,j).\\nConstraints ( 29) impose that the capacity of each depot is at most H.\\nThe number of decision variables used in the new MILP formula tion isO(V2)and the number of decision variables\\nused in the model by Allahyari et al. [4] is O(V3). AnO(V2)algorithm can separate constraints ( 24) for integer\\nsolutions using a lazy constraint strategy. Given a graph in duced by an integer solution, the separation of constraints\\n(24) is performed using Depth-First-Search (DFS) [16]. For eve ry pair(i,j)such that i,j∈Nd, a DFS is performed\\n6' metadata={'source': './data/2403.06995.pdf', 'page': 5}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nstarting from i. Ifjis reached, the edges from the path between iandjare retrieved and then a constraint ( 24) is\\nadded to the formulation.\\n3 BRKGA for Capacitated Covering Salesman Problem\\nGuided by the Darwinian principle of the survival of the ﬁtte st, the Biased Random Key Genetic Algorithm (BRKGA)\\n[17] is an evolutionary metaheuristic in which a population of individuals, representing solutions of a combinatorial\\noptimization problem, evolves towards the optimal.\\nEach individual is represented by a chromosome encoded as a v ector, in which each allele is a random key uniformly\\ndrawn over the interval [0,1). The decoder method is the problem-speciﬁc component of the BRKGA which is\\nresponsible for mapping a chromosome into a solution.\\nAn initial population of random chromosomes is created and f orced into a selective pressure environment in which the\\nbest individuals are more likely to survive throughout the g enerations producing offsprings.\\nThe BRKGA partitions the population into elite andnon-elite sets, with sizes determined by ﬁxed parameters. The\\nelite set is composed by the best individuals; all the remain ing individuals form the non-elite set, including the mutants ,\\ni.e., random chromosomes introduced into the population as a form of diversiﬁcation.\\nIn each generation, the BRKGA executes the following steps:\\n1. Decode the chromosomes, evaluating their ﬁtness;\\n2. Identify the best individuals to form the elite set;\\n3. Preserve the elite set into the population for the next gen eration;\\n4. Introduce the mutants in the next generation;\\n5. Generate offsprings through the crossover of elite and no n-elite chromosomes, inserting them in the next\\ngeneration.\\nThe BRKGA has demonstrated its efﬁcacy as a robust method for addressing various routing problems [18, 19, 20, 21,\\n22]. In this sense, the following sections describe how the B RKGA can be employed to solve the CCSP.\\n3.1 Solution encoding\\nThe solution is encoded as a vector X= (x1,...,xn)of sizen=|Vd|, wherexiis a random number in the interval\\n[0,1), fori= 1,...,n . Each element ofXrepresents a vertex of Vd.\\n3.2 Decoder function\\nThe decoder function takes as input a vector Xand returns a feasible solution for the CCSP represented by a set of\\nroutesR. LetX′be the vector resulting by sorting the keys of Xin non-decreasing order.\\nThe proposed decoder for the CCSP has two phases, described i n the following sections.\\n3.2.1 First phase - Best Fit Algorithm\\nThe minimum number of vehicles required to service all deman ds can be determined by solving a Bin Packing Problem\\n(BPP) [23], which is NP-hard. Our decoder assigns vertices f romVdto vehicles by solving the BPP approximately,\\nusing the Best Fit Algorithm (BFA). The BFA assigns each vert ex to a vehicle with the least residual capacity that can\\nstill service the vertex; if no such vehicle exists, a new one is assigned (Figure 3).\\nAlgorithm 1 presents the BFA pseudo-code applied in the deco der, which can be implemented using self-balancing\\nsearch trees leading to a worst-time complexity of O(nlgn).\\n3.2.2 Second phase - route construction\\nConsider that route Rmof vehicle mis a sequence of vertices represented as Rm={Rm(0),...,R m(rm+ 1)},\\nwhereRm(i)andrmare, respectively, the i-th vertex visited by vehicle mand the number of vertices in V0visited by\\nvehiclem. The route starts and ends at the depot, i.e., Rm(0) =Rm(rm+1) =v0.\\n7' metadata={'source': './data/2403.06995.pdf', 'page': 6}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nFigure 3: Example of Best Fit Algorithm. 8' metadata={'source': './data/2403.06995.pdf', 'page': 7}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nAlgorithm 1 Best ﬁt algorithm\\nInput: a vectorX′.\\nOutput: a set of vehiclesM={1,...,M}and their assigned vertices A={A1,...,A M}.\\n1:m←0;M←{∅} ;A←{∅} ;\\n2:foreachx′\\ni∈X′do\\n3:v←getVertex (i)– returns the vertex associated to element x′\\ni;\\n4: if\\uf8eb\\n\\uf8ed∃m∈M:∑\\nu∈Amdu⩽Q−dv\\uf8f6\\n\\uf8f8then\\n5: m←arg max\\nm′∈M\\uf8f1\\n\\uf8f2\\n\\uf8f3∑\\nu∈A′mdu:∑\\nu∈A′mdu⩽Q−dv\\uf8fc\\n\\uf8fd\\n\\uf8fe;\\n6: Am←Am∪{v};\\n7: else\\n8: m←m+1;\\n9:M←M∪{ m};\\n10: Am←{v};\\n11:A←A∪{ Am}\\nThe second phase of the decoder creates a route for each vehic le by using the following insertion cost function,\\ng(v,Rm) = min\\ni={0,...,rm}{\\nc(u,v)+c(v,w)−c(u,w):u=Rm(i),w=Rm(i+1)}\\n,\\nwhich gives the minimum cost of inserting a vertex vinto a route Rm.\\nFor each vehicle m∈M and each vertex u∈Am, all unvisited vertices v∈C(u)are considered to be included in\\nrouteRmby checking the value of g(v,Rm)and taking the vertex resulting in the least cost increment.\\nAlgorithm 2 presents the route construction pseudo-code us ed in our decoder.\\nA vertexvis called redundant if when removed from a route it does not change the solution fe asibility. It occurs when\\nthe vertices serviced by vcan all be serviced by other vertices in the solution without violating the capacity of any\\ninvolved vehicle. After applying Algorithm 2, the decoder g reedily removes redundancies by considering their cost\\ndecrease, until a maximal set of redundancies is removed.\\n3.3 Intra-Route\\nOnce the BRKGA stopping criteria is triggered, the Lin-Kern ighan (LK) heuristic [24] performs the ﬁnal intra-routes\\nimprovements. The LK heuristic is based on k-opt neighborhood, which consists in applying up to kedge exchanges,\\nand it is considered one of the best local searches for the Tra veling Salesman Problem.\\n3.4 Inter-Route Intensiﬁcation\\nFollowing the ideas of Sartori and Buriol [25], this paper pr oposes a matheuristic for the CCSP using a formulation\\nwith covering and packing constraints. Let Fbe the set of all CCSP feasible routes, aifthe covering matrix where for\\neach pair (i,f),i∈Vdandf∈F,aif= 1 if and only if vertex iis serviced by route f, andbifthe visiting matrix\\nwhere for each pair (i,f),i∈V0andf∈F,bif= 1 if and only if vertex iis visited by route f. The formulation\\nincludes the binary variable λf, which denotes whether the feasible route f∈Fis used (1) or not (0).\\nThe formulation reads as follows:\\n9' metadata={'source': './data/2403.06995.pdf', 'page': 8}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nAlgorithm 2 Construction of Routes\\nInput: a set of vehiclesM={1,...,M}and their assigned vertices A={A1,...,A M}.\\nOutput: a set of routesR={R1,...,R m}.\\n1:R←∅ ;\\n2:\\n3:foreach vehicle m∈M do\\n4:Rm(0)←{v0};\\n5:\\n6:rm←0\\n7:\\n8: foreachv∈Amdo\\n9: S←{\\nu∈C(v):u̸=Rm′(u′),∀m′∈{1,...,m},∀u′∈{1,...,r m′}}\\n– set of unvisited candidates to\\nservicev\\n10:\\n11: u←argmin\\nu′∈S{\\ng(u′,Rm)}\\n– ﬁnd best candidate\\n12:\\n13: insert(u,Rm)– insertuin the best position of route Rm\\n14:\\n15: rm←rm+1\\n16:\\n17:Rm(rm)←{v0};\\n18:\\n19:R←R∪{ Rm};\\n20:\\n(Matheuristic )\\nMIN∑\\nf∈Fcfλf, (34)\\nsubject to\\n∑\\nf∈Faifλf⩾1 ∀i∈Vd (35)\\n∑\\nf∈Fbifλf⩽1 ∀i∈V0 (36)\\nλf∈{0,1} ∀ f∈F.\\nThe objective function ( 34) minimizes the costs of the routes. Constraints ( 35) ensure that each vertex in Vdmust\\nbe serviced by at least one route, while constraints ( 36) impose that every vertex in V0must be visited by at most\\none route. Considering that the cardinality of Fgrows exponentially, in this work we generate a pool of route sF′in\\nthe Matheuristic formulation. The pool of routes contains a set of CCSP feasible routes generated as follows. First,\\nan exhaustive search is conducted to ﬁnd every optimal route that services up to three vertices. All of these routes\\nare included into F′. The remaining routes of F′are ﬁlled with the elite individuals from the BRKGA generati ons,\\nstarting from the last generation and continuing until eith er the size limit of F′is reached or all elite individuals from\\neach BRKGA generation are added to F′.\\n4 Computational Experiments\\n4.1 Instances Benchmark\\nThe instances for the CCSP were obtained from the CVRP instan ces created by Christoﬁdes and Eilon [26] and Uchoa\\net al. [27], containing between 101and303vertices, and named as E-nA-kBandX-nA-kB, respectively. The\\n10' metadata={'source': './data/2403.06995.pdf', 'page': 9}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nsymbolAgives the number of vertices (with the depot), and Brepresents the number of vehicles required to service\\nall the demands.\\nTo generate CCSP instances, the following parameters were u sed:\\n•|Vd|: number of vertices with demand;\\n•⏐⏐D(v)⏐⏐: covering size, where v∈V0.\\nThe|Vd|parameter varied in 10%,20%, and40% ofn. The vertices with demand consist of the ﬁrst |Vd|vertices,\\nexcluding the depot, of the CVRP instance. It is worth mentio ning that all demands associated with Vdremain un-\\nchanged in relation to CVRP instance. Similarly, the vehicl e capacity Qin the CCSP instance remains the same as in\\nthe CVRP instance.\\nFor each vertex v∈V0, the setD(v)is deﬁned by the closest vertices from v. The cardinality of D(v)varied in 7,9,\\nand11.\\nFor each CVRP instance, all pairings of |Vd|and⏐⏐D(v)⏐⏐were considered, resulting in nine combinations and a bench -\\nmark of495instances. A pre-processing was conducted in the set of inst ances to remove any vertex v∈V0such that\\nD(v)∩Vd=∅.\\nThe MDCTVRP instances were created by Allahyari et al. [4], a nd they are divided in small ( 120instances) and large\\n(160instances). The small instances contain up to 30vertices, and the large instances have up to 90vertices. In small\\ninstances, the vehicle capacity ﬂuctuates between 140and150, while in large instances, the vehicle capacity alternates\\nbetween160and170. The small instances are divided into three categories, and the large instances are divided into\\nfour categories. Each category has eight different groups o f instances. The instances are named InputXYZT, where\\n“X”, “Y”, “Z”, and “T” give the category, number of depots, ve hicle capacity, and the coverage coefﬁcient (which\\ndeﬁnes the cost of serving a vertex), respectively. Five ins tances were generated for each group of instances.\\n4.2 Computational Settings\\nThe ILP and MILP formulations were implemented and solved us ing Gurobi 8.1.1 version. The execution time limit\\nwere set to a one hour, except for the MDCTVRP mformulation, which was set to two hours following Allahyari\\net al. [4]. The experiments were conducted on a PC under Ubunt u10.12, and CPU Intel Xeon(R) Silver 3114 2.20\\nGHz, with 32GB of RAM. The BRKGA developed for the CCSP used the C++ framew ork from Resende and Toso\\n[28]. The parameters used by the BRKGA are listed in Table 1. T he implementation of the LK heuristic proposed by\\nHelsgaun [29] was employed. The matheuristic adopted a size limit for the pool F′of1million routes.\\nTable 1: BRKGA parameters.\\nParameter Value\\nPopulation size 1000\\nFraction of population to be elite individuals 40%\\nFraction of population to be replaced by mutants 20%\\nCrossover probability 70%\\n4.3 Evaluated Methodologies\\nFive methodologies were implemented and evaluated in the co mputational experiments:\\n•CCSP1: solution of the CCSP 1model, initially ignoring Constraints (7), and later including them in the\\nformulation using a lazy constraint strategy;\\n•BRKGA : implementation of the BRKGA for the CCSP described in Secti on 3;\\n•CCSP1s: same as CCSP1, however the solution obtained by the BRKGA is given as warm s tart for the\\nCCSP 1model;\\n•Matheuristic : solution of the matheuristic described in Subsection 3.4;\\n•MDCTVRP m: solution of the MDCTVRP mformulation, initially ignoring Constraints ( 24), and including\\nthem on-demand in the formulation using a lazy constraint strategy.\\n11' metadata={'source': './data/2403.06995.pdf', 'page': 10}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\nTheMDCTVRP mwas compared with the ﬂow-based formulation ( Fflow) and node-based formulation ( Fnode), both\\nproposed by Allahyari et al. [4], which are the state-of-the -art exact methodologies for the MDCTVRP, to the best of\\nour knowledge.\\n4.4 Results for the CCSP\\nFull experimental data, results, instances, and source cod es are available on-line1. The results of the computational\\nexperiments show that the CCSP1methodology was able to obtain upper bounds for 430out of495instances. In\\naddition, the CCSP1methodology proved optimality for 71instances size up to 101vertices. Analyzing the results\\nof theBRKGA methodology, we can note that for 407instances, the obtained solutions were better than the uppe r\\nbounds obtained by CCSP1.\\nWith respect to methodologies CCSP1sandMatheuristic , the results show that the matheuristic was more effective\\nto improve the solutions obtained from BRKGA .Matheuristic methodology improved the BRKGA solutions for\\n187out of495instances, while CCSP1simproved for 88instances. The average cost of improvements made by\\nCCSP1sandMatheuristic on BRKGA solutions were approximately 1.69% and2.3%, respectively.\\nFigure 4 shows, for each methodology, the percentage of solv ed instances in function of the deviation from the best\\nupper bound ( (UB−BestUB\\nUB)∗100). The performance proﬁle clearly shows the BRKGA dominating CCSP1with re-\\nspect to upper bounds. The BRKGA obtained the best solutions for approximately 48% of instances, while the CCSP1\\nobtained for approximately 16% of instances. The CCSP1swas able to improve the warm start BRKGA solution\\nfor several instances, obtaining the best solutions for app roximately 58% of instances. Finally, comparing method-\\nologiesMatheuristic andCCSP1s,Matheuristic was more effective in improving the BRKGA solutions. The\\nMatheuristic methodology obtained the best solution for approximately 74% of instances, outperforming CCSP1s.\\n 20 30 40 50 60 70 80 90 100\\n 0  10  20  30  40  50  60  70Percentage of instances\\nPercentage deviation from the best upper boundCCSP−1 (without start solution)\\nBRKGA\\nCCSP−1s (with start solution)\\nMatheuristic 50 60 70 80 90 100\\n 0  5  10  15  20\\nFigure 4: Performance proﬁles in terms of deviation ( %) from the best upper bound.\\n1http://www.ic.unicamp.br/~fusberti/problems/ccsp\\n12' metadata={'source': './data/2403.06995.pdf', 'page': 11}\n",
            "page_content='Exact algorithms and heuristics for capacitated covering s alesman problems\\n4.5 Results for the MDCTVRP\\nTables 2 and 3 report the results for the small and large insta nces, respectively. Table 2 reports for each methodology\\nand for each group of small-size instances, the average gap ( Avg.gap), the number of optimal solutions (#Opt), and\\nthe average running time (Avg.time).\\nFor small instances, the overall optimality gaps were 0.10%,8.99%, and30.28% for theMDCTVRP m,Fflow, and\\nFnode methodologies, respectively. From 120small instances, 117,10, and4optimal solutions were obtained by\\nMDCTVRP m,Fflow, andFnode, respectively.\\nTable 3 gives the results from MDCTVRP mfor each group of large-size instances. The column group MDCTVRP m\\nreports the averages upper bound (Avg.ub), lower bound (Avg .lb), optimality gap (Avg.gap), running time (Avg.time),\\nand number of optimal solutions (#Opt). The column group GRA SP x ILS [4] reports the results obtained by the\\nhybrid meta-heuristic proposed by Allahyari et al. [4]. “Av g” gives the average cost obtained over ﬁve executions for\\neach instance. Avg.gap LPand Avg.gap MDCTVRP mgive the “Avg” gap from the linear relaxation of Fflow formulation,\\nand the average lower bound (Avg.lb) of MDCTVRP m, respectively.\\nIt should be noticed that, due to the large number of variable s, Allahyari et al. [4] have not executed experiments on\\nlarge instances with Fflow. The computational results have shown that MDCTVRP machieved good lower bounds\\nfor large. Previously, the GRASP x ILS overall gap with respe ct to the linear programming relaxation of Fflow was\\n20.98%, while with the new lower bounds obtained by MDCTVRP mthe GRASP x ILS overall gap was improved to\\n7.98%. It is worth noting that even though MDCTVRP mis an exact methodology, the upper bounds were close to the\\nsolutions cost obtained by the GRASP x ILS. More speciﬁcally , the solutions cost obtained by GRASP x ILS are, on\\naverage, only 1.37% apart from the upper bounds obtained by MDCTVRP m.\\n13' metadata={'source': './data/2403.06995.pdf', 'page': 12}\n",
            "page_content='14Table 2: Results of computational experiments for the small -size instances.\\nMDCTVRP m Fflow Fnode\\nCategory Group Avg.gap #Opt Avg.time Avg.gap #Opt Avg.time Avg.gap #Opt Avg.time\\n1Input1000 0.00 5 8.00 6.37 0 7200.00 22.25 0 7200.00Input1001 0.00 5 8.80 6.38 0 7200.00 21.29 0 7200.00Input1010 0.00 5 6.60 1.74 2 6015.00 17.71 0 7200.00Input1011 0.00 5 5.20 2.48 1 7066.00 18.63 0 7200.00Input1100 0.00 5 7.00 4.96 2 5621.00 14.19 1 7200.00Input1101 0.00 5 6.80 4.87 1 5848.00 12.44 1 7200.00Input1110 0.00 5 5.80 3.14 2 5504.00 13.85 1 7200.00Input1111 0.00 5 6.20 2.03 2 5220.00 11.57 1 7200.00\\n2Input2000 0.00 5 140.20 10.56 0 7200.00 42.30 0 7200.00Input2001 0.00 5 184.60 10.81 0 7200.00 38.64 0 7200.00Input2010 0.00 5 70.20 10.39 0 7200.00 37.45 0 7200.00Input2011 0.00 5 115.40 11.14 0 7200.00 33.94 0 7200.00Input2100 0.00 5 66.60 7.98 0 7200.00 31.33 0 7200.00Input2101 0.00 5 138.20 7.63 0 7200.00 27.85 0 7200.00\\ninput2110 0.00 5 73.20 9.02 0 7200.00 28.45 0 7200.00\\nInput2111 0.00 5 44.60 9.55 0 7200.00 27.43 0 7200.00\\n3Input3000 1.01 4 2653.20 14.48 0 7200.00 50.29 0 7200.00Input3001 0.71 4 2853.20 14.93 0 7200.00 44.15 0 7200.00Input3010 0.00 5 1865.80 15.23 0 7200.00 44.46 0 7200.00Input3011 0.58 4 1923.60 14.29 0 7200.00 41.20 0 7200.00Input3100 0.00 5 288.20 12.95 0 7200.00 39.75 0 7200.00Input3101 0.00 5 246.40 11.83 0 7200.00 35.90 0 7200.00Input3110 0.00 5 383.60 12.25 0 7200.00 36.70 0 7200.00Input3111 0.00 5 492.20 10.85 0 7200.00 35.00 0 7200.00\\nAverage 0.10 483.07 8.99 6869.75 30.28 7200.00' metadata={'source': './data/2403.06995.pdf', 'page': 13}\n",
            "page_content='15Table 3: Results of computational experiments for the large -size instances.\\nMDCTVRP m GRASP x ILS [4]\\nCategory Group Avg.ub Avg.lb Avg.gap #Opt Avg.time Avg Avg. gapLP Avg.gap MDCTVRP m\\n4Input4000 799.45 751.98 6.22 0 7200.00 796.21 19.30 5.88Input4001 808.23 764.91 5.55 0 7200.00 806.95 18.57 5.50Input4010 787.18 729.22 7.86 0 7200.00 775.66 19.68 6.37Input4011 792.13 744.53 6.26 0 7200.00 787.64 19.12 5.79Input4100 737.27 692.92 6.24 0 7200.00 733.72 21.90 5.89Input4101 750.94 701.21 6.98 0 7200.00 745.62 20.91 6.33Input4110 721.12 678.30 6.09 1 6650.80 711.94 21.26 4.96Input4111 730.01 689.79 5.72 0 7200.00 728.17 21.03 5.56\\n5Input5000 899.58 815.45 10.27 0 7200.00 880.01 19.80 7.92Input5001 903.58 829.00 8.95 0 7200.00 895.99 18.93 8.08Input5010 869.08 784.04 10.82 0 7200.00 857.27 21.41 9.34Input5011 886.07 800.62 10.63 0 7200.00 873.99 20.46 9.16Input5100 810.75 732.59 10.65 0 7200.00 796.88 22.12 8.78Input5101 827.35 748.43 10.52 0 7200.00 813.70 20.85 8.72Input5110 785.86 711.22 10.51 0 7200.00 777.99 23.23 9.39Input5111 804.80 728.69 10.37 0 7200.00 794.78 21.71 9.07\\n6Input6000 1005.58 922.85 8.88 0 7200.00 997.18 20.01 8.05Input6001 1037.50 936.43 10.75 0 7200.00 1016.10 19.43 8.51Input6010 987.04 898.72 9.63 0 7200.00 968.81 20.59 7.80Input6011 998.51 916.29 8.83 0 7200.00 989.60 20.16 8.00Input6100 955.65 875.82 9.07 0 7200.00 946.88 20.63 8.11Input6101 984.79 893.52 10.09 0 7200.00 967.23 20.11 8.25Input6110 936.80 848.97 10.27 0 7200.00 920.24 21.58 8.40Input6111 953.62 867.56 9.88 0 7200.00 939.51 20.77 8.29\\n7Input7000 1031.77 930.53 10.79 0 7200.00 1013.92 21.35 8.96Input7001 1047.06 935.57 11.88 0 7200.00 1019.34 20.89 8.95Input7010 1012.71 905.58 11.78 0 7200.00 989.22 21.90 9.24Input7011 1018.47 910.80 11.83 0 7200.00 995.08 21.47 9.25Input7100 933.58 842.19 10.81 0 7200.00 918.91 22.90 9.11Input7101 948.35 849.03 11.65 0 7200.00 924.61 22.39 8.90Input7110 917.68 830.41 10.51 0 7200.00 904.92 23.56 8.97Input7111 919.48 832.89 10.37 0 7200.00 915.05 23.24 9.86\\nAverage 893.81 815.63 9.40 881.35 20.98 7.98' metadata={'source': './data/2403.06995.pdf', 'page': 14}\n",
            "page_content='16\\n5 Final Remarks\\nThis work proposes the Capacitated Covering Salesman Probl em (CCSP), a problem that approaches the notion of\\ncoverage in vehicle routing problems. Two ILP formulations and a BRKGA are proposed to solve the CCSP. From the\\nset of instances for the CVRP [26, 27], a benchmark of instanc es for CCSP was generated.\\nComputational experiments conducted on a benchmark of 198instances for CCSP evaluated the ILP formulation and\\nthe BRKGA. The results show the effectiveness of the BRKGA in obtaining upper bounds for all instances. The\\nCCSP 1obtained optimal solutions for 71instances, with up to 101vertices.\\nFurthermore, a new MILP formulation is proposed for the Mult i-Depot Covering Tour Vehicle Routing Problem (MD-\\nCTVRP). Computational experiments were conducted on a benc hmark of 280instances from literature. The overall\\nresults show unequivocally the new formulation outperform ing the best known exact methodology from literature,\\nobtaining 118new optimal solutions and improving all known lower bounds.\\nFuture works should focus on valid inequalities and a branch -and-cut framework for the solution of CCSP and MD-\\nCTVRP. Another promising ﬁeld of research is to consider mul ti-objective vehicle routing problem with covering\\nrange being the additional objective function.\\nAcknowledgments\\nThis work was supported by CAPES, CNPq, and Fapesp (grants 14 0960/2017-1, 314384/2018-9, 435520/2018-0,\\n2015/11937-9).\\nReferences\\n[1] George B Dantzig and John H Ramser. The truck dispatching problem. Management science , 6(1):80–91, 1959.\\n[2] Jean-François Cordeau, Gilbert Laporte, Martin WP Save lsbergh, and Daniele Vigo. Vehicle routing. Handbooks\\nin operations research and management science , 14:367–428, 2007.\\n[3] Bruce L Golden, Subramanian Raghavan, and Edward A Wasil .The vehicle routing problem: latest advances\\nand new challenges , volume 43. Springer Science & Business Media, 2008.\\n[4] Somayeh Allahyari, Majid Salari, and Daniele Vigo. A hyb rid metaheuristic algorithm for the multi-depot\\ncovering tour vehicle routing problem. European Journal of Operational Research , 242(3):756–768, 2015.\\n[5] John R Current and David A Schilling. The covering salesm an problem. Transportation science , 23(3):208–213,\\n1989.\\n[6] David L. Applegate, Robert E. Bixby, Vasek Chvatal, and W illiam J. Cook. The Traveling Salesman Problem:\\nA Computational Study (Princeton Series in Applied Mathema tics). Princeton University Press, Princeton, NJ,\\nUSA, 2007.\\n[7] Bruce Golden, Zahra Naji-Azimi, S Raghavan, Majid Salar i, and Paolo Toth. The generalized covering salesman\\nproblem. INFORMS Journal on Computing , 24(4):534–553, 2012.\\n[8] Michel Gendreau, Gilbert Laporte, and Frédéric Semet. T he covering tour problem. Operations Research ,\\n45(4):568–576, 1997.\\n[9] Mondher Hachicha, M John Hodgson, Gilbert Laporte, and F rédéric Semet. Heuristics for the multi-vehicle\\ncovering tour problem. Computers & Operations Research , 27(1):29–42, 2000.\\n[10] Zara Naji-Azimi, Jacques Renaud, Angel Ruiz, and Majid Salari. A covering tour approach to the location of\\nsatellite distribution centers to supply humanitarian aid .European Journal of Operational Research , 222(3):596–\\n605, 2012.\\n[11] Minh Hoang Ha, Nathalie Bostel, André Langevin, and Lou is-Martin Rousseau. An exact algorithm and a\\nmetaheuristic for the multi-vehicle covering tour problem with a constraint on the number of vertices. European\\nJournal of Operational Research , 226(2):211–220, 2013.\\n[12] Keisuke Murakami. A column generation approach for the multi-vehicle covering tour problem. In Automation\\nScience and Engineering (CASE), 2014 IEEE International Co nference on , pages 1063–1068. IEEE, 2014.\\n[13] Nicolas Jozefowiez. A branch-and-price algorithm for the multivehicle covering tour problem. Networks ,\\n64(3):160–168, 2014.' metadata={'source': './data/2403.06995.pdf', 'page': 15}\n",
            "page_content='17\\n[14] Manel Kammoun, Houda Derbel, Mostapha Ratli, and Basse m Jarboui. An integration of mixed vnd and vns: the\\ncase of the multivehicle covering tour problem. International Transactions in Operational Research , 24(3):663–\\n679, 2017.\\n[15] Frank A Tillman. The multiple terminal delivery proble m with probabilistic demands. Transportation Science ,\\n3(3):192–204, 1969.\\n[16] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, a nd Clifford Stein. Introduction to algorithms . MIT\\npress, 2022.\\n[17] José Fernando Gonçalves and Mauricio GC Resende. Biase d random-key genetic algorithms for combinatorial\\noptimization. Journal of Heuristics , 17(5):487–525, 2011.\\n[18] Efrain Ruiz, Valeria Soto-Mendoza, Alvaro Ernesto Rui z Barbosa, and Ricardo Reyes. Solving the open vehicle\\nrouting problem with capacity and distance constraints wit h a biased random key genetic algorithm. Computers\\n& Industrial Engineering , 133:207–219, 2019.\\n[19] Alberto F Kummer N, Luciana S Buriol, and Olinto CB de Ara újo. A biased random key genetic algorithm\\napplied to the vrptw with skill requirements and synchroniz ation constraints. In Proceedings of the 2020 Genetic\\nand Evolutionary Computation Conference , pages 717–724, 2020.\\n[20] Levi R Abreu, Roberto F Tavares-Neto, and Marcelo S Naga no. A new efﬁcient biased random key genetic\\nalgorithm for open shop scheduling with routing by capacita ted single vehicle and makespan minimization. En-\\ngineering Applications of Artiﬁcial Intelligence , 104:104373, 2021.\\n[21] Alberto F Kummer, Olinto CB de Araújo, Luciana S Buriol, and Mauricio GC Resende. A biased random-key\\ngenetic algorithm for the home health care problem. International Transactions in Operational Research , 2022.\\n[22] Saúl Domínguez-Casasola, José Luis González-Velarde , Yasmín Á Ríos-Solís, and Kevin Alain Reyes-Vega. The\\ncapacitated family traveling salesperson problem. International Transactions in Operational Research , 2023.\\n[23] H Kellerer, U Pferschy, and D Pisinger. Knapsack Problems , volume 1. Springer-Verlag Berlin Heidelberg, 2004.\\n[24] Shen Lin and Brian W Kernighan. An effective heuristic a lgorithm for the traveling-salesman problem. Opera-\\ntions research , 21(2):498–516, 1973.\\n[25] Carlo S Sartori and Luciana S Buriol. A matheuristic app roach to the pickup and delivery problem with time\\nwindows. In International Conference on Computational Logistics , pages 253–267. Springer, 2018.\\n[26] Nicos Christoﬁdes and Samuel Eilon. An algorithm for th e vehicle-dispatching problem. Journal of the Opera-\\ntional Research Society , 20(3):309–318, 1969.\\n[27] Eduardo Uchoa, Diego Pecin, Artur Pessoa, Marcus Poggi , Thibaut Vidal, and Anand Subramanian. New\\nbenchmark instances for the capacitated vehicle routing pr oblem. European Journal of Operational Research ,\\n257(3):845–858, 2017.\\n[28] Rodrigo F Toso and Mauricio GC Resende. A c++ applicatio n programming interface for biased random-key\\ngenetic algorithms. Optimization Methods and Software , 30(1):81–93, 2015.\\n[29] Keld Helsgaun. An effective implementation of the lin– kernighan traveling salesman heuristic. European Journal\\nof Operational Research , 126(1):106–130, 2000.' metadata={'source': './data/2403.06995.pdf', 'page': 16}\n",
            "Processing  ./data/2403.06994.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='PHYSICS SENSOR BASED DEEPLEARNING FALL DETECTION\\nSYSTEM∗\\nZeyuan Qu, Tiange Huang, Yuxin Ji, Yongjun Li*\\nSchool of Computer, Northwestern Polytechnical University, Xi’an, Shaanxi 710072, China.\\nABSTRACT\\nFall detection based on embedded sensor is a practical and popular research direction in recent\\nyears. In terms of a specific application: fall detection methods based upon physics sensors such as\\n[gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them\\nin machine learning models like Markov chain or just threshold based classification methods. In this\\npaper, we build a complete system named TSFallDetect including data receiving device based on\\nembedded sensor, mobile deep-learning model deploying platform, and a simple server, which will\\nbe used to gather models and data for future expansion. On the other hand, we exploit the sequential\\ndeep-learning methods to address this falling motion prediction problem based on data collected by\\ninertial and film pressure sensors. We make a empirical study based on existing datasets and our\\ndatasets collected from our system separately, which shows that the deep-learning model has more\\npotential advantage than other traditional methods, and we proposed a new deep-learning model\\nbased on the time series data to predict the fall, and it may be superior to other sequential models in\\nthis particular field.\\nKeywords Fall detection ·Embedded sensor ·Deep learning\\n1 Introduction\\nAccording to the World Health Organization, the number and proportion of people aged 60 years and older in the\\npopulation is increasing. In 2019, the number of people aged 60 years and older was 1 billion. This number will\\nincrease to 1.4 billion by 2030 and 2.1 billion by 2050. With the decline of physical function, the probability of falling\\nin the elderly increases [33]. The World Health Organization estimates that 28% to 35% of older adults (>=65 years)\\nhave at least one fall per year. If you can’t get up after falling on the ground for more than 1 hour, this is called long\\nlye[32]. Long lye is dangerous for the elderly, which can lead to illness and even death. In China, accidental falls are\\nalready the leading cause of fatal and non-fatal injuries among people aged 65. How to prevent falls in time and conduct\\neffective early warning or notification after falls to avoid subsequent injuries is a technical problem that needs further\\nresearch.\\nFor many years, research activity has focused on two main areas related to falls, namely prevention and detection [21].\\nThese two areas are related to each other [22] because effective fall prevention requires accurate fall detection. In terms\\nof fall detection methods, several methods are proposed, which can be mainly divided into wearable and non-wearable\\nsolutions.\\nNon-wearable solutions primarily utilize environmental sensors for fall detection. However, regardless of any specific\\nsensing technology used, there are some limitations to environmental sensor-based systems. They are generally suitable\\nfor indoor spaces, and in most cases, they are more expensive and require more computing resources. In addition, the\\nvision sensor may be affected by light conditions and field of view.\\nThe advantages of wearable devices are more prominent and suitable in terms of calculation and development cost.\\nIn addition, when a wearable sensor is used to detect a fall, the position of the sensor on the human body will affect\\nthe [detection ability] of the system [29]. Barshan et al. [28] fitted the six wireless sensor units tightly with special\\n∗Citation :Corresponding author: lyj@nwpu.edu.cnarXiv:2403.06994v1  [eess.SP]  29 Feb 2024' metadata={'source': './data/2403.06994.pdf', 'page': 0}\n",
            "page_content='Running Title for Header\\nstraps to the subjects’ head, chest, waist, right wrist, right thigh, and right ankle. And the use of waist sensor units [30]\\ngenerally results in the best accuracy of fall detection. The foot sensor is a special type of wearable sensor that can\\nperform accurate motion capture and provide a better human wear experience. Once the design is complete, there is no\\nneed for any external transceivers in indoor and outdoor environments. This way, users can walk comfortably without\\nbeing affected by the fall detection system. Based on the above analysis, we designed a fall detection system based on\\nsensors deployed in the foot.\\nWe built a set of fall detection system, including an embedded sensing hardware device with data collection and\\ntransmission functions, a mobile client with data receiving, running fall detection model and data preservation functions,\\nand an experimental server with cloud storage functions. On the basis of this, we collected fall data (pressure\\nacceleration, foot angular speed, etc., when falling or walking normally). Meanwhile, we designed a fall detection\\nmodel FallSeqTCN. Based on the sequence data, we trained the model and got a fall classifier, and deployed it in the\\nMobile Client. At the same time, the data generated in the process of using the APP will be uploaded to the server built\\nby us as a historical data backup.\\nIn the specific model construction, we propose a simple binary classification model for falls and non-falls based on\\nTCN, FallSeqTCN, which is a time series prediction network based on extended convolution. It has a special structure\\nof expansive causal convolution, which can effectively capture a long period of historical information before the fall. At\\nthe same time, we add residual structure to the network to deal with the too deep network structure, which also makes\\nour model have a good generalization ability. The results show that our fall classifier is better. Finally, we deploy the\\nwhole set of models on the system and realize the whole set of fall detection system with practical significance.\\nIn short, our deep learning fall detection system based on physical sensor has demonstrated reliable results in detecting\\nfalls accurately with no false alarms. Our system can potentially be used as an effective and low-cost fall detection\\nsystem, enabling healthcare professionals to respond early and quickly when incidents occur. In summary, our\\ncontributions are as follows.\\n1)We designed and constructed a set of human motion data intelligent sensing system. The sensor data acquisition\\ndevice adopts a wearable scheme, which can transmit raw data in real time. The mobile phone application\\ncan not only analyze the motion parameters in real time, but also display, store and visualize the parsed data.\\nFinally, we stored the uploaded data and updated the model through the Minio server. The overall system can\\nnot only analyze the state of human movement without affecting people’s normal actions, but also detect falls\\nat a higher resolution and early response during an accident.\\n2)We propose a simple binary classification model for falls and non-falls based on TCN, FallSeqTCN. It has a\\nspecial extended causal convolution and residual structure, which not only allows us to make more efficient\\nuse of the captured long historical information before the fall, but also enables our model to have good\\ngeneralization ability.\\n3)We verify the reliability of FallSeqTCN model on two public data sets and conduct training tests on the actual\\ndata sets. By analyzing the data and adjusting the data set, we get a reliable fall classifier, which is very\\nsuitable for the actual fall detection equipment, and there is no missing judgment.\\nThe structure of this article is as follows. In the second section, we summarize the existing implementation methods\\nand literature of the fall detection system, analyze the existing problems and put forward the corresponding solutions.' metadata={'source': './data/2403.06994.pdf', 'page': 1}\n",
            "page_content='and literature of the fall detection system, analyze the existing problems and put forward the corresponding solutions.\\nAt the same time we make a brief description of our system in this section. In the third section, we mainly describe the\\nspecific implementation methods of each module, including hardware platform, APP and server. The fourth section\\nintroduces deep learning model used in this system. The fifth section discusses the data collection and evaluation results\\nof the model. Finally, the sixth section summarizes the main work.\\n2 Related works\\n2.1 Existing related detection systems\\nMany studies have proposed the use of vision sensors as a solution. Frequently seen vision sensors include depth camera\\nsensors [6] [7] and radar sensors [23]. Depth camera sensors can provide three-dimensional information about the\\nscene, including distance and depth data, to better understand the state of objects and human bodies in the environment.\\nIn addition to this, several other solutions have been proposed, such as acoustic signal-based solutions [24] and smart\\nflooring embedded with pressure-sensitive fibers [25].\\nOn the contrary, with the support of the development of Microelectromechanical systems (MEMS), the advantages of\\nwearable devices are more prominent and suitable in terms of [calculation and development cost]. For these reasons,\\nthey provide a viable means for scalable health monitoring of the elderly [26] [27].\\n2' metadata={'source': './data/2403.06994.pdf', 'page': 1}\n",
            "page_content='Running Title for Header\\nAlthough the use of waist sensor units [30] generally results in the best accuracy of fall detection, El-Bendary et\\nal., however, emphasize that the use of wearable sensors may affect [user acceptance] [31], especially when the\\nsensor or its location is sensitive. Previous studies have explored the potential of using machine-assisted fall detection\\nin a variety of domains, including vision-based fall detection using camera sensors [6] [7] [11] [13] [14] [16],\\nfall detection based accelerometer using smartphones [12], and inertial measurement unit data based fall detection\\nmethods [17] [18] [19] [20].\\nFor vision based fall detection, the utilization of Convolutional Neural Networks (CNNs) was prominent. Two notable\\nstudies [13] [14], incorporated depth camera data as a key input. Paper [13] introduced a fall detection system employing\\nvideo frames captured by Kinect RGB depth cameras. CNNs were used to distinguish between Activities of Daily Living\\n(ADL) and fall events. The dataset consisted of 21,499 images collected from various individuals in different indoor\\nenvironments. The dataset was divided into training and testing sets in a 73-27 ratio, yielding an overall accuracy of\\n74%. In the other paper [13] focused on extracting human body shape deformity features using CNNs directly on frame\\nimages. The URFD dataset was used, and the system’s performance was evaluated through 10-fold cross-validation,\\nachieving an average sensitivity of 100%, specificity, and accuracy of 99.98%. However, it’s worth noting that the\\nsystem’s performance may be affected by limited background and foreground variations in the dataset. Additionally, a\\nnovel approach, MyNet1D-D, proposed by Tsai and Hsu [16], introduced a robust one-dimensional CNN architecture\\nfor transforming depth image data into skeleton information, emphasizing computational efficiency and suitability for\\nembedded systems.\\nIn the realm of sensor-based fall detection, Casilari et al. Paper [17] presented a fall detection system based on deep\\nCNNs. Their system identified fall events by recognizing patterns in three-axis accelerometer data. It incorporated an\\nextensive dataset that encompassed up to 14 publicly available datasets, including MobiAct, SisFall, MobiFall, UniMiB\\nSHAR, and UP-Fall. Meanwhile, paper [18] harnessed the power of CNNs and Long Short-Term Memory networks\\n(LSTMs) to perform feature extraction on time-series accelerometer data. This approach outperformed a combination\\nof Support Vector Machine (SVM) and CNN-based methods. The method introduced in the paper [19] introduced a\\nmethod for implementing fall detection with Recurrent Neural Network (RNN) architectures, including LSTMs, making\\nit compatible with microcontroller units (MCUs) equipped with three-axis accelerometers. This approach builds upon\\nprevious work by Theodoridis et al. [20] on RNN-based fall detection, demonstrating the capability of handling and\\nencoding sequential data obtained from body-worn accelerometers.\\n2.2 Problems\\nBased on the analysis of the existing research, we present the following questions.\\n1)Fall detection solutions have suffered from a lack of reliable fall detection algorithms that can accurately and\\nreliably detect falls, particularly those involving dynamic movement changes and combinations of movements.\\nThis is due in part to the difficulty of modelling static poses with static models, which are insufficient to\\ncapture changes in dynamic body configurations and movements during falls.\\n2)Traditional wearable sensors, such as waist sensors and leg sensors, tend to cause great discomfort when users\\nwear them, which has affected the normal life of users. In addition, the strange eyes brought by different\\nclothes are also a reason why users are not willing to use.\\n3)Deep learning models are sensitive to noise and undesired signals, which may lead to biases and inaccuracies' metadata={'source': './data/2403.06994.pdf', 'page': 2}\n",
            "page_content='clothes are also a reason why users are not willing to use.\\n3)Deep learning models are sensitive to noise and undesired signals, which may lead to biases and inaccuracies\\nin the detection of falls. Traditional sensors and threshold-based systems may be prone to a variety of noise\\nsources, such as extraneous vibrations and other distorting effects, leading to inaccurate detection.\\n3 System construction\\n3.1 Overview of our system\\nIn response to challenges discussed above, we propose a physical sensor-based deep learning fall detection system and\\nmake a prototype of the system. You can see this in the 1. It mainly consists of three parts: data acquisition device,\\nmobile client APP and an experimental server. The sensor data acquisition device mainly initializes the sensor and\\ntransmits the original data in real time. The mobile application is to realize the mobile phone BLE receiving library,\\nUI interactive viewing and operation of connected devices. At the same time, we can analyze the motion parameters\\ntransmitted by the hardware data acquisition system to the smart phone in real time on the APP, and store and visualize\\nthe parsed data. The Minio server is used to save data and update models.\\n3' metadata={'source': './data/2403.06994.pdf', 'page': 2}\n",
            "page_content='Running Title for Header\\n1)Data Acquisition Device We use physical sensors including accelerators, gyroscopes and pressure sensors to\\nmeasure the pressure, acceleration and orientation generated during motion. The data sensing device uses the\\ndata collected by these sensors to send the data to the mobile phone APP through Bluetooth for decoding.\\n2)Mobile Client Application The mobile APP is based on the Android system platform. The main thread is\\nresponsible for registering the threads executing specific functions into the corresponding thread pool (queue),\\nwhich mainly includes data receiving, visualization and model prediction pipelines, data uploading and model\\ndownloading sub-pipelines.\\n3)Data processing server We see it as a data shelter, mainly used to store historical movement data generated\\nby users over time.\\nFigure 1: System Structure\\n3.2 Data Acquisition Device\\nFigure 2: Data Acquisition Device\\nIn view of the data uniformity and inaccuracy brought by a single sensor, we integrate multiple sensors based on\\nSTM32F103 MCU platform, including pressure sensor (FSR), voltage conversion module, acceleration, angular\\nvelocity and gyro sensor (IMU901) and Bluetooth communication module (ATK-BLE) . The overall prototype system\\narchitecture design is shown in figure 2. The data acquisition system is designed as two independent data sources, each\\ndeployed on the left and right foot, which is taken into account the user experience.\\nSee the figure 4. The system uses Bluetooth protocol as an interface to communicate instructions and exchange\\ndata between embedded sensors platform and Android mobile platform. The STM32 chip platform’s firmware can\\ncommunicate with PC through COM port for serial debugging and writing. The following is a brief introduction to\\nthe development platform of the data acquisition system, as well as each component. STM32 MCU adopts ARM\\narchitecture. In figure 3, the main loop of the program includes initialization and setting of each module, including GPIO\\ninitialization, timer interrupt, UART communication frequency convention, sending and receiving specific instructions\\nfor communication, and analyzing data structure serialization. Read the values of pressure, acceleration, angular\\n4' metadata={'source': './data/2403.06994.pdf', 'page': 3}\n",
            "page_content='Running Title for Header\\nFigure 3: Data Acquisition Interaction\\nvelocity and altitude angles, and send the timing data to the mobile phone through the Bluetooth module for further data\\ncollection and processing.\\nConsidering the compatibility of the model with the device, we chose a similar sensor on the later wearable sensor.\\nThe acquired signals include plantar pressure values, acceleration values, angular velocity values, and azimuth values.\\nThe embedded device is worn on the subject’s left and right feet, and the sensor signals of the subject in the normal\\nwalking and falling state including forward fall and left fall are collected. The sampling frequency is 18HZ, and the\\nsignal collected each time includes 20 features of plantar pressure values, acceleration values, angular velocity values\\nand azimuth on the left and right feet, constituting the initial time series dataset. In the same experimental environment,\\nthe subjects carried out 11 normal walks, 10 forward falls, and 5 left falls for a total of 26 experiments, which were\\nsorted into 26 TXT files. The 20 characteristic values collected in each experiment are shown in Table 1.\\nHere we also introduce two similar public data sets. UMAFall is a new dataset of movement traces acquired through the\\nsystematic emulation of a set of predefined ADLs (Activities of Daily Life) and falls, as shown in figure 5. In opposition\\nto other existing databases for FDSs, which only include the signals captured by one or two sensing points, the testbed\\ndeployed for the generation of UMAFall dataset incorporated five wearable sensing points, which were located on five\\ndifferent points of the body of the participants that developed the movements. We trained and tested the model on this.\\n5' metadata={'source': './data/2403.06994.pdf', 'page': 4}\n",
            "page_content='Running Title for Header\\nFigure 4: Our dataset obtained using this foot sensor group\\nTable 1: Signal characteristics acquired by embedded devices\\nFeature Meaning Feature Meaning\\nl_voltage_ao The voltage value of the left foot r_voltage_ao The voltage value of the right foot\\nL_attitude_roll Azimuth around the x-axis on the left\\nfootr_attitude_roll Azimuth around the x-axis on the right\\nfoot\\nL_attitude_pitch Azimuth around the y-axis on the left\\nfootr_attitude_roll Azimuth around the y-axis on the right\\nfoot\\nL_attitude_yaw azimuth around the z-axis on the left\\nfootr_attitude_roll azimuth around the z-axis on the right\\nfoot\\nL_acc_x acceleration value on the x-axis on the\\nleft footr_acc_x acceleration value on the x-axis on the\\nright foot\\nL_acc_y acceleration value on the y-axis on the\\nleft footr_acc_y acceleration value on the y-axis on the\\nright foot\\nL_acc_z acceleration value on the z-axis on the\\nleft footr_acc_z acceleration value on the z-axis on the\\nright foot\\nL_gyro_x The angular velocity value on the x-\\naxis on the left footr_gyro_x The angular velocity value on the x-\\naxis on the right foot\\nL_gyro_y The angular velocity value on the y-\\naxis on the left footr_gyro_y The angular velocity value on the y-\\naxis on the right foot\\nL_gyro_z The angular velocity value on the z-\\naxis on the left footr_gyro_z The angular velocity value on the z-\\naxis on the right foot\\nThe TST FB4FD dataset was measured by a smart shoe, as shown in figure 6, which is equipped with three Force\\nSensing Resistors (FSR) and a three-axis accelerometer, as well as a processing unit board. The latter analyzes the gait\\ncycle phase, distinguishes between falls and non-falls, and transmits data remotely.\\n3.3 Mobile Client Application\\nThe concurrency problem caused by multi-device connection must be considered in the process of software design, and\\nthe Message-handler mechanism provided by the Android system platform can handle this problem well. Message\\nrepresents an action or a sequence of actions, and each message has a specific target Handler when it is added to the\\nmessage queue. Handler is the actual handler of the message. Handler allows Message and anonymous function objects\\nassociated with a thread’s message queue to be sent and processed. Each Handler instance is associated with a message\\nqueue for a thread. Creating a Handler binds the Handler to a Looper. It can pass messages and anonymous function\\nobjects to Looper’s message queue and execute them in Looper’s thread. Handler has two main uses: (1) Schedule\\nmessages and anonymous function objects for execution at some point in the future; (2) Execute the action on a different\\nthread.\\n6' metadata={'source': './data/2403.06994.pdf', 'page': 5}\n",
            "page_content='Running Title for Header\\nFigure 5: Basic architecture of the system\\nFigure 6: The TST FB4FD dataset obtained using the foot sensor group\\nThe figure 7 shows the process architecture of this design. The mobile APP we designed is based on the Android\\nsystem platform, and the main thread is responsible for registering the thread executing specific functions into the\\ncorresponding thread pool (queue). There are mainly data receiving, visualization and model prediction pipelines, data\\nupload and model download sub-pipelines. When the corresponding tasks are completed, the thread will encapsulate\\nthe results into Message and return to the main thread by Handler. Model prediction is supported by the Tensorflow Lite\\nlibrary.\\nIn order to facilitate the user’s visual operation, we try our best to ensure the simplicity of the software, which is\\nachieved on the basis of ensuring the integrity of fall detection and early warning functions. The first is the ease of\\ndevice connection, as shown in Figure 8, and users simply click the Connect button. [This is because after implementing\\nthe BLE connection library, we performed a registration scan connection callback and put the scanned devices into the\\nlist view adapter.]\\nThe form of data is also a problem that we focus on. On the one hand, our data should be convenient for subsequent\\nmodel training, and on the other hand, these data can clearly describe the current motion state of users. Although\\nreal-time data reception is realized through the Bluetooth connection library, the Bluetooth connection library always\\nFigure 7: The framework diagram of APP design\\n7' metadata={'source': './data/2403.06994.pdf', 'page': 6}\n",
            "page_content='Running Title for Header\\nFigure 8: Device connection\\nreceives byte fragments originally due to the byte limitation of BLE sending data. Proper data parsing and serialization\\nbecame the top priority, and we implemented data processing for the data sink class. The data sink class maintains\\nan internal, concurrently secure string cache queue, lines_queue, for parsed rows of data. The receiveData method\\naccepts the raw byte type as a parameter, and is used to continuously concatenate the received byte fragments into the\\nstring cache, and truncate the byte fragments at the newline when the byte fragments contain newlines. The first half of\\nthe byte fragments is concatenated into the global string cache queue after the string cache is concatenated, and the\\nsecond half is used to initialize the string cache. The parseData method uses an asynchronous callback scheme, when\\nlines_queue is not empty, the first line string (a set of raw data) is removed from the queue, and parsed into an internally\\ndefined BleUartData type, and marked isParsed as true, passed to a callback function initialized in the real application.\\nIt is used to perform the next operation on the parsed data. The data sink class provides an asynchronous callback\\nfunction Interface after the data is parsed. We initialized the data parser in the onCreate function of the application\\ninterface portion of the code, and placed the receiveData and parseData calls in the main thread queue after successfully\\nsetting Notify to get the new raw data bytes. Finally, we set the file saving path and immediately pass in a sequence of\\nbytes to save the data to the file. After testing, the frequency of data collection can be 12-80Hz. In the figure 9, we can\\nsee the storage format of the data.\\nFigure 9: Data storage\\n8' metadata={'source': './data/2403.06994.pdf', 'page': 7}\n",
            "page_content='Running Title for Header\\nTo keep the interface simple, we use the open source ECharts library for data visualization.When the returned call data\\nobtains the parsed data object, it displays the data in String format on the interface, and uses the parsed data tensor as\\ninput to run the deep learning model to obtain the motion state results corresponding to the motion data. These results\\ncan be visualized via EChartsView or used for motion state alarms.\\nIn addition, to ensure that the model runs efficiently on devices with limited compute and memory resources, and to\\nenable devices to run machine learning models offline, we use Tensorflow Lite as a set of tools to implement end-to-end\\nworkflows. TensorFlow Lite can be seen as consisting of two main parts: a converter that compresses and optimizes the\\nmodel, converting it to.tflite format; a set of interpreters for various runtimes. [Tensorflow Lite]\\n3.4 Data Processing Server\\nThe system we developed generates a large amount of unstructured data during use, such as log files of wearer walking,\\nbackup algorithm models, and so on. For the user’s experience, we adopted the method of incremental training, for\\nwhich we built a MinIO server in the figure 10.\\nMinIO is an object storage service based on Apache License v2.0 open source protocol. Compatible with the Amazon\\nS3 cloud storage service interface, it is ideal for storing large volumes of unstructured data, such as images, videos,\\nlog files, backup data, and container/virtual machine images, and an object file can be any size, from a few kb to a\\nmaximum of 5T. MinIO is a very lightweight service that can be easily combined with other applications, such as\\nNodeJS, Redis, or MySQL.\\nWe used docker tools to help build minio server. After getting the minio image from the official website, we activated it\\naccording to the specified instructions, then set the user account information, and created the model folder and data\\nfolder. During the subsequent operation, the dynamic data collected by the device will be uploaded here. At the same\\ntime, with the subsequent updates of our team, there will be more detection models to provide for everyone to use.\\nFigure 10: Minio Server\\n4 Models\\n4.1 Fall Data acquizition\\nIn view of the characteristics of time series data, we uses Kalman filter for data denoising, sets the first estimate as the\\ncurrent value, sets the parameters A=1, W(k)=0 (Because the measurement of the parameter error of the equipment is\\nmore complicated, so we ignore this part, and this prediction is more estimated according to the trend of the overall\\ndata) and H=1.\\nX(k|k−1) =AX(k−1|k−1) +BU(k) +W(k) (1)\\nFigure 11 is the three-dimensional azimuth and pressure of the left foot four dimensions of the comparison chart. From\\n(a) can be seen that the abrupt change of the azimuth angle at group 57 has been denoising into a buffer value decline;\\nin (b), the value decline process of group 50 to group 80 is affected by noise ups and downs, and the numerical change\\nafter processing is more smooth in line with the laws of physics; (c) and (d) are the same. Kalman filter can smooth the\\nfluctuation of abnormal data well and make the data distributed according to the trend of the whole data. Overall, we\\ncan find that the denoising effect of the Kalman filter is very good.\\nSince the training data samples are relatively small, in order to improve the recognition accuracy of the training\\nmodel, we can use random transformation methods to enhance the data, such as dithering, flipping, zooming in or out,\\nbending, arranging, sliding windows, etc. These methods are the most direct ways to enhance them. In addition, neural\\nnetwork-based models can be used to acquire time series from feature distributions to generate new time series data.\\nCommonly used neural networks include LSTMs and time CNNs, which can map input sequences directly to output\\nsequences to generate samples that can be fake and authentic.\\n9' metadata={'source': './data/2403.06994.pdf', 'page': 8}\n",
            "page_content='Running Title for Header\\n(a)\\n (b)\\n(c)\\n (d)\\nFigure 11: Plot using the Kalman filter (blue line) versus not using the Kalman filter (red line).(a)Comparison chart of\\nbefore and after processing of roll angles.(b)Pitch angle before and after processing comparison chart.(c)Yaw before\\nand after processing comparison chart.(d)Comparison chart of left foot pressure before and after processing\\nFalling or walking normally is a physical law, so the dataset has to follow that law as well. Therefore, processing\\nmethods such as dithering and flipping are not suitable, which will destroy the integrity and regularity of the data\\nsequence.\\nTherefore, slicing is mainly adopted here, which means a long data series of normal walking is divided into several\\nsegments (I did not segment the fall data here because the fall data itself is relatively short and the fluctuations of the\\nfall data will be concentrated. Segmentation will affect the classification and judgment). Figure 12 is the data sequence\\ndiagram on the pressure dimension of the left foot, you can see that it shows a regular undulating state. Set the first slice\\nbreakpoint in group 25 and set the second slice breakpoint in group 275. And finally we intercept a new data segment\\nwith a length of 250 as shown in figure 13. After performing this operation on all normal walking data, the original\\ndataset was expanded by nearly half, achieving data enhancement.\\nIn this paper, we proposed the model named FallSeqTCNs for Fall Detection.\\n4.2 FallSeqTCN\\nIn figure 14, FallSeqTCN is a binary fall detection model based on Temporal Convolutional Network (TCN) [10]. TCN\\nis a time-series prediction network with dilated convolution, inspired by WaveNet and TCN, and it can process multiple\\n10' metadata={'source': './data/2403.06994.pdf', 'page': 9}\n",
            "page_content='Running Title for Header\\nFigure 12: Schematic diagram of pre-slice data and slice locations\\nFigure 13: Schematic diagram of the data after slicing\\ntemporal sequences simultaneously and require less time at the training time. Like TCN, residual-connected structure is\\nused in FallSeqTCN, and dilated convolution and causal convolution are introduced in each block.\\nThe SDC Block is combined by several dilated 1-dimensional same-length zero-padding convolution networks and a\\nlinear feed-forward network. The residual connection is set between every 3 sequential SDC Blocks. We don’t use the\\nnormalization skill because normalization may reduce the physical representational capacity of the original data. But\\nwe’re still exploring the way that can further exert the physical meaning of the data and the most potential of a more\\ngeneral model to all sensor data.\\nFor the FallSeqTCN model, each input sequence consists of 20 time-domain acceleration and 2 plantar pressure signals\\nsampled at a rate of 18 Hz. Like windowing filtering, we transfer the sequence data to window batches with step\\n1, length 64, then fit batches into the model for training and test. The ratio of train and test is 7:3. The prediction\\nprobability is obtained from softmax layer and finally a binary fall detection result is obtained. To address overfitting\\nissues during training, we also added optional Dropout.\\n5 Experiments\\nIn this section, we present details of our experiment settings and the corresponding results. We conduct comprehensive\\nanalyses and investigations to illustrate the effectiveness of our FallSeqTCN model. We have provided the data and\\ncode of FallSeqTCN along with this submission.\\n5.1 Datasets\\nWe use two datasets to evaluate the performance of FallSeqTCN:\\n1)UMAFall: is collected through the systematic emulation of a set of predefined ADLs (Activities of Daily Life)\\nand falls in 2016.\\n2)Our data: is collected by the left and right foot data awareness device that the subjects performed 11 normal\\nwalking, 10 forward falls and 5 left side falls in the same experimental environment.\\n11' metadata={'source': './data/2403.06994.pdf', 'page': 10}\n",
            "page_content='Running Title for Header\\nFigure 14: FallSeqTCN\\n5.2 Baselines\\nWe compare our FallSeqTCN with the following models to evaluate the effectiveness of our approach:\\n1)SVM: uses SVM algorithm to get the combined acceleration, acceleration and attitude Angle thresholds\\nof classified falls and daily behaviors, and finally reconstructs the prediction algorithm on the single chip\\ncomputer to realize real-time prediction of fall behaviors.\\n2)Decision Tree: uses decision tree to build the mapping relationship between object attributes and object values,\\nand then makes fall prediction..\\n3)LSTM: is based on a single-layer long short-term memory network to make predictions, using large memory\\nto store partial outputs of their multiple cell gates.\\n5.3 Experimental Setup\\nAs a binary classification problem model for normal and fall, this model evaluates the classification results by\\nconstructing a confusion matrix of the classification results of the test set. The four values in the confusion matrix are\\ndefined as T (True) for correct, F (False) for error, P (Positive) for 1, and N (Negative) for 0): TP: The predicted value is\\n1, the actual value is 1, and the prediction is correct. FP: The prediction is 1, but the actual value is 0, and the prediction\\nis wrong. FN: The predicted value is 0, but the actual value is 1, and the prediction is wrong. TN: The prediction is 0,\\nthe actual is 0, and the prediction is correct.\\nAccording to these four indicators, we can use the formula to calculate three performance indicators: Accuracy, which\\nis the percentage of the predicted correct results in the total sample; Precision, which is the probability that all predicted\\npositive samples will actually be positive; Recall, which is the probability that a sample is predicted to be positive in a\\nsample that is actually positive.\\nAccuracy =TP+TN\\nTP+TN +FP+FN×100% (2)\\nPrecision =TP\\nTP+FP×100% (3)\\nRecall =TP\\nTP+FN×100% (4)\\n5.4 Results\\nThrough comparative training, it can be seen that the time convolutional network have a stronger ability to capture the\\ncharacteristics of effective long-term series. The Recall of SeqTCN in self-test data set and UMAFall data set reached\\n83% and 85% respectively, and its F1 scores reached 0.90 and 0.85 respectively.\\n1)Overall Performance: Through comparative training, it can be seen that the time convolutional network have\\na stronger ability to capture the characteristics of effective long-term series.The Recall of SeqTCN in self-test\\n12' metadata={'source': './data/2403.06994.pdf', 'page': 11}\n",
            "page_content='Running Title for Header\\ndata set and UMAFall data set reached 83% and 85% respectively, and its F1 scores reached 0.90 and 0.85\\nrespectively.\\n2)Low Risk: Since this is a fall detection model, recall should be increased as much as possible while the\\nprecision is reasonable. After comparison, F1 scores of TCN are higher, and the recall of both of them reaches\\nmore than 80%, which is the reason why the TCN model is finally selected.\\nTable 2: Model Comparison\\ndataset UMAFall Our data UMAFall Our data UMAFall Our data UMAFall Our data\\nmodel name Accuracy Precision Recall F1 Score\\nSVM 0.76 0.83 1 0.31 0.01 0.83 0.02 0.45\\nDecision Tree 0.91 0.91 0.84 0.1 0.77 0.50 0.80 0.16\\nLSTM 0.76 0.77 1 0.25 0.01 0.83 0.02 0.38\\nSeqTCN (Ours) 0.92 0.98 0.84 1 0.85 0.83 0.85 0.90\\n6 Conclusion\\nThis paper has presented an extensive study on motion capture based on embedded sensors, including gyroscopes,\\naccelerometers and pressure sensors. We built a complete fall detection system named TSFallDetect. We have\\nconducted empirical studies on existing data sets and systematically collected data sets respectively, and the results\\nshow that the model has advantages over traditional methods, which confirms the feasibility and effectiveness of\\nour system. The time convolutional network has a strong ability to capture effective long time series features,\\nwhich confirms the potential of the network for embedded sensor-based motion capture. The code is available at\\nhttps://github.com/WuShaoa/SensorDataClassification-TCN/tree/main/SeqClassifyCNN .\\nReferences\\n[1]G. Kour and R. Saabne, “Real-time segmentation of on-line handwritten arabic script,” in Frontiers in Handwriting\\nRecognition (ICFHR), 2014 14th International Conference on . IEEE, 2014, pp. 417–422.\\n[2]G. Kour and R. Saabne, “Fast classification of handwritten on-line arabic characters,” in Soft Computing and\\nPattern Recognition (SoCPaR), 2014 6th International Conference of . IEEE, 2014, pp. 312–318.\\n[3]G. Hadash, E. Kermany, B. Carmeli, O. Lavi, G. Kour, and A. Jacovi, “Estimate and replace: A novel approach to\\nintegrating deep neural networks with existing applications,” arXiv preprint arXiv:1804.09028 , 2018.\\n[4]E. Casilari, J. A. Santoyo-Ramón, and J. M. Cano-García, “Umafall: A multisensor dataset for the research on\\nautomatic fall detection,” Procedia Computer Science , vol. 110, pp. 32–39, 2017.\\n[5]S. Spinsante, E. Gambi, L. Montanini, D. Perla, and A. Del Campo, “Tst footwear-based dataset for fall detection\\n(tst fb4fd),” 2017. [Online]. Available: https://dx.doi.org/10.21227/H2W01S\\n[6]S. Gasparrini, E. Cippitelli, S. Spinsante, and E. Gambi, “A depth-based fall detection system using a kinect ®\\nsensor,” Sensors , vol. 14, no. 2, pp. 2756–2775, 2014.\\n[7]E. E. Stone and M. Skubic, “Fall detection in homes of older adults using the microsoft kinect,” IEEE Journal of\\nBiomedical & Health Informatics , vol. 19, no. 1, pp. 290–301, 2017.\\n[8]N. Fletcher-Lloyd, A. I. Serban, M. Kolanko, D. Wingfield, D. Wilson, R. Nilforooshan, P. Barnaghi, and E. Soreq,\\n“A markov chain model for identifying changes in daily activity patterns of people living with dementia,” IEEE\\nInternet of Things Journal , vol. PP.\\n[9]S. T. Hsieh and C. L. Lin, “Fall detection algorithm based on mpu6050 and long-term short-term memory network,”\\nin2020 International Automatic Control Conference (CACS) , 2020.\\n[10] A. V . D. Oord, S. Dieleman, H. Zen, K. Simonyan, and K. Kavukcuoglu, “Wavenet: A generative model for raw\\naudio,” 2016.\\n[11] T. Vaiyapuri, E. L. Lydia, M. Y . Sikkandar, V . G. Díaz, I. V . Pustokhina, and D. A. Pustokhin, “Internet of\\nthings and deep learning enabled elderly fall detection model for smart homecare,” IEEE Access , vol. 9, pp.\\n113 879–113 888, 2021.\\n[12] E. Casilari, R. Lora-Rivera, and F. García-Lagos, “A study on the application of convolutional neural networks to\\nfall detection evaluated with multiple public datasets,” Sensors , vol. 20, no. 5, p. 1466, 2020.\\n13' metadata={'source': './data/2403.06994.pdf', 'page': 12}\n",
            "page_content='Running Title for Header\\n[13] K. Adhikari, H. Bouchachia, and H. Nait-Charif, “Activity recognition for indoor fall detection using convolutional\\nneural network,” in 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA) , 2017,\\npp. 81–84.\\n[14] X. Li, T. Pang, W. Liu, and T. Wang, “Fall detection for elderly person care using convolutional neural networks,”\\nin2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics\\n(CISP-BMEI) , 2017, pp. 1–6.\\n[15] N. Lu, Y . Wu, L. Feng, and J. Song, “Deep learning for fall detection: 3d-cnn combined with lstm on video\\nkinematic data,” IEEE Journal of Biomedical and Health Informatics , vol. PP, pp. 1–1, 02 2018.\\n[16] T.-H. Tsai and C.-W. Hsu, “Implementation of fall detection system based on 3d skeleton for deep learning\\ntechnique,” 2019 IEEE 8th Global Conference on Consumer Electronics (GCCE) , pp. 389–390, 2019. [Online].\\nAvailable: https://api.semanticscholar.org/CorpusID:211686877\\n[17] E. Casilari-Pérez, R. Lora-Rivera, and F. García-Lagos, “A study on the application of convolutional neural\\nnetworks to fall detection evaluated with multiple public datasets,” Sensors (Basel, Switzerland) , vol. 20, 2020.\\n[Online]. Available: https://api.semanticscholar.org/CorpusID:212666988\\n[18] J. Xu, Z. He, and Y . Zhang, “Cnn-lstm combined network for iot enabled fall detection applications,”\\nJournal of Physics: Conference Series , vol. 1267, no. 1, p. 012044, jul 2019. [Online]. Available:\\nhttps://dx.doi.org/10.1088/1742-6596/1267/1/012044\\n[19] E. Torti, A. Fontanella, M. Musci, N. Blago, D. P. Pau, F. Leporati, and M. Piastra, “Embedded real-time fall\\ndetection with deep learning on wearable devices,” 2018 21st Euromicro Conference on Digital System Design\\n(DSD) , pp. 405–412, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:52985178\\n[20] T. Theodoridis, V . Solachidis, N. Vretos, and P. Daras, “Human fall detection from acceleration measurements\\nusing a recurrent neural network,” 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:196017607\\n[21] Y . Delahoz and M. Labrador, “Survey on fall detection and fall prevention using wearable and external sensors,”\\nSensors , vol. 14, no. 10, p. 19806, 2014.\\n[22] Kabalan, Chaccour, Rony, Darazi, Amir, Hajjam, El, Hassani, Emmanuel, and Andrès, “From fall detection to fall\\nprevention: A generic classification of fall-related systems,” IEEE Sensors Journal , 2017.\\n[23] E. Cippitelli, F. Fioranelli, E. Gambi, and S. Spinsante, “Radar and rgb-depth sensors for fall detection: A review,”\\nIEEE Sensors Journal , pp. 3585–3604, 2017.\\n[24] M. S. Khan, M. Yu, P. Feng, L. Wang, and J. Chambers, “An unsupervised acoustic fall detection system using\\nsource separation for sound interference suppression,” Signal Processing , vol. 110, no. C, pp. 199–210, 2015.\\n[25] G. Feng, J. Mai, Z. Ban, X. Guo, and G. Wang, “Floor pressure imaging for fall detection with fiber-optic sensors,”\\nIEEE Pervasive Computing , vol. 15, no. 2, pp. 40–47, 2016.\\n[26] A. G. A. B, “Wearables for independent living in older adults: Gait and falls,” Maturitas , vol. 100, pp. 16–26,\\n2017.\\n[27] Mukhopadhyay and S. Chandra, “Wearable sensors for human activity monitoring: A review,” IEEE Sensors\\nJournal , vol. 15, no. 3, pp. 1321–1330, 2014.\\n[28] A. Özdemir and B. Barshan, “Detecting falls with wearable sensors using machine learning techniques.” Sensors ,\\nvol. 14, no. 6, p. 10691, 2014.\\n[29] P. Ntanasis, E. Pippa, A. T. Zdemir, B. Barshan, and V . Megalooikonomou, “Investigation of sensor placement for\\naccurate fall detection,” in International Conference on Wireless Mobile Communication and Healthcare , 2016.\\n[30] O. Ahmet, “An analysis on sensor locations of the human body for wearable fall detection devices: Principles and\\npractice,” Sensors (Basel, Switzerland) , vol. 16, no. 8, 2016.' metadata={'source': './data/2403.06994.pdf', 'page': 13}\n",
            "page_content='[30] O. Ahmet, “An analysis on sensor locations of the human body for wearable fall detection devices: Principles and\\npractice,” Sensors (Basel, Switzerland) , vol. 16, no. 8, 2016.\\n[31] N. El-Bendary, Q. Tan, F. C. Pivot, and A. Lam, “Fall detection and prevention for the elderly: A review of trends\\nand challenges,” International Journal on Smart Sensing & Intelligent Systems , vol. 6, no. 3, pp. 1230–1266, 2013.\\n[32] L. Day, “Falls in older people: Risk factors and strategies for prevention.” age & ageing , 2007.\\n[33] W. H. Organization, 2023, https://www.who.int/health-topics/ageing/, Last accessed on 2023-10-25.\\n14' metadata={'source': './data/2403.06994.pdf', 'page': 13}\n",
            "Processing  ./data/2403.06983.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='Asymptotic estimations of a perturbed symmetric eigenproblem\\nArmand Gisslera,∗, Anne Augera, Nikolaus Hansena\\naInria, CMAP , CNRS, ´Ecole polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France\\nAbstract\\nWe study ill-conditioned positive definite matrices that are disturbed by the sum of mrank-one matrices of a specific\\nform. We provide estimates for the eigenvalues and eigenvectors. When the condition number of the initial matrix\\ntends to infinity, we bound the values of the coordinates of the eigenvectors of the perturbed matrix. Equivalently, in\\nthe coordinate system where the initial matrix is diagonal, we bound the rate of convergence of coordinates that tend\\nto zero.\\nKeywords: Perturbation of symmetric matrices, estimation of eigenvectors\\n2020 MSC: 15A42, 15B57\\n1. Introduction\\nGiven a d×dsymmetric matrix with known eigenvectors and eigenvalues denoted B, and a rank-one matrix vvT\\nwhere v∈Rd, eigenvalues and eigenvectors of matrices of the form\\nA=B+vvT(P1)\\nhave been widely studied, notably in the context of perturbation theory. For instance, the eigenvalues of ( P1) can be es-\\ntimated and a formula for the eigenvectors is known [1, 2, 3]. Specifically, if Bis a diagonal matrix diag( λ1,λ2,...,λ d)\\nwith distinct eigenvalues and vhas only nonzero entries, then the component jof the unit eigenvector associated to\\neigenvalueνiof the updated matrix Asatisfies the so-called Bunch-Nielsen-Sorensen formula\\nCi×[v]j\\nλj−νifori,j=1,..., d (1)\\nwhere Ciis a nonzero normalization constant and [ .]jdenotes the j-th coordinate, a notation we will continue to\\nuse in the sequel. Several results have been established for additive perturbations of rank 1 [4, 5] and of higher rank\\n[6, 7]. Symmetric and nonsymmetric perturbation eigenvalue problems have been studied [8] as well as perturbation\\nresults for invariant subspaces [9]. In this paper, we provide relative perturbation bounds for the eigenvectors of\\npositive definite matrices. In contrast to previous relative perturbation results for eigenvalues [10] and invariant\\nsubspaces [11, 12], the bounds in our result depend on the eigenvalues of the initial matrix Brather than the norm of\\nthe perturbation, see Eq. (2) below.\\nSpecifically, we consider the perturbation with a sum of mrank-one matrices of the form\\nA(m)=B+√\\nBmX\\ni=1[v(i)][v(i)]T√\\nB (Pm)\\nwith B=Pdiag(λ1,...,λ d)PTwhere Pis an orthogonal matrix, λ1⩾···⩾λd>0 are the eigenvalues of B, and\\nv(1),..., v(m)∈Rd. The square root√\\nB:=Pdiag(√λ1,...,√λd)PTis defined as the unique symmetric positive\\n∗Corresponding author\\nEmail addresses: firstname.lastname@polytechnique.edu (Armand Gissler), firstname.lastname@inria.fr (Anne Auger),\\nfirstname.lastname@inria.fr (Nikolaus Hansen)\\nPreprint submitted to Applied Mathematics Letters March 13, 2024arXiv:2403.06983v1  [math.NA]  10 Jan 2024' metadata={'source': './data/2403.06983.pdf', 'page': 0}\n",
            "page_content='definite matrix such that√\\nB×√\\nB=B, see e.g. [13, Theorem 7.2.6]. Matrices of the form ( Pm) are used in\\nvarious applications in di fferent domains. For instance, low rank updates of covariance matrices are used in stochastic\\noptimization [14, 15], system identification [16, p. 369], and adaptive Markov Chain Monte Carlo methods [17]. Our\\nmotivation is to study the eigenvectors of Ain (Pm), denoted as e(m)\\niin the sequel, when the matrix B is highly ill-\\nconditioned . When d=2 and m=1 we can compute the eigenvectors of A(1)explicitly. As an example, consider\\nB=diag(λ1,1) whereλ1>1 and v(1)=[1,1]T. Then, the unit eigenvector associated to the largest eigenvalue of\\nA(1)obeys√\\n1+s2×e(1)\\n1=[1,s]Twith s=λ1/2\\n1×(1−λ−1\\n1−q\\n1−λ−1\\n1+λ−2\\n1)=−λ−1/2\\n1/2+O(λ−3/2\\n1) and hence\\n[e(1)\\n1]2=λ−1/2\\n1+o(λ−1/2\\n1) whenλ1→∞ . Hence, the (second) coordinate of the (first) unit eigenvector of A(1)vanishes\\nlike 1/√λ1whenλ1→∞ . In this paper, we generalize this result to the case where d⩾2 and m⩾1, as summarized\\nin the following theorem which directly follows from Theorem 4 below.\\nTheorem 1. If e(m)\\niis a unit eigenvector corresponding to the i-th largest eigenvalue (counted with multiplicity) of\\nA(m)in(Pm)and e(0)\\njis a unit eigenvector corresponding to the j-th largest eigenvalue λjof B, then\\n\\x0c\\x0c\\x0c\\x0cD\\ne(m)\\ni,e(0)\\njE\\x0c\\x0c\\x0c\\x0c⩽Cm×s\\nmin{λi,λj}\\nmax{λi,λj}(2)\\nwhere C m>0is a constant which depends polynomially on d and max k=1,...,m∥v(k)∥.\\nWhen Bis diagonal, e(0)\\njis the j-th canonical unit vector. Hence |⟨e(m)\\ni,e(0)\\nj⟩|=[e(m)\\ni]jand the theorem implies\\nin particular that the j-th coordinate of e(m)\\niconverges to zero at least as fast aspmin{λi,λj}/max{λi,λj}when the\\nlatter tends to 0 (which is tight in the above example when d=2 and m=1), thereby limiting the change of the\\nangle between these eigenvectors. Considerations on the angle between eigenspaces have been made previously [18],\\nhowever matrices on the form of ( Pm) have not been studied in this context. In the remainder, we always choose\\nw.l.o.g. the coordinate system where the matrix Bof (Pm) is diagonal and has decreasingly ordered diagonal values.\\nThis inequality is crucial to study the stability of a Markov chain underlying the CMA-ES algorithm [19, 15].\\nProofs of linear convergence for Evolutionary Strategies (ES) rely on a drift condition [20, Theorem 17.0.1] to prove\\nthe ergodicity of an underlying Markov chain, see e.g. [21, 22]. To apply this approach to CMA-ES, a potential\\nfunction is defined on the state-space of this Markov chain and its expected decrease is proven outside a compact set.\\nThe state space includes a covariance matrix, updated as\\nCt+1=(1−c)Ct+cp\\nCtmX\\ni=1wiUiUT\\nip\\nCt, (3)\\nwhere c∈[0,1],w1,..., wmare positive weights that sum to 1, and the vectors Ui,i=1,..., m, are Gaussian vectors\\nranked according to a fitness function [15, Eq. (11)]. Hence, ( Pm) encompasses the update of this covariance matrix.\\nEq. (2) is needed to bound the expected condition number of the updated covariance matrix, since it controls the\\ninfluence of small eigenvalues on the growth of the largest eigenvalues.\\nThis paper is organized as follows. In Section 2, we study the eigenvalues of ( Pm). In Section 3, we provide\\nbounds for the coordinates of the eigenvectors using Eq. (1), and provide an empirical result suggesting that these\\nbounds are tight.\\n2. Bounds on the eigenvalues of (Pm)\\nThe Bunch-Nielsen-Sorensen formula (1) which we will use in Section 3 requires the eigenvalues of the updated\\nmatrix. Thus, we first derive bounds on the (decreasingly ordered) eigenvalues\\nλi(A(m))= max\\nV⊂Rd,dimV=imin\\nv∈V,v,0vTA(m)v\\nvTv= min\\nV⊂Rd,dimV=d−i+1max\\nv∈V,v,0vTA(m)v\\nvTvfori=1,..., d (4)\\nwhere the equalities ensue from the min-max principle and from Gersgorin’s circle theorem [13, Theorems 4.2.6 and\\n6.1.1].\\n2' metadata={'source': './data/2403.06983.pdf', 'page': 1}\n",
            "page_content='Theorem 2.7 in [10] and Theorem 2.1 in [23, p. 175] provide an estimation for the eigenvalues of ( Pm):\\nλi⩽νi⩽λi× \\n1+md×max\\nk=1,...,m∥v(k)∥2\\n∞!\\nfori∈{1,..., d}. (5)\\nThe next lemma provides a slightly tighter upper bound on these eigenvalues after a single rank-one pertubation\\n(m=1) and is used in Proposition 3.\\nLemma 2. Let D =diag(λ1,...,λ d)be a diagonal matrix with λ1>···>λ d>0. Let v∈Rd\\n,0be a vector with only\\nnonzero entries. Let A =D+√\\nDvvT√\\nD andν1⩾ν2⩾···⩾νddenote the eigenvalues of A. Then,\\nνi⩽λi×\\x10\\n1+(d−i+1)∥v∥∞|[v]ji|\\x11\\nfor all i∈{1,..., d} (6)\\nwhere j i∈Arg maxj=i,...,d\\x1a\\n|[v]j|\\x0c\\x0c\\x0cλj⩾λi×\\x12\\n1−q\\nλj\\nλi(d−i+1)∥v∥∞|[v]j|\\x13\\x1b\\n.\\nProof. Fixi∈{1,..., d}and remark that, by Eq. (4), we have νi⩽maxv∈¯Vi,∥v∥=1vTAv=λ1\\x00[A]i:d,i:d\\x01,where ¯Vi=\\nVect( ei,..., ed) with eibeing the ithvector of the standard basis of Rd, and with [ A]i:d,i:ddenoting the submatrix of A\\nfrom rows and columns with indices between ianddincluded. But, by [13, Theorem 6.1.1], we also have that\\nλ1\\x00[A]i:d,i:d\\x01⩽max\\nj=i,...,d\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8eddX\\nk=i|[A]j,k|\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8Cmax\\nj⩾iBj.\\nSince A=D+√\\nDvvT√\\nD, then|[A]j,k|⩽pλjλk(1{j=k}+∥v∥∞|[v]j|). If j⩾iis such that λj⩾λi×(1−pλj/λi(d−\\ni+1)∥v∥∞|[v]j|), then by definition of jiwe have then|[v]j|⩽|[v]ji|, yielding to Bj⩽λi×(1+(d−i+1)∥v∥∞|[v]ji|).\\nAny other j⩾isatisfiesλj< λ i−pλjλi(d−i+1)∥v∥∞|[v]j|,hence by sum Bj⩽λi. All in all, max j⩾iBj⩽\\nλi×(1+(d−i+1)∥v∥∞|[v]ji|), proving Eq. (6).\\n3. Estimating the eigenvectors of (Pm)\\nWe use the bounds from Lemma 2 and Eq. (5) to estimate the eigenvectors of ( Pm) by applying Eq. (1), for m=1\\nin the next section and m⩾1 in Section 3.2.\\n3.1. Rank-one perturbation\\nIn Proposition 3, we obtain bounds on the coordinates of the eigenvectors of ( Pm) (Bis assumed to be diagonal)\\nwhen m=1, which comes as a consequence of Eq. (1).\\nProposition 3. Let D =diag(λ1,...,λ d)be a diagonal matrix with λ1⩾···⩾λd>0. Let v∈Rdand V :=\\nmax{d−1/2,∥v∥∞}. Consider the matrix A =D+√\\nDvvT√\\nD andν1⩾···⩾νdits eigenvalues and (e(1)\\n1,..., e(1)\\nd)a\\ncorresponding orthonormal basis of eigenvectors. Then,\\n\\x0c\\x0c\\x0c[e(1)\\ni]j\\x0c\\x0c\\x0c⩽5d2V4s\\nmin{λi,λj}\\nmax{λi,λj}for all i,j∈{1,..., d} (7)\\nProof. We prove first that, if max {λi,λj}>(1+dV2)×min{λi,λj}, then\\n\\x0c\\x0c\\x0c[e(1)\\ni]j\\x0c\\x0c\\x0c⩽(d−i+1)V2×infρ∈(0,1)ψ(ρ,(d−i+1)V2)\\n1−(1+(d−i+1)V2)min{λi,λj}\\nmax{λi,λj}s\\nmin{λi,λj}\\nmax{λi,λj}(8)\\nwithψ(ρ,W)=max{2(1−ρ)−1/2,2ρ−1W}, from which we deduce Eq. (7).\\n3' metadata={'source': './data/2403.06983.pdf', 'page': 2}\n",
            "page_content='First suppose that the eigenvalues λiofDare distinct, and that all entries of vare nonzero. Then, by Eq. (1), we\\nhave for i,j∈{1,..., d}that [ e(1)\\ni]j=Ci[√\\nDv]j\\nλj−νi=Ci√λi[v]j\\nλj−νi,where Ci∈Ris chosen such that ∥e(1)\\ni∥=1, hence\\n|Ci|=\\r\\r\\r\\r\\r\\r\\r\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edpλj[v]j\\nλj−νi\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nj=1,...,d\\r\\r\\r\\r\\r\\r\\r−1\\n=\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8eddX\\nj=1\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cpλj[v]j\\nλj−νi\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c2\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8−1/2\\n⩽min\\n1⩽j⩽d|λj−νi|pλj|[v]j|. (9)\\nCombining [23, Theorem 2.1, p. 175] with Eq. (6), we have λi<ν i⩽λi×(1+(d−i+1)V|[v]ji|),where jiis defined\\nin Lemma 2. By definition of jiwe have 0 ⩽λi−λji⩽λi(d−i+1)V|[v]ji|.By sum, we obtain 0 < ν i−λji⩽\\n2λi(d−i+1)V|[v]ji|.We apply this to Eq. (9) to get\\n|Ci|⩽νi−λjipλji|[v]ji|⩽2λi(d−i+1)V|[v]ji|pλji|[v]ji|=λipλji2(d−i+1)V. (10)\\nLetρ∈(0,1). If ( d−i+1)V2pλji⩽ρ√λi, then by definition of ji,λji⩾λi×(1−ρ), and by Eq. (10), |Ci|⩽\\n(1−ρ)−1/2×2(d−i+1)V√λi. Otherwise,|Ci|⩽ρ−1×2(d−i+1)2V3√λi. All in all, for ρ∈(0,1),\\n|Ci|⩽(d−i+1)Vp\\nλi×maxn\\n2(1−ρ)−1/2,2ρ−1(d−i+1)V2o\\nCCρp\\nλi. (11)\\nThen,|[e(1)\\ni]j|=|Ci|pλj|[v]j|/|λj−νi|⩽pλiλj/|λj−νi|×infρ∈(0,1)Cρ.By Eq. (6), when λj< λ i,|[e(1)\\ni]j|⩽\\nminρ∈[0,1]Cρ×(1−λj/λi)−1pλj/λi.By Eq. (5), when λj>(1+dV2)λi,|[e(1)\\ni]j|⩽minρ∈[0,1]Cρ×(1−(1+\\ndV2)λi/λj)−1pλi/λj.\\nIf the eigenvalues of Dare not distinct or not all entries of vare nonzero, we consider a sequence of diagonal\\nmatrices{Dk=diag(λk\\n1,...,λk\\nd)}k∈Nsuch that the diagonal elements λk\\n1>λk\\n2>···>λk\\nd>0 are distinct and Dk→D\\nwhen k→∞ , and a sequence of vectors {vk∈Rd\\n,0}k∈Nwith only nonzero entries where ∥vk∥∞⩽Nandvk→v\\nwhen k→∞ . Denote then Ak=Dk+√DkvkvT\\nk√Dkandνk\\n1⩾···⩾νk\\ndits eigenvalues. Note that Ak→Awhen\\nk→∞ , so by continuity of the eigenvalues, νk\\ni→νiwhen k→∞ . Furthermore, we just proved that if ek\\n1,..., ek\\ndare\\nunit eigenvectors of Akcorresponding respectively to the eigenvalues νk\\n1,...,νk\\nd, then ek\\n1,..., ek\\ndandλk\\n1,...,λk\\ndsatisfy\\nEq. (8). Moreover, the vectors ek\\niall belong to the unit sphere of Rd, so up to considering a subsequence of {Ak}k∈N,\\nwe can assume w.l.o.g. that each ek\\nitends to a vector e(1)\\ni∈Rdwhen k→∞ . As ( ek\\n1,..., ek\\nd) is an orthonormal system\\nofRd, so is its limit ( e(1)\\n1,..., e(1)\\nd) and e(1)\\niis an eigenvector of Acorresponding to the eigenvalue νi. Therefore, Eq. (8)\\nholds by taking the limit k→∞ in the equation satisfied by ek\\n1,..., ek\\ndandλk\\n1,...,λk\\nd.\\nTo obtain Eq. (7), note that when ρ=1/2, we have 2(1−ρ)−1/2⩽4⩽4dV2, and 2ρ−1(d−i+1)V2⩽4dV2, and\\nwhen max{λi,λj}/min{λi,λj}>1+4dV2, by Eq. (8), then,\\n(1−(1+dV2) min{λi,λj}/max{λi,λj})−1⩽(1+4dV2)/(4dV2)⩽5/4,\\nasV⩾d−1/2, and thus Eq. (7) holds. If otherwise max {λi,λj}⩽(1+4dV2) min{λi,λj}, as max{λi,λj}/min{λi,λj}⩾1,\\nwe find|[ei]j|⩽1⩽(1+4dV2)pmax{λi,λj}/min{λi,λj}. Since 1 ⩽dV2, then (1 +4dV2)⩽5dV2and Eq. (7)\\nholds.\\n3.2. Sum of m rank-one matrices pertubation\\nOur final Theorem 4 generalizes Proposition 3 to any value m⩾1 and is obtained by induction using Eq. (8).\\nTheorem 4 implies in particular Theorem 1 via the spectral theorem.\\nTheorem 4. Let D =diag(λ1,...,λ d)be a diagonal matrix with λ1⩾···⩾λd>0. Let V ⩾1/√\\nd and consider a\\nsequence of vectors v(i)∈Rdsuch that∥v(i)∥∞⩽V for all i∈N. For m∈N, let A(m)=D+√\\nDPm\\ni=1[v(i)][v(i)]T√\\nD\\nandν(m)\\n1⩾···⩾ν(m)\\ndthe eigenvalues of A(m)and(e(m)\\n1,..., e(m)\\nd)a corresponding orthonormal system of eigenvectors.\\nThen\\n|[e(m)\\ni]j|⩽Cms\\nmin{λi,λj}\\nmax{λi,λj}for all i,j∈{1,..., d}and m∈N (12)\\nwith C 0=1and C m+1=5d7V4C5\\nm√\\n1+dmV2.\\n4' metadata={'source': './data/2403.06983.pdf', 'page': 3}\n",
            "page_content='101102103104105106107108\\n1/2\\n107\\n106\\n105\\n104\\n103\\n102\\n101\\n|[e(m)\\n1]2|\\nd=2, m=1\\nd=5, m=1\\nd=5, m=3\\nd=10, m=1\\nd=10, m=3\\nd=10, m=5(a)j=2\\n101102103104105106107108\\n1/d\\n107\\n106\\n105\\n104\\n103\\n102\\n101\\n|[e(m)\\n1]d|\\nd=2, m=1\\nd=5, m=1\\nd=5, m=3\\nd=10, m=1\\nd=10, m=3\\nd=10, m=5 (b)j=dFigure 1: Value of |[e(m)\\n1]j|as a function of λ1/λjwhere\\ne(m)\\n1is an eigenvector associated to the largest eigenvalue\\nofA(m)from ( Pm) for di fferent dimensions and values of\\nmas given in the legend. The v(i)are independent standard\\nGaussian vectors (with the same realization for all values\\nofλ1) and the eigenvalues of the diagional matrix Bare\\nchosen uniformly on a log scale between λd=1 andλ1.\\nThe value|[e(m)\\n1]j|behaves consistent with Θ(pλj/λ1).\\nProof. Fora,b>0, denoteα(a,b)=√min{a,b}/max{a,b}. Let m∈Nand assume that Eq. (12) holds which is\\ntrue if m=0 since C0=1. Observe now that A(m+1)=A(m)+√\\nD[v(m+1)][v(m+1)]T√\\nD.In the system of coordinates\\nB(m)B(e(m)\\n1,..., e(m)\\nd),A(m)writes as D(m)Bdiag(ν(m)\\n1,...,ν(m)\\nd). Sinceλ1,...,λ d>0, and as A(m)⪰D, then\\nν(m)\\ni⩾λi>0 for i∈{1,..., d}, and\\nD√\\nDv(m+1),e(m)\\niE\\n=dX\\nj=1[e(m)\\ni]jp\\nλj[v(m+1)]j=q\\nD(m)\\nii×dX\\nj=1q\\nλj/ν(m)\\ni[e(m)\\ni]j[v(m+1)]jCh√\\nD(m)w(m+1)i\\ni.\\nHenceh\\nA(m+1)i\\nB(m)=D(m)+√\\nD(m)[w(m+1)] [w(m+1)]T√\\nD(m)with\\n∥w(m+1)∥∞⩽dX\\nj=1q\\nλj/ν(m)\\ni|[e(m)\\ni]j|×V⩽dX\\nj=1q\\nλj/λi|[e(m)\\ni]j|×V⩽dV×Cm.\\nWe apply Proposition 3 toh\\nA(m+1)i\\nB(m)so that, for i,k∈{1,..., d},\\n|⟨e(m+1)\\ni,e(m)\\nk⟩|=|[[e(m+1)\\ni]B(m)]k|⩽5d6V4C4\\nmα(ν(m)\\ni,ν(m)\\nk).\\nBy Eq. (5),|⟨e(m+1)\\ni,e(m)\\nk⟩|⩽(5d6V4C4\\nm)(1+dmV2)1/2α(λi,λk). Since|[e(m)\\nk]j|⩽Cmα(λi,λk), then,\\n|[e(m+1)\\ni]j|⩽dX\\nk=1|[e(m)\\nk]j|×|⟨e(m+1)\\ni,e(m)\\nk⟩|⩽dCm×5d6V4C4\\nm(1+dmV2)1/2α(λi,λj).\\nThis proves by induction that Eq. (12) holds for all m∈N.\\n3.3. Thightness\\nFigure 1 shows numerical computations of coordinates of the first eigenvector of A(m)in dimension 2, 5, 10. The\\ncoordinates seem to obey Θ(min{λi,λj}/max{λi,λj}) in all cases which suggests that this rate in our upper bounds is\\ntight. However we do not expect the constant Cmgiven in Theorem 4 to be tight.\\nAcknowledgments\\nThe authors would like to thank St ´ephane Gaubert for his constructive remarks and feedback on a prior version of\\nthe manuscript as well as the anonymous referee for their valuable review and comments.\\nReferences\\n[1] G. H. Golub, Some Modified Matrix Eigenvalue Problems, SIAM Review 15 (2) (1973) 318–334.\\n[2] J. R. Bunch, C. P. Nielsen, D. C. Sorensen, Rank-one modification of the symmetric eigenproblem, Numerische Mathematik 31 (1) (1978)\\n31–48.\\n5' metadata={'source': './data/2403.06983.pdf', 'page': 4}\n",
            "page_content='[3] I. C. F. Ipsen, B. Nadler, Refined Perturbation Bounds for Eigenvalues of Hermitian and Non-Hermitian Matrices, SIAM Journal on Matrix\\nAnalysis and Applications 31 (1) (2009) 40–53.\\n[4] J. Ding, A. Zhou, Eigenvalues of rank-one updated matrices with some applications, Applied Mathematics Letters 20 (12) (2007) 1223–1226.\\n[5] J. B ´enass ´eni, A. Mom, Inequalities for the eigenvectors associated to extremal eigenvalues in rank one perturbations of symmetric matrices,\\nLinear Algebra and its Applications 570 (2019) 123–137.\\n[6] R. C. Thompson, The behavior of eigenvalues and singular values under perturbations of restricted rank, Linear Algebra and its Applications\\n13 (1) (1976) 69–78.\\n[7] R. Mathias, Spectral Perturbation Bounds for Positive Definite Matrices, SIAM Journal on Matrix Analysis and Applications 18 (4) (1997)\\n959–980.\\n[8] R. Bhatia, Perturbation Bounds for Matrix Eigenvalues, no. 53 in Classics in Applied Mathematics, Society for Industrial and Applied\\nMathematics, Philadelphia, PA, 2007.\\n[9] M. Karow, D. Kressner, On a Perturbation Bound for Invariant Subspaces of Matrices, SIAM Journal on Matrix Analysis and Applications\\n35 (2014) 599–618.\\n[10] I. C. F. Ipsen, Relative perturbation results for matrix eigenvalues and singular values, Acta Numerica 7 (1998) 151–201.\\n[11] N. Truhar, I. Slapni ˇcar, Relative perturbation bound for invariant subspaces of graded indefinite Hermitian matrices, Linear Algebra and its\\nApplications 301 (1) (1999) 171–185.\\n[12] I. C. F. Ipsen, An overview of relative sin Θtheorems for invariant subspaces of complex matrices, Journal of Computational and Applied\\nMathematics 123 (1) (2000) 131–153.\\n[13] R. A. Horn, C. R. Johnson, Matrix analysis, Cambridge university press, 2012.\\n[14] G. Kjellstrom, L. Taxen, Stochastic optimization in system design, IEEE Transactions on Circuits and Systems 28 (7) (1981) 702–715.\\n[15] N. Hansen, S. D. M ¨uller, P. Koumoutsakos, Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix\\nAdaptation (CMA-ES), Evolutionary Computation 11 (1) (2003) 1–18.\\n[16] L. Ljung, System Identification. Theory for the User., Information and System Sciences Series, Prentice Hall PTR, Upper Saddle River, NJ\\n07458, 1999.\\n[17] H. Haario, E. Saksman, J. Tamminen, An Adaptive Metropolis Algorithm, Bernoulli 7 (2) (2001) 223–242.\\n[18] C. Davis, W. M. Kahan, The Rotation of Eigenvectors by a Perturbation III, SIAM Journal of Numerical Analysis 7 (Mar. 1970).\\n[19] N. Hansen, A. Ostermeier, Completely Derandomized Self-Adaptation in Evolution Strategies, Evolutionary Computation 9 (2) (2001) 159–\\n195.\\n[20] S. P. Meyn, R. L. Tweedie, Markov Chains and Stochastic Stability, Springer Science & Business Media, 2012.\\n[21] A. Auger, N. Hansen, Linear Convergence of Comparison-based Step-size Adaptive Randomized Search via Stability of Markov Chains,\\nSIAM Journal on Optimization 26 (3) (2016) 1589–1624.\\n[22] C. Toure, A. Auger, N. Hansen, Global linear convergence of evolution strategies with recombination on scaling-invariant functions, Journal\\nof Global Optimization 86 (1) (2023) 163–203.\\n[23] G. W. Stewart, Matrix Algorithms: V olume II: Eigensystems, SIAM, 2001.\\n6' metadata={'source': './data/2403.06983.pdf', 'page': 5}\n",
            "Processing  ./data/2403.06993.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='Automatic driving lane change safety prediction model based on LSTM   Wenjian Sun1 Electronic and Information Engineering Yantai University Yantai, Shandong swjhuman@gmail.com   Jingyu Xu 3 Computer Information Technology Northern Arizona University Flagstaff,Arizona,USA jyxu01@outlook.com        Linying Pan2  Information Studies Trine university Phoenix, Arizona, USA panlinying2023@gmail.com    Weixiang Wan 4 Electronics & Communication Engineering University of Electronic Science and Technology of China ChengDu,China danielwanwx@gmail.com    Yong Wang5 Information Technology University of Aberdeen, Aberdeen, United Kingdom fredia4jane@gmail.com   Abstract：Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell state simulation algorithm simulates the trackability of the trajectory generated by this model. The research results show that compared with the traditional model-based method, the trajectory prediction method based on LSTM network has obvious advantages in predicting the trajectory in the long time domain. The intention recognition module considering interactive information has higher prediction and accuracy, and the algorithm results show that the trajectory is very smooth based on the premise of safe prediction and efficient lane change. And autonomous vehicles can efficiently and safely complete lane changes. Keywords：Prediction model; Safe lane change; LSTM; Deep learning I. INTRODUCTION  In recent years, automatic driving has attracted wide attention around the world, and the public believes that automatic driving can improve traffic safety and reduce traffic accidents. In addition, it can improve traffic fluency, reduce congestion, save energy, and improve travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. The higher the level of autonomous driving, the heavier the responsibility of the decision-making level, because the autonomous vehicle needs to have a very good sense of the surrounding environment, so that it can make judgments without driver intervention, so that the decision task is sent to the planning and control level[1-2]. Therefore, some self-driving manufacturers have conducted large-scale road tests, such as Google self-driving cars and Apple self-driving cars. However, due to the complexity of the traffic system, there are more or less some safety problems in the current automatic driving, leading to a series of accidents in the test of these automatic driving vehicles, the main reason is that the automatic driving algorithm inside the automatic driving' metadata={'source': './data/2403.06993.pdf', 'page': 0}\n",
            "page_content='vehicle is not enough to cope with the dynamic change of the traffic environment[3]. According to research, nearly one-third of all traffic accidents are caused by unsafe lane changes. The human lane change execution model using machine learning is a data-driven model whose parameters need to be determined by training a large amount of lane change execution data. At present, there are few researches on human lane change trajectory planning using machine learning. Some relevant researchers apply k nearest neighbor algorithm to lane change trajectory planning, but the amount of data used in this model is very limited. Considering that existing machine learning algorithms can only predict the position of lane changing vehicles, DING Chenxi constructed a two-layer BP neural network to make real-time prediction of lane changing vehicles. This model learns NGSIM data and expands the data volume based on the existing research. However, lane change data is a kind of time series, and the above two machine learning methods are only a single copy of the position of the vehicle in a certain state, and do not take into account the connection between the lane change data of each planned step during lane change. On this basis, XIE Dongfan and other researchers built LSTM neural network to predict the lane change trajectory of vehicles, and achieved an accuracy of more than 99% for human lane change trajectory learning. Although these studies are all about the learning of human lane change behavior, they fail to take into account the safety problems generated in the process of lane change. However, rule-based lane change models do not provide uniform mathematical descriptions of lane change trajectories, resulting in a large number of trajectory curve equations, such as polar coordinate polynomial trajectories and quintic polynomial trajectories[4-6]. In addition, there is still no conclusion on which equation can best describe the lane change trajectory curve, and the current kinematic model describing the lane change process also has large errors. Considering the shortcomings of the existing methods, this paper modified the LSTM neural network and proposed a new lane change trajectory planning model for automatic driving based on a safety-sensitive improved long short-term memory network[7]. II. LSTM NEURAL NETWORK LANE CHANGE MODEL A. LSTM structure The LSTM model realizes the dynamic change of time scale under fixed parameters through self-cycling inside the cell. The data of vehicle driving behavior contains time-continuous information about vehicle driving behavior, so using LSTM model to model lane change intention recognition can achieve good simulation results. In this paper, long and short term memory neural network is used to train the safe lane change prediction model of autonomous vehicles, and long-term storage of safe lane change memory of autonomous driving is realized[8-10]. Besides, the long and short term memory (LSTM) network is trained and tested by using high-dimensional data set, and the information extracted by vehicle-to-vehicle (V2V) technology is combined. Using proven LSTM to predict the motion trajectories of surrounding vehicles. Based on the predicted trajectory information of the surrounding vehicles, a local path planning algorithm based on risk assessment and prevention is proposed.' metadata={'source': './data/2403.06993.pdf', 'page': 1}\n",
            "page_content='Figure. 1. LSTM model structure The core structure of the LSTM network is composed of several cell structures in the above figure, and each cell structure has three control gates: forgetting gate, input gate, and output gate. The forget gate, which controls whether to forget, controls whether to forget the hidden cell state of the previous layer with a certain probability. (1) Where δ represents the sigmoid function; wf represents the weight of the forgotten gate; ht-1 indicates the hidden state at t-1 moment. xt represents input data; bf represents the deviation of the forgetting gate. The input gate determines what information is updated to the cell state. it consists of two parts, the first part uses the sigmoid activation function, output as it; The second part uses the tanh activation function, which outputs at. The two results are then multiplied to update the cell status. The mathematical expression is:  (3) (4) Where δ represents the sigmoid function; tanh stands for tanh function; wt indicates the input gate weight; wc represents information about the candidate cell state that will be updated to the cell state; ht-1 indicates the hidden state at t-1 moment. xt represents input data; b represents the deviation of the forgetting gate; bc indicates candidate cell state deviation. Then, the updating process of LSTM cell status is as follows: (5) The output gate outputs the information selectively and with the participation of the cell state:' metadata={'source': './data/2403.06993.pdf', 'page': 1}\n",
            "page_content='(6) ot is the output of the output gate. w0 is the output gate weight; ht-1 indicates the hidden state at t-1 moment. ht indicates the hidden state at time t. xt represents input data; b0 represents the candidate cell state deviation. B. Lane change planning model Table1. Digital image technology application scenario case Since the acceleration is required to change continuously during vehicle driving, the polynomial curve adopted by the lane change track of automatic driving should not be less than 3 times. In order to avoid too complicated parameter solving caused by too high number of polynomial curves, the polynomial curve is determined as a cubic polynomial curve, whose expression is as follows: (1) Where: a0, a1, a2,; Are parameters that need to be determined later; The longitudinal position of the lane changing vehicle; y is the lateral position of the lane changing vehicle. Determine the following parameters: (2) In the formula, θ is the heading Angle of the starting point of the planned step size. C. Collision avoidance algorithm Gipps model is a classic vehicle safety distance model in the field, which can better fit the driving state between two following vehicles, but it regards the vehicle as a particle without considering the vehicle body length[11]. Therefore, based on the classic Gipps model, the body length is added to improve it as the constraint condition of the cubic trajectory curve. In the process of lane change, vehicles are affected by the surrounding vehicles in real time. In order to ensure the safety of lane change process, it is necessary to detect the driving state of surrounding vehicles in real time and predict the driving state of surrounding vehicles.  Figure. 2. Road safety distance model Gipps model is to solve the current vehicle emergency stop, the rear car after the reaction time also take emergency stop action, so as not to crash the speed of the front car. In the classical Gipps model, the length of the car body is not taken into account, nor is the time-varying speed of the front and rear cars taken into account in the actual process of following[12]. III. LSTM PREDICTION MODEL VERIFICATION RESULTS The above experiments build two driver scenarios based on high-dimensional data sets for verification and evaluation. The proposed LSTM-MPC algorithm is compared with the MPC algorithm with constant velocity prediction and NIO network prediction. In the process of road planning, the driving safety and driving efficiency are compared according to RAI and collision avoidance speed[13]. Scenario 1: Active Lane Change When the car starts driving along lane 2, the target lane changes from lane 2 to lane 3 after a few miles. At the same time, there are three obstacles for vehicles near the self car in the 3rd lane, as shown in the picture below. The car needs to complete the lane change without a collision. \\n (a) \\n (b) Figure. 3. Active lane change model Horizontal position and RAI comparison between LSTM-MPC, NIO-MPC and conventional MPC implemented on the basic LSTM algorithm. The results in Figure 4(a) show that the path planning using LSTM-MPC has the lowest RAI than the other two methods, while the NIO-MPC method has a slightly higher RAI with a maximum error of 0.5 at 4 seconds. Figure 4(b) compares the lateral position of LSTM-MPC and conventional MPC during lane change. However, due to high forecast uncertainty, the lateral position of NIO-MPC is somewhat out of line with reality.' metadata={'source': './data/2403.06993.pdf', 'page': 2}\n",
            "page_content='(a)  (b) Figure. 4. Comparison of LSTM-MPC and MPC algorithm results   The second scenario is the emergency braking situation, and the track of the car is shown in the figure below. Blue and red represent the truck and the car, respectively, and the truck has a sudden deceleration time of about 15 seconds. The results showed that the self-driving car would also slow down to maintain a safe lane distance from the leading truck.  (a) \\n (b) Figure. 4. Autopilot emergency braking model The following figure shows the RAI of the three methods. The results show that in all kinds of collision avoidance methods, the self-driving vehicle will slow down as the main vehicle slows down. Using LSTM and NIO methods, the self-vehicle is decelerated in advance, and the RAI is improved compared with the constant method. However, due to the lower prediction accuracy of NIO, this leads to an unnecessarily larger deceleration than the other two methods.  (a)  (b) Figure. 5. Comparative analysis of LSTM and NIO  Therefore, it can be seen that in various collision avoidance methods, the self vehicle will slow down with the deceleration of the main vehicle. Using LSTM and NIO methods, the self-vehicle is decelerated in advance, and the RAI is improved compared with the constant method. However, due to the lower prediction accuracy of NIO, this leads to an unnecessarily larger deceleration than the other two methods. In addition, the RAI value of LSTM-MPC algorithm is the lowest among the three methods, indicating that the self-vehicle has a higher safety level when using LSTM-MPC algorithm for path planning and path tracking[14]. By taking into account changes in the speed of surrounding vehicles during autonomous driving, the LSTM-MPC algorithm can be used to enable autonomous vehicles to travel with lower risk and shorter deceleration times. IV. CONCLUSIONS In order to study the lane change technology of autonomous driving, an improved LSTM neural network for lane change trajectory planning is proposed in the field of autonomous driving. A deep learning lane change trajectory planning model is established from the perspectives of safety and efficiency, and the improved LSTM neural network model can improve the safety of the vehicle lane change process to a certain extent, so that the vehicle can use the rule-based algorithm to monitor and correct the safety of the trajectory while learning. In addition, it is verified that the comfort and efficiency of the improved LSTM neural network model are higher than that of the real trajectory, and the influence of lane-changing vehicles and surrounding vehicles on the lane-changing process is analyzed through Python simulation results[15-17]. Although the conventional lane change trajectory planning model has a high learning precision for the target trajectory during the execution of lane change, it ignores the changes of the surrounding environment of the vehicle, and the lane change vehicle cannot respond to unexpected situations, so there are' metadata={'source': './data/2403.06993.pdf', 'page': 3}\n",
            "page_content='still shortcomings in safety[18]. The experimental results of this paper fully demonstrate the path planning method based on risk assessment and risk prevention. The LSTM network is trained based on high-dimensional data sets and used to predict the motion trajectories of surrounding vehicles. By introducing the RAI index, risk assessment of the speed of surrounding vehicles is carried out to improve the driving safety of vehicles. The final comparative analysis shows that autonomous vehicles using LSTM-MPC always have the lowest RAI (highest safety) during autonomous driving under both active lane change and deceleration conditions. At the same time, the RAI comparison between this method and NIO method also shows that the higher the prediction accuracy of surrounding vehicles, the safer the self-vehicle will be in the process of path planning and tracking. REFERENCES  [1] Chang Che, Bo Liu, Shulin Li, Jiaxin Huang, and Hao Hu. Deep learning for precise robot position [2] Tianbo, Song, Hu Weijun, Cai Jiangfeng, Liu Weijia, Yuan Quan, and He Kun. \"Bio-inspired Swarm Intelligence: a Flocking Project With Group Object Recognition.\" In 2023 3rd International Conference on Consumer Electronics and Computer Engineering (ICCECE), pp. 834-837. IEEE, 2023. [3] JACOBSTEIN N.Autonomous Vehicles: An ImperfectPath to Saving Millions of Lives [I]. Science Robotics.2019，4(28): aaw8703. [4] S. Tianbo, H. Weijun, C. Jiangfeng, L. Weijia, Y. Quan and H. Kun, \"Bio-inspired Swarm Intelligence: a Flocking Project With Group Object Recognition,\" 2023 3rd International Conference on Consumer Electronics and Computer Engineering (ICCECE), Guangzhou, China, 2023, pp. 834-837, doi: 10.1109/ICCECE58074.2023.10135464. [5] Li Xiaopeng，AMIR G，XU Zhigang, et al. A PiecewiseIrajectory Optimization Model for Connected AutomatedVehicles: Exact Optimization Algorithm and QueuePropagation Analysis [j]. Transportation Research Part B:Methodological，2018，118: 429 456. [6] Zheng Yang, Tien Tuan Anh Dinh, Chao Yin, Yingying Yao, Dianshi Yang, Xiaolin Chang, and Jianying Zhou. \"LARP: A Lightweight Auto-Refreshing Pseudonym Protocol for V2X.\" Proceedings of the 27th ACM on Symposium on Access Control Models and Technologies, 2022, pp. 49-60. DOI:https://doi.org/10.1145/3532105.3535027 [7] RAHMAN M S，ABDEL-ATY M，LEE JY，et alSafety Benefits of Arterials’ Crash Risk Under Connectedand Automated Vehicles [J. Transportation Research PartC: Emerging Technologies， 2019，100: 354-371. [8] Hao Hu, Shulin Li, Jiaxin Huang, Bo Liu, and Change Che. Casting product image data for quality inspection with xception and data augmentation. Journal of Theory and Practice of Engineering Science, 3(10):42–46, 2023. [9] Lin, Q., Che, C., Hu, H., Zhao, X., & Li, S. (2023). A Comprehensive Study on Early Alzheimer’s Disease Detection through Advanced Machine Learning Techniques on MRI Data. Academic Journal of Science and Technology, 8(1), 281–285. [10] HAN Y J,AHN S Y Stochastic Modeling of Breakdownat Freeway Merge Bottleneck and Traffic Control MethodUsing Connected Automated Vehicle [J]. TransportationResearch Part B: Methodological，2018，107: 146-166. [11] GHIASI A，OMAR H，OIAN Zhen, et al.A Mixed[raffic Capacity Analysis and Lane Management Model forConnected Automated Vehicles: A Markov Chain Method[J]. Transportation Research Part B: Methodological,2017，106: 266-292. [12] DE ALMEIDA CORREIA G H，VAN AREM B. Solvingthe User Optimum Privately Owned Automated VehiclesAssignment Problem (UO-POAVAP): A Model to Explorethe lmpacts of Self-Driving Vehicles on Urban Mobility[J][ransportation Research Part B-Methodological，2016,87: 64 88. [13] Che, C., Hu, H., Zhao, X., Li, S., & Lin, Q. (2023). Advancing Cancer Document Classification with R andom Forest. Academic Journal of Science and Technology, 8(1), 278–280. [14] ROY A G, CONJETI S, NAVAB N, et al. Bayesian QuickNAT:model uncertainty in deep whole-brain segmentation for structure-wise quality control[]. Neuroimage, 2019, 195: 11-22.WILSON' metadata={'source': './data/2403.06993.pdf', 'page': 4}\n",
            "page_content='8(1), 278–280. [14] ROY A G, CONJETI S, NAVAB N, et al. Bayesian QuickNAT:model uncertainty in deep whole-brain segmentation for structure-wise quality control[]. Neuroimage, 2019, 195: 11-22.WILSON A G, IZMAILOV P Bayesian deep learning and aprobabilistic perspective of generalization[J]. Advances inneural information processing systems, 2020, 33: 4697-4708. [15] Bo L ,Bingchuan B ,Xuefeng Z . Vision-based structural displacement measurement under ambient-light changes via deep learning and digital image processing [J]. Measurement, 2023, 208. [16] SUTTON R S,BARTO A G. Renforcement learning: AnintroductionM]. Cambridge, MA:MIT press, 2018.WATKINS C，DAYAN P. O-learning[J].Machine learning1992,8: 279-292. [17] Y. Wang, K. Yang, W. Wan, Y. Zhang and Q. Liu, \"Energy-Efficient Data and Energy Integrated Management Strategy for IoT Devices Based on RF Energy Harvesting,\" in IEEE Internet of Things Journal, vol. 8, no. 17, pp. 13640-13651, 1 Sept.1, 2021, DOI: 10.1109/JIOT.2021.3068040. [18] Hou Xiang dan, Zheng Meng jing, Liu Hong pu, etal. Medical image enhancement algorithm based onshearlet domain and improve Pal-King algorithm[I].Laser &. Optoelectronics Progress ,2019， 56(3):123-129. Note: ①The authors of the references cited in the article should not be of one nationality only. They should be from three or more; ②It is not possible to have less than five references.' metadata={'source': './data/2403.06993.pdf', 'page': 4}\n",
            "page_content='View publication stats' metadata={'source': './data/2403.06993.pdf', 'page': 4}\n",
            "Processing  ./data/2403.06992.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='Phase autoencoder for limit-cycle oscillators\\nPhase autoencoder for limit-cycle oscillators\\nKoichiro Yawata,1Kai Fukami,2Kunihiko Taira,2and Hiroya Nakao1\\n1)Department of Systems and Control Engineering, Tokyo Institute of Technology, Tokyo 152-8552,\\nJapan\\n2)Department of Mechanical and Aerospace Engineering, University of California, Los Angeles, CA 90095,\\nUSA\\n(*Electronic mail: koichiro.yawata.rt@gmail.com)\\n(Dated: 13 March 2024)\\nWe present a phase autoencoder that encodes the asymptotic phase of a limit-cycle oscillator, a fundamental quantity\\ncharacterizing its synchronization dynamics. This autoencoder is trained in such a way that its latent variables directly\\nrepresent the asymptotic phase of the oscillator. The trained autoencoder can perform two functions without relying on\\nthe mathematical model of the oscillator: first, it can evaluate the asymptotic phase and phase sensitivity function of\\nthe oscillator; second, it can reconstruct the oscillator state on the limit cycle in the original space from the phase value\\nas an input. Using several examples of limit-cycle oscillators, we demonstrate that the asymptotic phase and phase\\nsensitivity function can be estimated only from time-series data by the trained autoencoder. We also present a simple\\nmethod for globally synchronizing two oscillators as an application of the trained autoencoder.\\nSpontaneous rhythmic phenomena are widely observed in\\nthe real world. They are generally modeled as limit-cycle\\noscillators in dynamical systems theory, and phase reduc-\\ntion theory has proven useful for understanding synchro-\\nnization phenomena of interacting limit-cycle oscillators.\\nIn phase reduction theory, the state of the oscillator is\\nrepresented by using a single variable called the asymp-\\ntotic phase, but it is computationally challenging to eval-\\nuate; reproducing the original oscillator state from the\\nphase value can also be difficult. In this study, we pro-\\npose a machine-learning method based on the autoencoder\\nto address these issues, which facilitates evaluation of the\\nasymptotic phase only from time-series data and repro-\\nduction of the oscillator state from the phase value as an\\ninput. Our phase autoencoder can be used for data-driven\\nsynchronization control of limit-cycle oscillators without\\nrelying on their mathematical models.\\nI. INTRODUCTION\\nSpontaneous rhythmic phenomena are widely observed in\\nthe real world and play important roles in the functioning of\\nbiological or engineered systems1–12, such as heartbeats and\\nrespiration10, brain waves4,5and power grids12. Such rhyth-\\nmic phenomena can be mathematically modeled as limit-cycle\\noscillators in nonlinear dynamical systems, and phase reduc-\\ntion theory1,3,13–17has been widely used for understanding the\\nsynchronization dynamics of coupled limit-cycle oscillators.\\nIn phase reduction theory, the multidimensional state of\\na limit-cycle oscillator is described by a single phase vari-\\nable that increases with a constant natural frequency along the\\nlimit cycle and in its basin of attraction, called the asymptotic\\nphase. The phase sensitivity function (PSF, aka infinitesimal\\nphase resetting curve) calculated from the asymptotic phase\\nis important for characterizing the dynamics of weakly per-\\nturbed limit-cycle oscillators and has been studied both theo-\\nretically and experimentally1,13,14,18–21.By phase reduction, the state of an oscillator can be ap-\\nproximately described by a simple phase equation character-\\nized only by the natural frequency and PSF of the oscillator,\\nwhich is useful for analyzing the synchronization dynamics of\\nthe oscillator in detail1,3,14,22. Recently, more general phase-\\namplitude reduction theory has also been developed15,19,23–30,\\nwhich is an extension of the phase reduction theory to include\\nthe deviations of the oscillator state from the limit cycle using\\nexponentially decaying amplitudes.\\nFIG. 1. Phase autoencoder. The encoder maps the original state' metadata={'source': './data/2403.06992.pdf', 'page': 0}\n",
            "page_content='the deviations of the oscillator state from the limit cycle using\\nexponentially decaying amplitudes.\\nFIG. 1. Phase autoencoder. The encoder maps the original state\\nspace to a latent space, and the decoder maps the latent space to the\\noriginal state space. The autoencoder is trained in such a way that\\na pair of variables in the latent space represent the asymptotic phase\\nof the oscillator, and that the oscillator state on the limit cycle is\\nmapped to a plane in the latent space on which another latent variable\\nis zero. The red line shows the limit cycle, the blue line shows the\\noscillator orbit, and the dashed line shows the isochron (level set of\\nthe asymptotic phase).\\nThe asymptotic phase and PSF are generally not obtainable\\nanalytically. When the differential equation describing the os-\\ncillator is known, the PSF can be calculated numerically by\\nthe adjoint method31. However, since detailed mathematical\\nmodels are often unavailable for real-world systems, studiesarXiv:2403.06992v1  [nlin.AO]  28 Feb 2024' metadata={'source': './data/2403.06992.pdf', 'page': 0}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 2\\nhave also been conducted to obtain the asymptotic phase and\\nPSF directly from time-series data24,32–40.\\nRecently, it has been shown that the asymptotic phase is es-\\nsentially related to the Koopman eigenfunction of the oscilla-\\ntor with a pure imaginary eigenvalue characterized by the nat-\\nural frequency41, and Dynamic Mode Decomposition (DMD)\\nand its extensions35–37, which can estimate Koopman eigen-\\nvalues and eigenfunctions from time-series data, have been\\nemployed to estimate the asymptotic phase in a data-driven\\nmanner. However, there remain challenges with such meth-\\nods: first, it is not easy to estimate the asymptotic phase for\\nhigh-dimensional systems, and second, it is difficult to recon-\\nstruct the original oscillator state from the phase as an input.\\nIn this study, we propose a machine-learning method for\\nestimating the asymptotic phase based on an autoencoder\\n(Fig. 1). The autoencoder is a neural network that encodes\\noriginal input data into latent variables and then decodes the\\noriginal input data from the latent variables. Specifically, we\\ndesign an autoencoder with a latent space in which a pair of\\nlatent variables represent the asymptotic phase of the oscil-\\nlator, while another latent variable represents the oscillator’s\\ndeviation from the limit cycle. This enables the estimation of\\nthe asymptotic phase and PSF of the oscillator and reconstruc-\\ntion of the oscillator state in the original space from the phase\\nvalue in a data-driven manner. As an application, we propose\\na simple method for globally synchronizing two oscillators\\nusing the trained autoencoder.\\nThis paper is organized as follows. First, we briefly out-\\nline the phase reduction method in Sec. II. We then describe\\nthe proposed phase autoencoder in Sec. III. The validity of\\nthe proposed phase autoencoder is illustrated through numer-\\nical simulations in Sec. IV, and a simple method for globally\\nsynchronizing two oscillators using the trained autoencoder\\nis presented in Sec. V. Section VI discusses the relationship\\nof the latent variables with the Koopman eigenfunctions, and\\nSec. VII gives conclusions.\\nII. PHASE REDUCTION\\nA. Limit-cycle oscillator\\nWe describe the state of a limit-cycle oscillator in terms of\\na vector X∈RdX, which obeys the following ordinary differ-\\nential equation (ODE):\\nd\\ndtX(t) =F(X(t)), (1)\\nwhere tis the time and F(X):RdX→RdXis a smooth vector\\nfield representing the oscillator dynamics. We assume that\\nthis dynamical system has an exponentially stable limit-cycle\\nsolution X0(t)of period Tin the state space satisfying X0(t+\\nT) =X0(t), and denote its basin of attraction as A⊆RdX.B. Asymptotic phase\\nTo characterize the oscillator state X, we introduce a phase\\nfunction Θ(X):A→[0,2π]that gives the asymptotic phase\\nofX1,3. First, the phase for a state X0(t)on the limit cycle is\\ndefined as\\nΘ(X0(t)) = ( tmod T)ω, (2)\\nwhere ω=2π/Tis the natural frequency. By this definition,\\na phase value between 0 and 2 πis assigned to the state X0(t)\\non the limit cycle, and this phase value increases constantly\\nwith t, i.e.,\\nd\\ndtΘ(X0(t)) =ω. (3)\\nNote that the point of phase 0 is given by X0(0), which can\\nbe chosen arbitrarily. In what follows, we denote the state at\\nphase θon the limit cycle as X0(θ). Next, the phase of a state\\nXin the basin Ais defined to be the same phase value as the\\nstate on the limit cycle X0(θ)if they eventually converge to\\nthe same state on the limit cycle as shown in Fig. 1. In this\\nway, the phase function Θcan be defined so that\\nd\\ndtΘ(X(t)) =ω (4)\\nholds for any state XinAobeying Eq. (1).\\nC. Phase sensitivity function\\nThe gradient of the phase function\\nZ(θ) =gradX=X0(θ)Θ(X)∈RdX (5)\\nevaluated at X0(θ)on the limit cycle characterizes the lin-\\near response property of the oscillator’s phase to small per-\\nturbations given at phase θand is called the phase sensitivity\\nfunction (PSF)3. It can be seen from Eqs. (1) and (5) that\\nd\\ndtΘ(X0(θ)) = gradX=X0(θ)Θ(X))·d\\ndtX0(θ)' metadata={'source': './data/2403.06992.pdf', 'page': 1}\n",
            "page_content='turbations given at phase θand is called the phase sensitivity\\nfunction (PSF)3. It can be seen from Eqs. (1) and (5) that\\nd\\ndtΘ(X0(θ)) = gradX=X0(θ)Θ(X))·d\\ndtX0(θ)\\n=gradX=X0(θ)Θ(X))·F(X0(θ))\\n=Z(θ)·F(X0(θ)). (6)\\nThus, from Eq. (3), the following normalization condition for\\nthe PSF should be satisfied:\\nZ(θ)·F(X0(θ)) =ω. (7)\\nIt is well known that the PSF can be calculated by solving the\\nadjoint equation31\\nd\\ndθZ(θ) =−J(θ)⊤Z(θ), (8)\\nwhere J(θ) =DF(X0(θ))is the Jacobian matrix of F(X)\\nevaluated at X=X0(θ)and⊤denotes matrix transpose.' metadata={'source': './data/2403.06992.pdf', 'page': 1}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 3\\nWhen the oscillator is subjected to a weak perturbation p∈\\nRdXand is described by\\nd\\ndtX(t) =F(X(t))+p(X(t),t), (9)\\nthe phase θ(t) =Θ(X(t))of this oscillator state approxi-\\nmately obeys a phase equation\\nd\\ndtθ(t) =ω+Z(θ)·p(X(t),t) (10)\\nat the lowest order in p. The simplicity of this phase equation\\nhas facilitated detailed analysis of the synchronization dynam-\\nics of limit-cycle oscillators1,3,13–17,42.\\nIII. PHASE AUTOENCODER\\nA. Background\\nBefore presenting our proposed phase autoencoder, we\\nbriefly explain the background of our study. A closely re-\\nlated idea to our phase autoencoder is the Koopman autoen-\\ncoder38,43–52. The Koopman autoencoder trains the autoen-\\ncoder so that the time evolution of the latent variables is de-\\nscribed by a linear mapping representing the evolution of the\\nobservables by the Koopman operator. This is achieved by\\ntraining the autoencoder to reduce the difference between the\\nlatent variables at the next time step predicted by the linear\\nmapping and the latent variables encoded from the system\\nstate at the next time step. In pioneering works, Takeishi et\\nal.46proposed a method for searching the latent space that al-\\nlows linear mapping, and Lusch et al.38proposed a method\\nfor learning both encoding and linear mapping in the latent\\nspace.\\nSince the asymptotic phase is the argument of the Koop-\\nman eigenfunction associated with a pure imaginary eigen-\\nvalue (see Sec. VI)26,41, the Koopman autoencoder could, in\\nprinciple, be used for evaluating the oscillator’s asymptotic\\nphase. However, these Koopman autoencoders were not de-\\nsigned to perform phase reduction of limit cycles. In Ref.39,\\na simple method to perform data-driven phase reduction was\\nproposed, which approximates the asymptotic phase based on\\npolynomial regression without relying on the Koopman oper-\\nator theory. The drawbacks of the latter method were the dif-\\nficulties in handling oscillators with higher dimensions ( >2)\\nand also in reconstructing the oscillator state in the original\\nspace from the given phase value.\\nIn this study, we design an autoencoder in such a way that\\na pair of the latent variables directly represent the asymptotic\\nphase and another latent variable decays with time as the os-\\ncillator state approaches the limit cycle. This allows direct\\nestimation of the asymptotic phase θfrom the oscillator state\\nXand reconstruction of the oscillator state X0(θ)on the limit\\ncycle when the phase θis given. In what follows, we explain\\nthe phase autoencoder and its loss functions for learning the\\nabove-described latent variables.B. Proposed Phase Autoencoder\\nAn autoencoder53,54is an artificial neural network consist-\\ning of an encoder fenc:RdX→RdY, which transforms an in-\\nput vector X∈RdXto a latent vector Y∈RdY, and a decoder\\nfdec:RdY→RdX, which approximately reconstructs the orig-\\ninal vector ˆX∈RdXfrom the latent vector Y. Here, dXand\\ndYare the dimensions of the input space and the latent space,\\nrespectively. In this study, dYis fixed at 3 (see discussion in\\nSec. VI). The architecture of the autoencoder is shown in the\\nFig. 2 (a).\\nWe assume that the oscillator state X(t)is sampled at a\\nsampling time interval of τ>0, which is denoted as Xt,\\nand transformed to a latent vector Yt=fenc(Xt)by the au-\\ntoencoder. We design the autoencoder so that the latent\\nvariables (components of the three-dimensional latent vector)\\nYt= (Y1,t,Y2,t,Y3,t)at time step tsatisfy\\nY2\\n1,t+Y2\\n2,t=1, (11)\\nY1,t+τ=Y1,tcos(ωτ)−Y2,tsin(ωτ), (12)\\nY2,t+τ=Y1,tsin(ωτ)+Y2,tcos(ωτ), (13)\\nY3,t+τ=eλτY3,t, (14)\\nwhere ωandλare also the parameters to be learned (Fig. 2\\n(b)). The first three equations for the latent variables Y1,tand\\nY2,trepresent that the oscillator state Xtis mapped onto a unit\\ncircle on the Y1−Y2plane in the latent space and rotates on\\nit with a constant frequency ωas the oscillator state evolves.\\nThus, Y1andY2correspond to the asymptotic phase. The last' metadata={'source': './data/2403.06992.pdf', 'page': 2}\n",
            "page_content='circle on the Y1−Y2plane in the latent space and rotates on\\nit with a constant frequency ωas the oscillator state evolves.\\nThus, Y1andY2correspond to the asymptotic phase. The last\\nequation for another latent variable Y3,trepresents that the de-\\nviation of the oscillator state from the Y1−Y2plane shrinks\\nexponentially with time at a rate λ. We stress that we design\\nthe autoencoder to satisfy Eqs. (12)-(14) for all τ>0.\\nSince the limit cycle is exponentially stable, we assume that\\nλis negative and the oscillator states on the limit cycle are\\nembedded on the Y1−Y2plane satisfying Y3=0, where we\\nexpect that the information around the limit cycle is embedded\\ncollectively in Y3. See Fig. 1 for a schematic of our definition\\nof the latent variables. We will discuss the relationship of\\nthis variable Y3with the amplitude variable19defined via the\\nKoopman operator theory in Sec. VI.\\nOur phase autoencoder learns the encoding function fenc\\nand the decoding function fdecto satisfy Eqs. (11)-(14). To\\nguarantee a one-to-one correspondence between the phase and\\nthe two latent variables Y1andY2, we internally train an en-\\ncoder ˜fencthat maps Xto a non-normalized latent vector Ye\\nasYe= (Ye,1,Ye,2,Ye,3) =˜fenc(X), then normalize this Yeas\\nR=q\\nY2\\ne,1+Y2\\ne,2, (15)\\nY= (Ye,1/R,Ye,2/R,Y3), (16)\\nand denote the result after the normalization as the final en-\\ncoder, Y=fenc(X). The decoder is represented as\\nˆX=fdec(Y), (17)\\nwhich takes the normalized latent vector Yas the input and\\noutput the oscillator state ˆXapproximating the original state\\nX.' metadata={'source': './data/2403.06992.pdf', 'page': 2}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 4\\nFIG. 2. Phase autoencoder architecture. (a) The encoder transforms the inputs into three-dimensional latent variables. The first two variables\\nare normalized and correspond one-to-one with the asymptotic phase. The decoder reconstructs the input from the three latent variables. (b)\\nTime evolution of the three latent variables transformed by the encoder. The blue lines and circles ( ωandλ) indicate the parameters to be\\ntrained.\\nIn addition to the autoencoder, we also introduce a neural\\nnetwork fstepthat learns the dynamics of the latent vector so\\nthat\\nYt+τ=fstep(Yt) (18)\\nholds approximately. Specifically, fsteplearns the parameters\\nωandλfrom the discretized dynamics in the latent space\\ndescribed by Eqs. (12)-(14).\\nUsing the trained encoder, the phase function can be written\\nas\\nΘ(X) =arctan\\x12Y2\\nY1\\x13\\n=arctan\\x12fenc(X)2\\nfenc(X)1\\x13\\n, (19)\\nwhere the subscript i=1,2 indicates the ith component of\\nthe vector-valued function fenc. This indeed gives the asymp-\\ntotic phase of the oscillator, because it satisfies Θ(Xt+τ) =\\nΘ(Xt)+ωτfrom Eqs. (12) and (13), which gives ˙Θ(Xt) =ω\\nin the τ→0 limit, provided that the autoencoder is success-\\nfully trained. We can also estimate the PSF by\\nZ(θ) =gradXΘ(X)|X=fdec(θ), (20)\\nwhere the gradient can be easily computed by using automatic\\ndifferentiation of the machine learning framework55.\\nC. Loss functions\\nTo train the autoencoder, we introduce three types of loss\\nfunctions: reconstruction loss, dynamics loss (phase and de-\\nviation), and auxiliary loss. The reconstruction loss of the\\nautoencoder is the distance between the input data and the re-\\nconstructed output,\\nLrecon=Eh\\n∥Xt−fdec(fenc(Xt))∥2i\\n, (21)where ∥···∥ represents the Euclidean norm and E[g(Xt)] =\\n(1/Tmax)∑Tmax−1\\nt=0g(Xt)represents the average of a quantity\\ng(Xt)over the input data {Xt|t=1,...,Tmax}of length Tmax.\\nHere, Tmaxis the maximum number of inputs that can be ob-\\ntained from a single orbit, determined by the simulation length\\nand its computation step width.\\nThe dynamics loss is introduced to control the property of\\nthe latent space. We consider k-step evolution ( k=1,..., K)\\nof the latent variables by the neural network fstepand intro-\\nduce the following loss functions characterizing the prediction\\nerrors for the latent variables corresponding to the phase and\\ndeviation:\\nLpha=E\"\\nK\\n∑\\nk=1αk2\\n∑\\ni=1\\x0c\\x0c\\x0cfenc(Xt+k∆t)i−fk\\nstep(fenc(Xt))i\\x0c\\x0c\\x0c2#\\n,\\n(22)\\nLdev=E\"\\nK\\n∑\\nk=1αkdY\\n∑\\ni=3\\x0c\\x0c\\x0cfenc(Xt+k∆t)i−fk\\nstep(fenc(Xt))i\\x0c\\x0c\\x0c2#\\n,\\n(23)\\nwhere the latent variables are summed componentwise (the\\nsubscript iindicates the ith component), and ∆tis the sam-\\npling interval of the input data. This ∆tneeds to be scaled ap-\\npropriately to take account of the period Tof the limit cycle.\\nIn training, ∆tis fixed, but it is expected that Eqs. (12)-(14)\\nare satisfied for any τ>0 if trained with sufficient data.\\nIn the above equations, the coefficient αkrepresents the\\nweight of the prediction error for each time step kand is set as\\nαk=1/kmin(1.0,Lpha). (24)\\nNote that αkis updated at each epoch of learning (‘epoch’ is\\na single learning of the entire training dataset). The reason\\nfor choosing the weight αkas above is as follows. In the early\\nstage of the learning where small-step predictions of the phase\\nare not accurate ( Lpha>1), the above αkemphasizes the pre-\\ndiction errors for small kto accelerate the learning. As the' metadata={'source': './data/2403.06992.pdf', 'page': 3}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 5\\nlearning progresses and Lphabecomes smaller, αkapproaches\\n1 and prediction errors for larger kare also taken into account.\\nSince the loss function Lphahas a local solution that repre-\\nsents a false stationary state in the latent space corresponding\\ntoω=0, we also introduce the following loss function to\\navoid such an inappropriate solution:\\nLaux=E\\uf8ee\\n\\uf8f0 \\n1\\nBB\\n∑\\nb=1fenc(Xb\\nt)1!2\\n+ \\n1\\nBB\\n∑\\nb=1fenc(Xb\\nt)2!2\\uf8f9\\n\\uf8fb,\\n(25)\\nwhich characterizes the deviation of the center of mass of the\\nlatent variables (Yb\\n1,Yb\\n2) = ( fenc(Xb)1,fenc(Xb)2)on the\\nY1−Y2plane in the latent space from the origin. Here, Bis\\nthe batch size of the learning and XbandYb\\nidenote the input\\ndata and latent variables in a batch, respectively (‘batch’ is a\\nsmall subset of the entire dataset used in the learning). Since\\neach(Yb\\n1,Yb\\n2)exists on a unit circle in the latent space by the\\nnormalization, Lauxtakes smaller values when these variables\\ndistribute evenly on the unit circle within a batch. We use this\\nLauxin the early stage of learning to avoid local solutions and\\nturn it off as the learning progresses.\\nUsing the above three types of loss functions, we introduce\\nthe following total loss function:\\nL=wreconLrecon+wphaLpha+wampLdev+wauxLaux,(26)\\nwhere wrecon,wpha,wamp, and wauxare weight parameters con-\\ntrolling relative contributions from the individual loss func-\\ntions. We minimize this total loss function to train our phase\\nautoencoder.\\nIV. NUMERICAL EXPERIMENTS\\nA. Training datasets and parameters\\nWe evaluated the proposed phase autoencoder using four\\ntypes of limit-cycle oscillators as examples. We considered\\nthe Stuart-Landau oscillator, for which the analytical solution\\nof the phase function is known, the FitzHugh-Nagumo model,\\na fast-slow oscillator with a more complex phase function,\\nthe Hodgkin-Huxley model, a 4-dimensional oscillator real-\\nistically describing spiking neurons, and a ring network of ex-\\ncitable FitzHugh-Nagumo units exhibiting a traveling pulse,\\nan example of a high-dimensional oscillator.\\nThe data for all of these oscillators were generated by di-\\nrect numerical simulations of the models. The datasets for\\nthe training were generated as follows. First, we evaluated\\nthe limit-cycle solution and the period T. We then generated\\nthe training data and determined the the training parameters.\\nWe note that the parameters of the training datasets were the\\nsame for all oscillators. We calculated the standard deviation\\nof each component of the state vector of the oscillator overone period on the limit cycle as\\nσi=vuut1\\nNsNs−1\\n∑\\nj=0\\x12\\nX0(2πj\\nNs)i\\x132\\n− \\n1\\nNsNs−1\\n∑\\nj=0X0(2πj\\nNs)i!2\\n(27)\\nfori=1,...,dX, where Nsis the number of sample points on\\nthe limit cycle, and randomly chose Nsinitial points Xs,jas\\nXs,j=X0(2πj\\nNs)+γ2σ⊙ξ(j=0,1,···,Ns−1),\\nξ∼NdX(0,1), (28)\\nwhere σ= (σ1,...,σdX),⊙denotes the Hadamard (element-\\nwise) product, and NdX(0,1)denotes the dX-dimensional nor-\\nmal distribution. We then evolved these initial points for γ1T.\\nIn the above setting, Ns,γ1, and γ2are the parameters for\\ndata generation, which affect the estimation accuracy of the\\nautoencoder. For example, when γ2=0, the autoencoder\\ncould not accurately estimate the asymptotic phase near the\\nlimit cycle and, consequently, it was difficult to estimate the\\nphase sensitivity function. Also, when γ1was too large, most\\nof the data were on the limit cycle, which adversely affected\\nthe learning and did not capture the relaxation dynamics. In\\nall of the following numerical experiments, unless otherwise\\nnoted, we used the same set of parameter values, Ns=1000,\\nγ1=3, and γ2=0.5, respectively.\\nOur autoencoder has a standard structure53except for the\\nnormalization of the latent variables Y1andY2, which con-\\nsists of linear transformations, ReLU56, and batch normal-\\nization57(Fig. 2). The encoder has two hidden layers, each\\nwith 100 units, and the decoder has three hidden layers, each\\nwith 100 units. The epoch size was 50 and the learning rate' metadata={'source': './data/2403.06992.pdf', 'page': 4}\n",
            "page_content='ization57(Fig. 2). The encoder has two hidden layers, each\\nwith 100 units, and the decoder has three hidden layers, each\\nwith 100 units. The epoch size was 50 and the learning rate\\nwas 0 .001. The weights of individual loss functions were\\nwrecon=1.0,wpha=0.5,wamp=0.5, and waux=2.0, and the\\nmaximal step of evolution by the neural network fstepwas\\nK=20. Additionally, we varied wphaandwauxtowpha=5.0\\nandwaux=0.0 after the initial stage of training, i.e., when\\nthe loss functions satisfied Lpha<0.01 and Laux<0.05. The\\ninputs were normalized by the standard deviations of the indi-\\nvidual variables as a preprocessing of the data from the limit-\\ncycle oscillators. We used the standard machine-learning li-\\nbrary PyTorch58for the actual implementation of our phase\\nautoencoder.\\nB. Stuart-Landau oscillator\\nFirst, we applied our method to the Stuart-Landau (SL) os-\\ncillator3,59,60, which is a normal form of the supercritical Hopf\\nbifurcation described by\\nd\\ndt\\x14\\nx1\\nx2\\x15\\n=\\x14\\nx1−αx2−(x1−βx2)(x2\\n1+x2\\n2)\\nαx1+x2−(βx1+x2)(x2\\n1+x2\\n2)\\x15\\n, (29)\\nwhere x1and x2are the variables and αandβare the\\nparameters. The limit-cycle solution is (x1(t),x2(t)) =\\n(cosωt,sinωt)if we choose (1,0)as the initial condition,' metadata={'source': './data/2403.06992.pdf', 'page': 4}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 6\\nwhich is a unit circle on the x1−x2plane, where the natu-\\nral frequency is ω=α−β. We set α=2πandβ=1 in\\nthe numerical experiment, which gives the oscillation period\\nT≃1.19. We trained the autoencoder and estimated the phase\\nfunction of the SL oscillator.\\nWe set the number of initial points as n=1000, the number\\nof maximal evolution steps as K=20, and the training sample\\ninterval as ∆t=0.05. Here, we chose Kand∆tso that the\\nproduct of Kand∆troughly coincides with the oscillation\\nperiod T. We also used the same values for nandKin the\\nsubsequent numerical experiments.\\nFirst, we examined whether the correct phase values were\\nassigned to the oscillator states on the limit cycle. We sampled\\nthe oscillator state X0(θ)on the limit cycle with regular inter-\\nvals of the phase θ. We then estimated the phase values of the\\nsampled oscillator states by the autoencoder to confirm if the\\nphase function is properly embedded. Figure 3 (a) compares\\nthe original phase θand the estimated phase ˆθfromX0(θ),\\nshowing that the asymptotic phase is accurately estimated by\\nthe trained autoencoder. We also verified that the limit cycle is\\nembedded in the latent space on a plane with Y3=0. Figure 3\\n(b) shows the decoder’s output of the limit cycle ˆχcalculated\\nby setting Y3=0 and sampling (Y1,Y2)at regular intervals of\\nthe phase θasY= (cosθ,sinθ,0)and compare it with the\\noriginal limit cycle (unit circle) χof the SL oscillator, show-\\ning good agreement.\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/2\\n3/2\\n2\\n/uni0000000b/uni00000044/uni0000000c\\n/uni00000014\\n /uni00000013 /uni00000014\\nx1/uni00000014\\n/uni00000013/uni00000014x2/uni0000000b/uni00000045/uni0000000c\\nFIG. 3. (a) Comparison of the estimated phase (red solid line) ˆθand\\nthe true phase (black dotted line) θof the SL oscillator. (b) Recon-\\nstructed limit cycle ˆχby the autoencoder (red solid curve) compared\\nwith the true limit cycle χ(black dotted curve).\\nNext, we examined the asymptotic phase estimated by the\\nautoencoder. The true asymptotic phase function of the SL\\noscillator is analytically given by13\\nΘ(x1,x2) =arctan (x2/x1)−logq\\nx2\\n1+x2\\n2. (30)\\nFigure 4 compares the estimated phase function with the true\\nphase function on the x1−x2plane. It can be seen that\\nthe trained autoencoder reproduces the phase function well\\naround the limit cycle. For the oscillator states far from the\\nlimit cycle, the estimation accuracy is degraded due to lack of\\ntraining data in these regions.\\nWe also compared the estimated PSF with the true PSF in\\nFig. 5, showing good agreement. Here, the true PSF is analyt-ically obtained from the true asymptotic phase as13\\nZ(θ) = ( Z1(θ),Z2(θ))\\n= (−sinθ−βcosθ,cosθ−βsinθ). (31)\\nSince the PSF is a derivative of the phase function, the results\\nwere sensitive to small errors in the estimated phase func-\\ntion and therefore exhibited small fluctuations around the true\\ncurves.\\nFIG. 4. Asymptotic phase of the Stuart-Landau oscillator. (a) Esti-\\nmated phase function by the autoencoder; (b) True phase function.\\nThe colors represent the phase value from 0 to 2 π(discretized for\\nvisual clarity), where (x1,x2) = ( 1,0)is chosen as the origin of the\\nphase with θ=0. The black circle in each figure represents the limit\\ncycle.\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000015\\n/uni00000014\\n/uni00000013/uni00000014/uni00000015Z/uni0000000b/uni00000044/uni0000000c\\nZ1\\nZ1\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000015\\n/uni00000014\\n/uni00000013/uni00000014/uni00000015Z/uni0000000b/uni00000045/uni0000000c\\nZ2\\nZ2\\nFIG. 5. Phase sensitivity function (PSF) of the SL oscillator. Each\\nfigure shows the estimated PSF (red solid curve) and the true PSF\\n(black dashed curve). (a) x1component; (b) x2component.\\n/uni00000014\\n /uni00000013 /uni00000014 /uni00000015\\nx1/uni00000014\\n/uni00000013/uni00000014x2\\nxs,0=1.5/uni00000041/uni00000015\\nxs,0=1.5/uni00000041/uni00000014\\nxs,0=1.5/uni00000041/uni00000013\\nxs,0=1.5/uni00000041/uni00000010/uni00000014\\nxs,0=1.5/uni00000041/uni00000010/uni00000015' metadata={'source': './data/2403.06992.pdf', 'page': 5}\n",
            "page_content='xs,0=1.5/uni00000041/uni00000015\\nxs,0=1.5/uni00000041/uni00000014\\nxs,0=1.5/uni00000041/uni00000013\\nxs,0=1.5/uni00000041/uni00000010/uni00000014\\nxs,0=1.5/uni00000041/uni00000010/uni00000015\\n0 T 2T 3T 4T/uni00000013/uni00000011/uni00000015\\n/uni00000013/uni00000011/uni00000014\\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014Y3\\nxs,0=1.5/uni00000041/uni00000015\\nxs,0=1.5/uni00000041/uni00000014\\nxs,0=1.5/uni00000041/uni00000013\\nxs,0=1.5/uni00000041/uni00000010/uni00000014\\nxs,0=1.5/uni00000041/uni00000010/uni00000015\\nFIG. 6. Time evolution of the latent variable Y3for five trajectories\\nwith different initial points of the SL oscillator. (a) Dynamics in the\\noriginal state space. (b) Time evolution of the latent variable Y3.\\nFinally, we describe the evolution of the third latent variable\\nY3characterizing the deviation of the oscillator state from the\\nlimit cycle. Figure 6 shows how Y3evolved for five different' metadata={'source': './data/2403.06992.pdf', 'page': 5}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 7\\ninitial states converging to the limit cycle. We observe that\\nY3decayed to zero roughly exponentially as intended. How-\\never, it did not converge to zero accurately but exhibited small-\\namplitude oscillations due to estimation errors of the trained\\nautoencoder.\\nC. FitzHugh-Nagumo model\\nAs the second example, we applied the proposed method to\\nthe FitzHugh-Nagumo (FHN) model1,14described by\\nd\\ndt\\x14\\nx1\\nx2\\x15\\n=\\x14\\nx1−x3\\n1/3−x2+I\\nε(x1+a−bx2)\\x15\\n, (32)\\nwhere x1,x2are the variables and ε=0.08,a=0.7,b=0.8,\\nandI=0.8 are the parameters. This system has a stable limit\\ncycle of period T≃36.56 with fast-slow dynamics. We set the\\nnumber of initial points as n=1000, the number of maximal\\nevolution steps as K=20, and the training sample interval as\\n∆t=1.8.\\nAs in the case of the SL oscillator, we first examined if\\nthe phase values were properly assigned to the states on the\\nlimit cycle and if the limit cycle was embedded in the plane\\nwith Y3=0 in the latent space. As Fig. 7 shows, the phase\\non the limit cycle was accurately learned by the autoencoder.\\nThe limit cycle was also reconstructed reasonably well, but it\\nslightly shifted inward from the original limit cycle near the\\nnullclines. We presume that the cause of this small shift is that\\nthe training data generated by our method were slightly biased\\ninside the true limit cycle.\\nNext, we estimated the asymptotic phase using the autoen-\\ncoder and compared with the result obtained by direct numer-\\nical calculation in Fig. 8. The autoencoder reproduced the\\nphase function near the limit cycle well, but there were dis-\\ncrepancies around the unstable fixed point inside the limit cy-\\ncle and also in the regions away from the limit cycle. This\\nis because the training data did not contain the trajectories\\npassing through these regions and therefore the autoencoder\\ncould not learn the dynamics in these regions. In practice,\\nbecause the limit cycle is strongly attracting in general, the\\noscillator state is almost always near the limit cycle and rarely\\nvisits these regions. Thus, these discrepancies are not a se-\\nrious problem in using the trained autoencoder for analyzing\\nand controlling synchronization.\\nFinally, the PSF Z= (Z1,Z2)estimated by the autoencoder\\nand the true PSF obtained from the adjoint equation are com-\\npared in Fig. 9. Since estimating the derivatives enhances\\nnoise, the estimated PSF was jagged, but still mostly repro-\\nduced the true PSF; reflecting the discrepancy of the trajecto-\\nries near the nullclines, we also observe discrepancies in the\\nPSF near the nullclines.\\nD. Hodgkin-Huxley model\\nAs the third example, we applied the proposed method to\\nthe Hodgkin Huxley (HH) model of spiking neurons61. The\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/2\\n3/2\\n2\\n/uni0000000b/uni00000044/uni0000000c\\n/uni00000015\\n /uni00000013 /uni00000015\\nx1/uni00000013/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013x2/uni0000000b/uni00000045/uni0000000c\\nFIG. 7. (a) Comparison of the estimated phase ˆθ(red solid line) and\\nthe true phase θ(black dotted curve) of the FHN model. (b) Recon-\\nstructed limit cycle ˆχby the autoencoder (red solid curve) compared\\nwith the true limit cycle χ(black dotted curve).\\nFIG. 8. Asymptotic phase of the FHN model. (a) Estimated phase\\nby the autoencoder. (b) Analytical phase value. The colors represent\\nthe phase value from 0 to 2 π(discretized for visual clarity), where\\n(x1,x2) = (−0.445,0)is chosen as the origin of the phase ( θ=0).\\nThe black curve in each figure represents the limit cycle.\\nHH model realistically describes how the action potentials in\\nneurons are generated and is given by\\nd\\ndt\\uf8ee\\n\\uf8ef\\uf8f0V\\nm\\nh\\nn\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0(GNam3h(ENa−V)+GKn4(EK−V)\\n+GL(EL−V)+I)/C\\nαm(V)(1.0−m)−βm(V)m\\nαh(V)(1.0−h)−βh(V)h\\nαn(V)(1.0−n)−βn(V)n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb,(33)\\nwhere Vis the membrane potential, m,h,nare the channel' metadata={'source': './data/2403.06992.pdf', 'page': 6}\n",
            "page_content='d\\ndt\\uf8ee\\n\\uf8ef\\uf8f0V\\nm\\nh\\nn\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0(GNam3h(ENa−V)+GKn4(EK−V)\\n+GL(EL−V)+I)/C\\nαm(V)(1.0−m)−βm(V)m\\nαh(V)(1.0−h)−βh(V)h\\nαn(V)(1.0−n)−βn(V)n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb,(33)\\nwhere Vis the membrane potential, m,h,nare the channel\\nvariables, and the voltage-dependent rate constants are given\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000014\\n/uni00000013/uni00000014Z/uni0000000b/uni00000044/uni0000000c\\nZ1\\nZ1\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000018/uni00000011/uni00000013\\n/uni00000015/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018Z/uni0000000b/uni00000045/uni0000000c\\nZ2\\nZ2\\nFIG. 9. Phase sensitivity function (PSF) of the FHN model. Each\\nfigure shows the estimated FHN (red solid curve) and the true PSF\\n(black dashed curve). (a) x1component; (b) x2component. True PSF\\nwas calculated by solving the adjoint equation.' metadata={'source': './data/2403.06992.pdf', 'page': 6}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 8\\nby\\nαm(V) =0.1(V+40)(1−exp(−(V+40)/10)),\\nβm(V) =0.1exp(−(V+65)/18),\\nαh(V) =0.07exp (−(V+65)/20),\\nβh(V) =1/(1+exp(−V+35)/10)),\\nαn(V) =0.01(V+55)/(1−exp(−(V+55)/10)),\\nβn(V) =0.125exp (−(V+65)/80). (34)\\nWe set the parameters as C=1.0,GNa=120.0,GK=\\n36.0,GL=0.3,ENa=50.0,EK=−77.0, and EL=−54.4,\\nfor which the HH model possessed a stable limit cycle of pe-\\nriodT≃10.12. For the training, we set the number of initial\\npoints as n=1000, the number of maximal evolution steps\\nasK=20, and the sampling interval as ∆t=0.5. Figure 10\\nshows the evolution of the variables V,m,h, and nfor one\\noscillation period.\\n/uni00000013 /2\\n 3/2\\n2\\n/uni0000001a/uni00000018\\n/uni00000018/uni00000013\\n/uni00000015/uni00000018\\n/uni00000013V\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013\\nm\\nh\\nn\\nFIG. 10. Evolution of the state variables on the limit cycle of\\nthe Hodgkin-Huxley model. Membrane potential V(left) and other\\nchannel variables m,h, and n(right) vs. phase.\\nAs in the previous cases, we confirmed that appropriate\\nphase values were assigned to the states on the limit cycle\\n(Fig. 11 (a)). We also examined if the limit cycle could be\\nreconstructed from the embedded representation. Since the\\nHH model is 4-dimensional, we projected the trajectory on\\nthe plane spanned by Vand one of other variables, m(Fig. 11\\n(b)), h(Fig. 11 (c)), or n(Fig. 11 (d)). The reconstructed limit\\ncycle agreed with the original one reasonably well, but, as in\\nthe case with the FHN model, it was somehow shifted inward\\nfrom the original limit cycle.\\nThe PSF Z= (ZV,Zm,Zh,Zn)estimated by the autoencoder\\nis compared with the true PSF obtained from the adjoint equa-\\ntion in Fig. 12, showing reasonably good agreement; the devi-\\nations in the hcomponent from the true value near θ=π/4 is\\ndue to the difficulty in estimating the asymptotic phase around\\nthis point because of the sudden change in V. We note that, in\\nphysiological experiments, only the Vcomponent of the HH\\nmodel can be stimulated and thus the Vcomponent of the PSF\\nis important practically; the PSF with respect to the channel\\nvariables m,h, and nwere calculated here to verify the validity\\nof our trained autoencoder.\\nE. Collectively Oscillating Network\\nAs the final example, we applied the proposed method to a\\ncollectively oscillating network (CON) model as an example\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/2\\n3/2\\n2\\n/uni0000000b/uni00000044/uni0000000c\\n/uni0000001a/uni00000018\\n /uni00000018/uni00000013\\n /uni00000015/uni00000018\\n /uni00000013\\nV/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013m/uni0000000b/uni00000045/uni0000000c\\n/uni0000001a/uni00000018\\n /uni00000018/uni00000013\\n /uni00000015/uni00000018\\n /uni00000013\\nV/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017h/uni0000000b/uni00000046/uni0000000c\\n/uni0000001a/uni00000018\\n /uni00000018/uni00000013\\n /uni00000015/uni00000018\\n /uni00000013\\nV/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001bn/uni0000000b/uni00000047/uni0000000c\\nFIG. 11. (a) Comparison of the estimated phase ˆθ(red solid line)\\nand the true phase θ(black dotted curve) of the HH model. (b-d)\\nReconstructed limit cycle ˆχprojected on (b) V−m, (c) V−h, and\\n(d)V−nplanes (red solid curves) compared with the true limit cycle\\nχ(black dotted curves).\\n/uni00000013 /2\\n 3/2\\n2' metadata={'source': './data/2403.06992.pdf', 'page': 7}\n",
            "page_content='Reconstructed limit cycle ˆχprojected on (b) V−m, (c) V−h, and\\n(d)V−nplanes (red solid curves) compared with the true limit cycle\\nχ(black dotted curves).\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000018Z/uni0000000b/uni00000044/uni0000000c\\nZV\\nZV\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000015/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013/uni0000001a/uni00000011/uni00000018Z/uni0000000b/uni00000045/uni0000000c\\nZm\\nZm\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000018/uni00000011/uni00000013\\n/uni00000015/uni00000011/uni00000018\\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018Z/uni0000000b/uni00000046/uni0000000c\\nZh\\nZh\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000017/uni00000013\\n/uni00000015/uni00000013\\n/uni00000013Z/uni0000000b/uni00000047/uni0000000c\\nZn\\nZn\\nFIG. 12. Phase sensitivity function (PSF) of the HH model. Each fig-\\nure shows the estimated PSF (red solid curve) and the true PSF (black\\ndashed curve). (a) Vcomponent; (b) mcomponent; (c) hcomponent;\\n(d)ncomponent. The true PSF was calculated by solving the adjoint\\nequation.\\nof a higher-dimensional limit cycle62,63. This model consists\\nof multiple FitzHugh-Nagumo elements as the nodes coupled\\nover the links, described by\\nd\\ndt\\x14\\nui\\nvi\\x15\\n=\\x14ε(x1+a−bx2)\\nx1−x3\\n1/3−x2+I\\x15\\n+N\\n∑\\nj=1Ai j\\x14\\n0\\nvj−vi\\x15\\n(35)\\nfor(i=1,2,···,N), where Nis the number of elements, ui,vi\\nare the variables of each element, and ε=0.08,a=0.7,b=\\n0.8, and I=0.32 are the parameters. We consider a ring net-\\nwork with N=10 and assume that the adjacency matrix is\\ngiven by\\nAi j=\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3+0.3 if i=j+1,\\n−0.3 if i=j−1,\\n0 (otherwise),(36)' metadata={'source': './data/2403.06992.pdf', 'page': 7}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 9\\nfori,j=1,...,10, where i=11 is identified with i=1 and\\ni=0 is identified with i=10. This system has a stable limit\\ncycle of period T≃17.7.\\nFor the training, we set the number of initial points as\\nn=1000, the number of maximal evolution steps as K=20,\\nand the sampling interval as ∆t=0.888. Figure 13 shows the\\nevolution of the variables uiandvi(i=1,2,···,10)for one\\noscillation period. The stable limit cycle of this model corre-\\nsponds to a traveling pulse rotating around the ring network63.\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni0000000b/uni00000044/uni0000000cu1\\nu2u3\\nu4u5\\nu6u7\\nu8u9\\nu10\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000015\\n/uni00000014\\n/uni00000013/uni00000014/uni00000015/uni0000000b/uni00000045/uni0000000cv1\\nv2v3\\nv4v5\\nv6v7\\nv8v9\\nv10\\nFIG. 13. Limit cycle of the CON model. Evolution of the state vari-\\nables for one oscillation period. (a) uiand (b) vi(i=1,2,···,10)vs.\\nphase.\\nAs in the previous cases, we verified that the state on the\\nlimit cycle was assigned suitable phase values (Fig. 14 (a)).\\nWe also examined if the limit cycle could be reconstructed\\nfrom the embedded representation. Since the CON model is\\n20-dimensional, we showed only the trajectory on the plane\\nspanned by u1andv1(Fig. 14 (b)). The reconstructed limit\\ncycle agreed with the original one reasonably well, but, as in\\nthe case with FHN model and HH model, the state embedded\\natY3=0 was somehow shifted inward from the original limit\\ncycle.\\n/uni00000013 /2\\n 3/2\\n2\\n/uni00000013/2\\n3/2\\n2\\n/uni0000000b/uni00000044/uni0000000c\\n/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018\\nu1/uni00000015\\n/uni00000013/uni00000015v1/uni0000000b/uni00000045/uni0000000c\\nFIG. 14. (a) Comparison of the estimated phase ˆθ(red solid line)\\nand the true phase θ(black dotted curve) of the CON model. (b)\\nReconstructed limit cycle ˆχprojected on u1−v1plane (red solid\\ncurve) compared with the true limit cycle χ(black dotted curve).\\nThe PSF Z= (Zu1,...,Zu10,Zv1,...,Zv10)with respect to the\\nvariables u1,...,u10andv1,...,v10estimated by the autoen-\\ncoder is compared with the true PSF obtained from the adjoint\\nequation in Fig. 15. For the PSF estimated by the autoencoder,\\nthe plotted data points are smoothed by the Fourier approxi-\\nmation truncated up to the 5th modes to reduce fluctuations.\\nEach component of the PSF is roughly estimated, though con-\\nsiderable deviations from the true curve are observed. This\\nis because the system’s dimension is high, dX=20, and theautoencoder requires a large amount of data for accurate train-\\ning. In practice, however, we typically need only the first sev-\\neral Fourier modes of the PSF in the phase-reduction analysis,\\nand the results obtained here could already be used for ex-\\nplaining synchronization dynamics of the CON model.\\nFIG. 15. Phase sensitivity function (PSF) of the CON model. Each\\nfigure shows a single component ( u1,...,u10orv1,...,v10) of the es-\\ntimated PSF (red points) and the true PSF (black dashed curve).\\nV. AUTOENCODER-AIDED SYNCHRONIZATION\\nHere, as an application of the trained phase autoencoder,\\nwe present a simple method for globally synchronizing two\\noscillators. This method gives a coupling function between\\nthe two oscillators that yields a simple pair of sinusoidally\\ncoupled phase oscillators when the phase reduction is per-\\nformed, which has a single in-phase synchronized state as a\\nstable fixed point.\\nWe consider two weakly coupled oscillators with identical\\nproperties described by\\nd\\ndtX1=F(X1)+εG12(X1,X2),\\nd\\ndtX2=F(X2)+εG21(X2,X1), (37)\\nwhere εis a coupling intensity and Gi j:RdX×RdX→RdXis' metadata={'source': './data/2403.06992.pdf', 'page': 8}\n",
            "page_content='We consider two weakly coupled oscillators with identical\\nproperties described by\\nd\\ndtX1=F(X1)+εG12(X1,X2),\\nd\\ndtX2=F(X2)+εG21(X2,X1), (37)\\nwhere εis a coupling intensity and Gi j:RdX×RdX→RdXis\\na coupling function. Using the phase autoencoder, we couple\\nthese two oscillators by the following coupling function:\\nθi=fenc(Xi), (38)\\nGi j(Xi,Xj) =sin(θj−θi)∂\\n∂θifdec(θi), (39)\\nwhere (i,j) = ( 1,2)or(2,1).' metadata={'source': './data/2403.06992.pdf', 'page': 8}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 10\\nThe reduced phase equation is then given by\\nd\\ndtθi=Z(θi)·d\\ndtXi\\n=Z(θi)·(F(Xi)+εGi j(Xi,Xj))\\n=ω+εZ(θi)·Gi j(Xi,Xj)\\n=ω+εsin(θj−θi)\\x12\\nZ(θi)·∂\\n∂θifdec(θi)\\x13\\n=ω+εsin(θj−θi), (40)\\nwhere we used Z(θi)·∂\\n∂θifdec(θi) =1 from the normaliza-\\ntion condition, Eq. (7), because X0(θi) =fdec(θi)if the de-\\ncoder can accurately reconstruct the oscillator state on the\\nlimit cycle. Since the phase difference φ=θ1−θ2obeys\\nd\\ndtφ=−2εsinφ, the two oscillators are expected to converge\\nto the mutually in-phase synchronized state φ=0 as t→∞\\nfrom any initial phase difference φ(0)except for π.\\nWe here illustrate the autoencoder-aided phase synchro-\\nnization using the HH model. We prepared two HH models\\nexhibiting limit-cycle oscillations with the same parameters\\nas before, and introduced the mutual coupling from t=3T\\nwith the intensity ε=0.05 as in Eq. (37). Figure 16 (a) shows\\nthe time evolution of the HH models, and (b) shows the esti-\\nmated phases and their phase difference. We can confirm that\\nthe two HH models are synchronized with each other when\\nthe mutual coupling is introduced, confirming the validity of\\nour proposed coupling scheme.\\n/uni00000013 /uni00000037 /uni00000015/uni00000037 /uni00000016/uni00000037 /uni00000017/uni00000037 /uni00000018/uni00000037 /uni00000019/uni00000037 /uni0000001a/uni00000037 /uni0000001b/uni00000037 /uni0000001c/uni00000037 /uni00000014/uni00000013/uni00000037 /uni00000014/uni00000014/uni00000037/uni00000018/uni00000013\\n/uni00000013/uni00000018/uni00000013\\n/uni0000000b/uni00000044/uni0000000c /uni00000036/uni00000057/uni00000044/uni00000055/uni00000057/uni00000003/uni00000050/uni00000058/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni0000004c/uni00000051/uni0000004aV/uni0000000b/uni00000052/uni00000056/uni00000046/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000057/uni00000052/uni00000055/uni00000003/uni00000014/uni0000000c V/uni0000000b/uni00000052/uni00000056/uni00000046/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000057/uni00000052/uni00000055/uni00000003/uni00000015/uni0000000c\\n/uni00000013 /uni00000037 /uni00000015/uni00000037 /uni00000016/uni00000037 /uni00000017/uni00000037 /uni00000018/uni00000037 /uni00000019/uni00000037 /uni0000001a/uni00000037 /uni0000001b/uni00000037 /uni0000001c/uni00000037 /uni00000014/uni00000013/uni00000037 /uni00000014/uni00000014/uni00000037/uni00000013\\n2\\n/uni0000000b/uni00000045/uni0000000c /uni00000053/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni0000000b/uni00000052/uni00000056/uni00000046/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000057/uni00000052/uni00000055/uni00000003/uni00000014/uni0000000c /uni00000053/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni0000000b/uni00000052/uni00000056/uni00000046/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000057/uni00000052/uni00000055/uni00000003/uni00000015/uni0000000c /uni00000053/uni0000004b/uni00000044/uni00000056/uni00000048/uni00000003/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048\\nFIG. 16. Autoencoder-aided mutual synchronization of two HH\\nmodels. (a) Evolution of the membrane potential Vof the HH models\\n1 and 2. The red line shows the time after which the mutual coupling\\nis turned on. (b) Estimated phases and their difference. The blue line\\nshows the phase of the oscillator 1, the orange line shows the phase\\nof the oscillator 2, and the green line shows the phase difference.\\nVI. DISCUSSION\\nIn our phase autoencoder, we assumed that the latent vari-\\nables Y1andY2represent the asymptotic phase and rotate on\\na unit circle with a constant frequency ω, and that another\\nlatent variable Y3represents the overall deviation of the os-\\ncillator state from the limit cycle and exponentially decays to\\nzero with a constant rate λ. Here, we discuss their relationship' metadata={'source': './data/2403.06992.pdf', 'page': 9}\n",
            "page_content='latent variable Y3represents the overall deviation of the os-\\ncillator state from the limit cycle and exponentially decays to\\nzero with a constant rate λ. Here, we discuss their relationship\\nwith the Koopman eigenfunctions of the oscillator.The Koopman operator25,26,41Kτ(τ≥0) of Eq. (1) is de-\\nfined as Kτg(X(t)) = g(X(t+τ))for general observables\\ng:RdX→C. It describes the evolution of observables in the\\nfunction space and is a linear operator even if the underly-\\ning dynamics of Xis nonlinear. For limit-cycle oscillators,\\nthe phase-amplitude reduction frameworks have been devel-\\noped recently based on the Koopman operator theory24–27; it\\nis well known that the asymptotic phase and amplitude can be\\ndefined by using the principal Koopman eigenfunction asso-\\nciated with the Floquet exponent of the limit cycle.\\nFirst, the latent variables Y1andY2approximately represent\\nthe Koopman eigenfunction with a pure imaginary exponent\\niω(eigenvalue eiωτ), because if we define ψ(X) =Y1+iY2,\\nit satisfies the eigenvalue equation Kτψ(X) =eiωτψ(X)\\nfrom Eqs. (12)-(14). Next, for two-dimensional oscillators\\nwith dX=2, the latent variable Y3approximately corresponds\\nto the Koopman eigenfunction with a negative exponent λ\\n(eigenvalue eλτ), that is, if we define r(X) =Y3, it satisfies\\nKτr(X) =eλτr(X)and characterizes the decay of the devi-\\nation (amplitude) of the oscillator state from the limit cycle.\\nThough we did not consider in the present study, for higher-\\ndimensional oscillators, we could assume the dimension dYof\\nthe latent space to be larger than 3 and similarly introduce la-\\ntent variables Yi(i=4,···,dY)that exponentially decay with\\nthe rates λiasYi,t+τ=eλiτYi,t. They will then correspond to\\nthe other Koopman eigenfunctions with exponents λi(eigen-\\nvalues eλiτ), provided that λiare real. If they are complex, Yi\\nwill not correspond to the Koopman eigenfunctions directly,\\nbut they still characterize the deviation of the oscillator state\\nfrom the limit cycle.\\nIn this study, we focused solely on the asymptotic phase\\nrepresented by Y1andY2, and introduced only a single vari-\\nableY3for characterizing the overall deviation from the limit\\ncycle even for the oscillators with dX>2. Also, though we\\nassumed that Y3decays to 0 exponentially in Eq. (14), the\\ntrained autoencoder did not accurately reproduce it as can be\\nseen in Fig. 6 because of the limited training data. There-\\nfore, the latent variable Y3in our phase autoencoder does not\\ndirectly correspond to the Koopman eigenfunction with the\\nslowest decaying exponent used in the phase-amplitude re-\\nduction theory26. However, it still captures the deviation of\\nthe oscillator state from the limit cycle as can be observed\\nin Figs. 3(b), 8(b), 12, and 14 showing that the limit cycles\\nare reasonably reconstructed by setting Y3=0. Thus, though\\nEq. (14) is not strictly satisfied by the trained phase autoen-\\ncoder, which is actually not easy to realize in practice, it suf-\\nfices our purpose of learning the asymptotic phase from time-\\nseries data.\\nOur phase encoder can be regarded as a kind of physics-\\ninformed machine learning, which takes advantage of incor-\\nporating physical properties into the learning models64,65. In\\nparticular, there are many studies whose target is Hamiltonian\\nsystems66–68. Recently, Toth et al.68proposed a variational\\nautoencoder whose latent variables correspond to generalized\\ncoordinate and momentum, and Daigavane et al.69proposed\\na network whose latent variables correspond to the action and\\nangle variables of Hamiltonian systems. Considering the for-\\nmal resemblance of the phase-amplitude variables of dissi-' metadata={'source': './data/2403.06992.pdf', 'page': 9}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 11\\npative limit-cycle oscillators with a decaying amplitude and\\nconstantly increasing phase to the action-angle coordinates of\\nHamiltonian oscillators with a constant action variable and\\nconstantly increasing phase, we may interpret the proposed\\nphase autoencoder in this study as a kind of physics-informed\\nnetwork similar to Hamiltonian neural networks.\\nVII. CONCLUSIONS\\nWe presented a phase autoencoder whose latent variables\\nrepresent the asymptotic phase of a limit-cycle oscillator. We\\nhave shown that the asymptotic phases and the phase sen-\\nsitivity functions can be accurately estimated by the trained\\nautoencoder for several types of low-dimensional limit-cycle\\noscillators. For high-dimensional network, the phase values\\nwere assigned on the limit cycle reasonably well. However,\\nthe phase sensitivity function could not be estimated accu-\\nrately, though its characteristics were qualitatively captured.\\nThis is due to insufficient data, accuracy of learning, and also\\nlimitation of the learning model, and further improving the\\naccuracy of the phase autoencoder for high-dimensional sys-\\ntems is a future problem. As an application, we proposed a\\nglobal synchronization method that uses the trained autoen-\\ncoder demonstrated its effectiveness by a numerical simula-\\ntion of the Hodgkin-Huxley model.\\nACKNOWLEDGMENTS\\nWe thank N. Namura for useful discussion. H.N. ac-\\nknowledges financial support from JSPS KAKENHI (Nos.\\nJP22K11919 and JP22H00516) and JST CREST (No. JP-\\nMJCR1913). K.T. acknowledges financial support from the\\nUS AFOSR (grant number FA9550-21-1-0178) and the Van-\\nnevar Bush Faculty Fellowship (grant number N00014-22-1-\\n2798).\\nDATA AVAILABILITY STATEMENT\\nThe data that support the findings of this study are available\\nwithin the article. The source codes used for generating the\\nfigures will be made available to the public with the publica-\\ntion of this paper.\\n1A. T. Winfree, The geometry of biological time , V ol. 2 (Springer, 1980).\\n2A. Pikovsky, M. Rosenblum, J. Kurths, and A. Synchronization, “A univer-\\nsal concept in nonlinear sciences,” Self 2, 3 (2001).\\n3Y . Kuramoto, “Chemical oscillations, waves and turbulence. mineola,”\\n(2003).\\n4T. Stankovski, V . Ticcinelli, P. V . McClintock, and A. Stefanovska, “Cou-\\npling functions in networks of oscillators,” New Journal of Physics 17\\n(2015), 10.1088/1367-2630/17/3/035002.\\n5T. Stankovski, V . Ticcinelli, P. V . McClintock, and A. Stefanovska, “Neural\\ncross-frequency coupling functions,” Frontiers in Systems Neuroscience 11\\n(2017), 10.3389/fnsys.2017.00033.\\n6L. Borgius, H. Nishimaru, V . Caldeira, Y . Kunugise, P. Löw, R. Reig, S. Ito-\\nhara, T. Iwasato, and O. Kiehn, “Spinal glutamatergic neurons defined by\\nepha4 signaling are essential components of normal locomotor circuits,”\\nJournal of Neuroscience 34(2014), 10.1523/JNEUROSCI.4992-13.2014.7J. J. Collins and I. N. Stewart, “Coupled nonlinear oscillators and the\\nsymmetries of animal gaits,” Journal of Nonlinear Science 3(1993),\\n10.1007/BF02429870.\\n8R. Kobayashi, H. Nishimaru, and H. Nishijo, “Estimation of ex-\\ncitatory and inhibitory synaptic conductance variations in motoneu-\\nrons during locomotor-like rhythmic activity,” Neuroscience 335 (2016),\\n10.1016/j.neuroscience.2016.08.027.\\n9T. Funato, Y . Yamamoto, S. Aoi, T. Imai, T. Aoyagi, N. Tomita, and\\nK. Tsuchiya, “Evaluation of the phase-dependent rhythm control of human\\nwalking using phase response curves,” PLoS Computational Biology 12\\n(2016), 10.1371/journal.pcbi.1004950.\\n10B. Kralemann, M. Frühwirth, A. Pikovsky, M. Rosenblum, T. Kenner,\\nJ. Schaefer, and M. Moser, “In vivo cardiac phase response curve eluci-\\ndates human respiratory heart rate variability,” Nature Communications 4\\n(2013), 10.1038/ncomms3418.\\n11M. Garcia, A. Chatterjee, A. Ruina, and M. Coleman, “The simplest walk-\\ning model: Stability, complexity, and scaling,” Journal of Biomechanical\\nEngineering 120(1998), 10.1115/1.2798313.' metadata={'source': './data/2403.06992.pdf', 'page': 10}\n",
            "page_content='11M. Garcia, A. Chatterjee, A. Ruina, and M. Coleman, “The simplest walk-\\ning model: Stability, complexity, and scaling,” Journal of Biomechanical\\nEngineering 120(1998), 10.1115/1.2798313.\\n12M. Rohden, A. Sorge, M. Timme, and D. Witthaut, “Self-organized syn-\\nchronization in decentralized power grids,” Physical review letters 109,\\n064101 (2012).\\n13H. Nakao, “Phase reduction approach to synchronisation of nonlinear os-\\ncillators,” Contemporary Physics 57, 188–214 (2016).\\n14G. B. Ermentrout and D. H. Terman, “Mathematical foundations of neuro-\\nscience,” (2010).\\n15B. Monga, D. Wilson, T. Matchen, and J. Moehlis, “Phase reduction and\\nphase-based optimal control for biological systems: a tutorial,” Biological\\ncybernetics 113, 11–46 (2019).\\n16Y . Kuramoto and H. Nakao, “On the concept of dynamical reduction: the\\ncase of coupled oscillators,” Philosophical Transactions of the Royal Soci-\\nety A 377, 20190041 (2019).\\n17B. Ermentrout, Y . Park, and D. Wilson, “Recent advances in coupled os-\\ncillator theory,” Philosophical Transactions of the Royal Society A 377,\\n20190092 (2019).\\n18L. Glass and M. C. Mackey, From clocks to chaos: The rhythms of life\\n(Princeton University Press, 1988).\\n19S. Shirasaka, W. Kurebayashi, and H. Nakao, “Phase-amplitude reduction\\nof limit cycling systems,” The Koopman Operator in Systems and Control:\\nConcepts, Methodologies, and Applications , 383–417 (2020).\\n20D. Wilson, “An adaptive phase-amplitude reduction framework without\\no(ε) constraints on inputs,” SIAM Journal on Applied Dynamical Systems\\n21, 204–230 (2022).\\n21K. Taira and H. Nakao, “Phase-response analysis of synchronization for\\nperiodic flows,” Journal of Fluid Mechanics 846, R2 (2018).\\n22F. C. Hoppensteadt and E. M. Izhikevich, Weakly connected neural net-\\nworks , V ol. 126 (Springer Science & Business Media, 1997).\\n23K. C. Wedgwood, K. K. Lin, R. Thul, and S. Coombes, “Phase-amplitude\\ndescriptions of neural oscillator models,” The Journal of Mathematical\\nNeuroscience 3, 1–22 (2013).\\n24D. Wilson and J. Moehlis, “Isostable reduction of periodic orbits,” Physical\\nReview E 94, 052213 (2016).\\n25A. Mauroy and I. Mezi ´c, “Global stability analysis using the eigenfunctions\\nof the Koopman operator,” IEEE Transactions on Automatic Control 61,\\n3356–3369 (2016).\\n26S. Shirasaka, W. Kurebayashi, and H. Nakao, “Phase-amplitude reduction\\nof transient dynamics far from attractors for limit-cycling systems,” Chaos:\\nAn Interdisciplinary Journal of Nonlinear Science 27(2017).\\n27A. Mauroy and I. Mezi ´c, “Global computation of phase-amplitude reduc-\\ntion for limit-cycle dynamics,” Chaos: An Interdisciplinary Journal of Non-\\nlinear Science 28(2018).\\n28K. Kotani, Y . Ogawa, S. Shirasaka, A. Akao, Y . Jimbo, and H. Nakao,\\n“Nonlinear phase-amplitude reduction of delay-induced oscillations,” Phys-\\nical Review Research 2, 033106 (2020).\\n29H. Nakao, “Phase and amplitude description of complex oscillatory patterns\\nin reaction-diffusion systems,” in Physics of Biological Oscillators: New\\nInsights into Non-Equilibrium and Non-Autonomous Systems (Springer,\\n2021) pp. 11–27.\\n30S. Takata, Y . Kato, and H. Nakao, “Fast optimal entrainment of limit-cycle\\noscillators by strong periodic inputs via phase-amplitude reduction and flo-\\nquet theory,” Chaos: An Interdisciplinary Journal of Nonlinear Science 31' metadata={'source': './data/2403.06992.pdf', 'page': 10}\n",
            "page_content='Phase autoencoder for limit-cycle oscillators 12\\n(2021).\\n31B. Ermentrout, “Type I membranes, phase resetting curves, and synchrony,”\\nNeural computation 8, 979–1001 (1996).\\n32K. Ota, M. Nomura, and T. Aoyagi, “Weighted spike-triggered average of\\na fluctuating stimulus yielding the phase response curve,” Physical review\\nletters 103, 024101 (2009).\\n33T. Netoff, M. A. Schwemmer, and T. J. Lewis, “Experimentally estimating\\nphase response curves of neurons: theoretical and practical issues,” Phase\\nResponse Curves in Neuroscience: Theory, Experiment, and Analysis , 95–\\n129 (2012).\\n34T. Imai, K. Ota, and T. Aoyagi, “Robust measurements of phase response\\ncurves realized via multicycle weighted spike-triggered averages,” Journal\\nof the Physical Society of Japan 86, 024009 (2017).\\n35P. J. Schmid, “Dynamic mode decomposition of numerical and experimen-\\ntal data,” Journal of fluid mechanics 656, 5–28 (2010).\\n36J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor, Dynamic mode\\ndecomposition: data-driven modeling of complex systems (SIAM, 2016).\\n37M. O. Williams, I. G. Kevrekidis, and C. W. Rowley, “A data–driven ap-\\nproximation of the Koopman operator: Extending dynamic mode decom-\\nposition,” Journal of Nonlinear Science 25, 1307–1346 (2015).\\n38B. Lusch, J. N. Kutz, and S. L. Brunton, “Deep learning for universal lin-\\near embeddings of nonlinear dynamics,” Nature communications 9, 4950\\n(2018).\\n39N. Namura, S. Takata, K. Yamaguchi, R. Kobayashi, and H. Nakao, “Esti-\\nmating asymptotic phase and amplitude functions of limit-cycle oscillators\\nfrom time series data,” Physical Review E 106, 14204 (2022).\\n40B. Kralemann, L. Cimponeriu, M. Rosenblum, A. Pikovsky, and\\nR. Mrowka, “Phase dynamics of coupled oscillators reconstructed from\\ndata,” Physical Review E 77, 066205 (2008).\\n41A. Mauroy, I. Mezi ´c, and J. Moehlis, “Isostables, isochrons, and Koopman\\nspectrum for the action–angle representation of stable fixed point dynam-\\nics,” Physica D: Nonlinear Phenomena 261, 19–30 (2013).\\n42K. Fukami, K. Taira, and H. Nakao, “Data-driven transient lift attenuation\\nfor extreme vortex gust-airfoil interactions,” in preparation.\\n43A. Mardt, L. Pasquali, H. Wu, and F. Noé, “Vampnets for deep learning of\\nmolecular kinetics,” Nature communications 9, 5 (2018).\\n44C. Wehmeyer and F. Noé, “Time-lagged autoencoders: Deep learning of\\nslow collective variables for molecular kinetics,” The Journal of chemical\\nphysics 148(2018).\\n45S. E. Otto and C. W. Rowley, “Linearly recurrent autoencoder networks\\nfor learning dynamics,” SIAM Journal on Applied Dynamical Systems 18,\\n558–593 (2019).\\n46N. Takeishi, Y . Kawahara, and T. Yairi, “Learning Koopman invariant sub-\\nspaces for dynamic mode decomposition,” Advances in neural information\\nprocessing systems 30(2017).\\n47E. Yeung, S. Kundu, and N. Hodas, “Learning deep neural network repre-\\nsentations for Koopman operators of nonlinear dynamical systems,” (2019)\\npp. 4832–4839.\\n48K. Champion, B. Lusch, J. N. Kutz, and S. L. Brunton, “Data-driven discov-\\nery of coordinates and governing equations,” Proceedings of the National\\nAcademy of Sciences 116, 22445–22451 (2019).\\n49O. Azencot, N. B. Erichson, V . Lin, and M. Mahoney, “Forecasting sequen-\\ntial data using consistent Koopman autoencoders,” (2020) pp. 475–485.\\n50Y . Li, H. He, J. Wu, D. Katabi, and A. Torralba, “Learning com-\\npositional Koopman operators for model-based control,” arXiv preprint\\narXiv:1910.08264 (2019).51N. Berman, I. Naiman, and O. Azencot, “Multifactor sequential dis-\\nentanglement via structured Koopman autoencoders,” arXiv preprint\\narXiv:2303.17264 (2023).\\n52M. Han, J. Euler-Rolle, and R. K. Katzschmann, “Desko: Stability-assured\\nrobust control with a deep stochastic Koopman operator,” (2021).\\n53G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\\ndata with neural networks,” science 313, 504–507 (2006).\\n54K. Fukami and K. Taira, “Grasping extreme aerodynamics on a low-\\ndimensional manifold,” arXiv preprint arXiv:2305.08024 (2023).' metadata={'source': './data/2403.06992.pdf', 'page': 11}\n",
            "page_content='data with neural networks,” science 313, 504–507 (2006).\\n54K. Fukami and K. Taira, “Grasping extreme aerodynamics on a low-\\ndimensional manifold,” arXiv preprint arXiv:2305.08024 (2023).\\n55A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in py-\\ntorch,” (2017).\\n56V . Nair and G. E. Hinton, “Rectified linear units improve restricted boltz-\\nmann machines,” in Proceedings of the 27th international conference on\\nmachine learning (ICML-10) (2010) pp. 807–814.\\n57S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift,” in International conference\\non machine learning (pmlr, 2015) pp. 448–456.\\n58A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,\\nZ. Lin, N. Gimelshein, L. Antiga, et al. , “Pytorch: An imperative style,\\nhigh-performance deep learning library,” Advances in neural information\\nprocessing systems 32(2019).\\n59J. T. Stuart, “On the non-linear mechanics of wave disturbances in stable\\nand unstable parallel flows part 1. the basic behaviour in plane poiseuille\\nflow,” Journal of Fluid Mechanics 9, 353–370 (1960).\\n60L. Landau, “Comptes rendues dokl,” in Acad. Sci. USSR , V ol. 44 (1944) p.\\n311.\\n61A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane\\ncurrent and its application to conduction and excitation in nerve,” The Jour-\\nnal of physiology 117, 500 (1952).\\n62H. Nakao, S. Yasui, M. Ota, K. Arai, and Y . Kawamura, “Phase reduction\\nand synchronization of a network of coupled dynamical elements exhibiting\\ncollective oscillations,” Chaos: An Interdisciplinary Journal of Nonlinear\\nScience 28(2018).\\n63P. Mircheski, J. Zhu, and H. Nakao, “Phase-amplitude reduction and opti-\\nmal phase locking of collectively oscillating networks,” Chaos: An Inter-\\ndisciplinary Journal of Nonlinear Science 33(2023).\\n64M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed neural\\nnetworks: A deep learning framework for solving forward and inverse prob-\\nlems involving nonlinear partial differential equations,” Journal of Compu-\\ntational physics 378, 686–707 (2019).\\n65G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and\\nL. Yang, “Physics-informed machine learning,” Nature Reviews Physics 3,\\n422–440 (2021).\\n66S. Greydanus, M. Dzamba, and J. Yosinski, “Hamiltonian neural networks,”\\nAdvances in neural information processing systems 32(2019).\\n67Z. Chen, M. Feng, J. Yan, and H. Zha, “Learning neural hamiltonian\\ndynamics: a methodological overview,” arXiv preprint arXiv:2203.00128\\n(2022).\\n68P. Toth, D. J. Rezende, A. Jaegle, S. Racanière, A. Botev, and I. Hig-\\ngins, “Hamiltonian generative networks,” arXiv preprint arXiv:1909.13789\\n(2019).\\n69A. Daigavane, A. Kosmala, M. Cranmer, T. Smidt, and S. Ho, “Learn-\\ning integrable dynamics with action-angle networks,” arXiv preprint\\narXiv:2211.15338 (2022).' metadata={'source': './data/2403.06992.pdf', 'page': 11}\n",
            "Processing  ./data/2403.06990.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='Accelerating Dimensionality Reduction in Wave-Resistance\\nProblems through Geometric Operators\\nStamatios Stamatatelopoulos1,2,∗,Shahroz Khan1,3Panagiotis Kaklis1,4\\n1Department of Naval Architecture, Ocean and Marine Engineering, University of Strathclyde, Glasgow (UK)\\n2Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, (USA)\\n3BAR Technologies Ltd, Portsmouth (UK)\\n4Foundation for Research & Technology Hellas (FORTH), Institute of Applied & Computational Mathematics (IACM),\\nDivision: Numerical Analysis & Computational Science, Group: Data Science, Heraklion, Crete (GR)\\nAbstract\\nReducing the dimensionality and uncertainty of design spaces is a key prerequisite for shape optimisation in compu-\\ntationally intensive fluid problems. However, running these analyses at an offline stage itself poses a computationally\\ndemanding task. In this work, we propose a unique framework for the inexpensive implementation of sensitivity\\nanalyses for reducing the dimensionality of the design space in wave-resistance problems. At the heart of our ap-\\nproach is the formulation of a geometric operator that leverages, via high-order geometric moments, the underlying\\nconnection between geometry and physics, specifically the wave-resistance coefficient ( Cw), of ships using the slen-\\nder body theory based on the well-known Vossers’ integral. The resulting geometric operator is computationally\\ninexpensive yet physics-informed and can act as a geometry-based surrogate to drive parametric sensitivities. To\\nanalytically demonstrate the capability of the proposed approach, we use a well-known benchmark geometry, namely,\\nthe modified Wigley hull. Its simple analytical formulation allows for closed expressions of the geometric operators\\nand exploration of computational domains that would otherwise be inaccessible. In this context, the proposed geo-\\nmetric operator outperforms existing similar approaches by achieving 100% similarity with Cwat a fraction of the\\ncomputational cost.\\nKeywords: Dimensionality Reduction; Sensitivity Analysis; Shape Optimisation; Wave Resistance; Geometric Oper-\\nators\\n1 Introduction\\nIdentifying satisfactory solutions to physics-based optimisation problems, such as wave resistance minimisation, is\\na task of fundamental importance in the community of Engineering Design. However, the relevant optimisation\\nalgorithms may require evaluating the physics-based objective a number of times, which can be computationally\\nprohibitive. In fact, modern design tools can sometimes prove to be too expensive to run even in cases where a\\nrelatively small number of responses need to be evaluated [1]. In tandem with the prohibitive cost of physics-based\\nsolvers is the well-known curse of dimensionality [2, 3, 4] which, in the context of sample-based design optimisation,\\nis realised by an explosion in the number of sample evaluations required by optimisation algorithms as the number of\\noptimisation parameters increases. This is precisely the point of intervention of the so-called method of Dimension-\\nality Reduction (DR), whose preliminary objective is to reduce the dimensionality of the design space while keeping\\nthe same design variability as the original design space.\\nIn the DR-pertinent literature, there are well-studied unsupervised techniques (e.g., Principal Component Analysis\\n(PCA)[5], auto-encoders [6], etc.) or supervised approaches (e.g., Active Subspace Method (ASM) [7], Sensitiv-\\nity Analysis (SA) [8]), which may or may not require evaluation of designs’ physics, respectively. Among these\\ntechniques, PCA, auto-encoders, and ASM extract the latent features of the original design space to create a lower-\\ndimensional subspace while capturing the maximum geometric variability. In contrast, SA is a selection process that\\nidentifies parameters that are less sensitive (or insensitive) to physics. As parameters with lower sensitivity have a' metadata={'source': './data/2403.06990.pdf', 'page': 0}\n",
            "page_content='identifies parameters that are less sensitive (or insensitive) to physics. As parameters with lower sensitivity have a\\nnegligible effect on performance, they can be excluded to reduce the dimensionality of the design space [9, 10, 11].\\nCompared to SA, unsupervised DR techniques can be computationally efficient since they do not rely on performance\\nlabels. However, their effectiveness may be limited when there is no direct correlation with shape modifications. In\\ncontrast, SA implementation is more informed because it not only reduces dimensionality but also provides valuable\\ninsights into the driving features of designs that contribute to the extreme variability in performance [8, 12]. Con-\\nsequently, SA enables users to allocate resources more effectively from the early stages of design, thus expediting\\nproduct development. However, implementing SA can be computationally demanding, especially when an analytical\\nsolution is not available, and costly numerical simulations become necessary. Although meta/surrogate models can\\naccelerate SA, constructing the surrogate itself can be a computationally intensive task due to the requirement of\\nevaluating performance labels for dataset creation [13].\\n∗Corresponding author. E-mail address: stamatis@mit.edu (S. Stamatatelopoulos)\\n1arXiv:2403.06990v1  [math.NA]  23 Feb 2024' metadata={'source': './data/2403.06990.pdf', 'page': 0}\n",
            "page_content='Therefore, the aim of this study is to reduce the computational cost of SA for DR purposes by leveraging the un-\\nderlying connection between physics and geometry. This is achieved by considering that evaluating geometry-based\\noperators is much cheaper compared to their physics-based counterparts. More precisely, for the wave-resistance\\noperator, two geometry-based operators are proposed and tested as surrogate models for SA. It is important to note\\nthat the only requirement for the surrogate geometric operators is to be sensitive, in a sense that will be defined\\nbelow, to the same parameters as the physics-based quantity they are related to. This requirement is much less\\nstrenuous when compared to surrogate modelling for optimisation, where the surrogate model must mimic the com-\\nplete behaviour of the high-fidelity physical model.\\nTo build a vigorous analytical foundation of the proposed approach we used a wigley hull as a test case, which is\\na standard benchmark geometry in the Naval Architecture domain. It is a simplified mathematical model of ship\\nhulls and is described by simple polynomial representations, making it analytically tractable. The simple analytical\\nexpression of the Wigley hull allows for closed forms of geometric operators, thereby reducing computational costs\\nand providing access to computational domains that would otherwise be inaccessible. Leveraging these closed-form\\nsolutions at the preliminary design stage, it offers the potential for explicit analysis of the relevant operators, facili-\\ntating a deeper understanding of their behaviour and implications.\\nThe remainder of this paper is organised as follows: Section 2 provides a brief overview of the concepts of SA\\nand Uncertainty Analysis (UA) which are fundamental to DR. In Section 3, a general framework is developed for\\nmatching physics-based operators to geometry-based ones, two such candidate operators are introduced and the\\nDR framework employed in this study is discussed in more detail. In Section 4, the parametric modeller and the\\nphysics-based property, that the methodology developed in the previous section will be applied to, are established.\\nIn Section 5 the relevant results are displayed and analysed. Finally in Section 6, conclusive remarks and potential\\nnext steps are provided.\\n2 Background\\nThe design of mathematical models which describe man-made systems reliably is of fundamental importance to the\\nscientific community. The principal role of SA and UA is to answer questions that arise in the design and validation\\nof such models. Such questions can be expressed as (see, e.g., [11, p. 3], [9, p. 183]):\\n•How well does the model under consideration represent the underlying physical phenomena?\\n•How far can the calculated results be extrapolated and how can this be achieved?\\n•Which factor or group of factors is most responsible for producing model outputs within or outside specified\\nbounds?\\nSpecifically, the role of UA given certain model input conditions (such as probability distributions for the input\\nvariables) is to quantify the resulting effect on the output of the model. On the other hand, the role of SA is to\\nidentify which input variables contributed most to said effect . Grounded on the above, a reasonable definition of SA\\nand UA can be stated as:\\nSensitivity and Uncertainty Analysis is a methodology for the formal evaluation of data and models [11,\\np. 3]. Uncertainty Analysis is the quantification of uncertainty in the model output given uncertainties\\nin the model input. Sensitivity Analysis is the apportioning of the uncertainty in the model output to\\ndifferent sources of uncertainty in the model input [9, p. 14].\\nEven though the goals of these two analyses differ, in practice, they are often performed simultaneously and the term\\nSensitivity Analysis has prevailed [10, p. 1]. For the sake of succinctness, this terminology has also been adopted in\\nthis paper to refer both to SA and UA.' metadata={'source': './data/2403.06990.pdf', 'page': 1}\n",
            "page_content='Sensitivity Analysis has prevailed [10, p. 1]. For the sake of succinctness, this terminology has also been adopted in\\nthis paper to refer both to SA and UA.\\nHaving provided a definition of SA that is agreed upon in the literature, we proceed by reviewing the various ap-\\nproaches to perform SA. Out of the many dichotomies of such methods2, this brief review will focus on the distinction\\nofLocal versus Global SA since the latter is central to this study. Local SA (LSA) investigates the model behaviour\\nin a small neighbourhood of predetermined points of high interest. On the other hand, Global SA (GSA) investigates\\nthe model behaviour over the entire parameter domain.\\nHistorically, LSA is the predecessor of GSA with the first systematic methodology being formulated in [15] as re-\\nported in [11, p. 4]. Perhaps the simplest of such methods is the One at A Time (OAT), where after a base point\\nis chosen, each of the parameters is varied by a specified amount, while all other others are held constant. Com-\\nparing the resulting change in model output between parameters can provide insight in their relative importance.\\n2Other than Local vsGlobal , other distinctions include Statistical vsDeterministic (see [11, p. 8] for a brief discussion) or Qualitative\\nvsQuantitative (see [14]). In this context, the SA methodology presented in Section 3.4 can be characterised as global, quantitative and\\ndeterministic.\\n2' metadata={'source': './data/2403.06990.pdf', 'page': 1}\n",
            "page_content='Symbolically, given a model f:Rn→Rone assigns a base point Rn∋X0= (x0\\n1, ..., x0\\nn) and a fixed varia-\\ntionϵi∈R,i= 1, ..., n for each parameter. Then, the respective sensitivity index for the ithparameter will be\\n|f(X0)−f(x0\\n1, ..., x0\\ni+ϵi, ..., x0\\nn)|. One of the shortcomings of OAT is that by fixing all parameters other than the\\nithone, it is impossible to investigate the interactions between two different parameters. For example take n= 2: it\\nmight be the case that |f(x0\\n1+ϵ1, x0\\n2+ϵ2)−f(x0\\n1, x0\\n2)|is much greater than either of the two sensitivity indexes but\\nit will never be evaluated. The more sophisticated Elementary Effects method (EE), also known as Morris method\\norwinding stairs [10], mitigates this issue by performing a number of steps where in each step a new OAT experi-\\nment is performed [14] and thereafter conclusions for each parameter can be drawn by considering all relevant indices.\\nThe OAT and EE methods belong in the category of screening methods, characterised by the subdivision of the\\ndomain of each parameter into a number of levels. For a more detailed review of LSA methods, which includes\\nderivative-based methods, the reader is referred to [16]. As stated earlier, in order to investigate the entire param-\\neter domain one employs GSA methods. Such methods include regression-based, variation-based and density-based\\napproaches among others (see [16]). Frameworks for determining which method to employ have been proposed in\\nthe literature [14], with the first step being the determination of how close to linear is the relationship of the model\\noutput to the model input. More specifically, for any constants βi∈R, the question is how well does the linear\\nrelationship f(X) =β1x1+...+βnxnrepresent the given model. To handle this issue, one can perform a number\\nof tests such as evaluating the so-called Pearson correlation coefficient for each parameter to determine whether its\\nrelationship with the output is linear. For any parameters that pass the chosen linearity test satisfactorily, there are\\nreadily available measures to evaluate the parameter’s sensitivity such as the Standard Regression Coefficient.\\nIf the user chooses not to make any assumptions about the model or the linearity hypothesis fails by the procedure\\nabove, [14] suggest to employ Sobol’s variance-based method. Introduced by Ilya M. Sobol [17], the method is based\\non the computation of the so called Sobol’s Index for each of the input parameters. This method applies to scalar\\nmodels ( f(X)∈R), however, there do exist generalisations to more than one outputs (see [18]). Since this is the SA\\napproach that has been adopted for this study, more details are provided in Section 3.4.\\nWe proceed to the concept of DR which relates to SA via the last of the questions in the start of this section; Which\\nfactor or group of factors is most influential to the model output? Having calculated Sobol’s indices SIi∈Rfor all\\nparameter values, a straightforward approach to answering this question is via specifying a threshold value ϵ[19]\\nand filtering the sensitive parameters accordingly. This approach is capable of identifying subsets of parameters but\\nfails to determine linear combinations of the parameters, which is a feature present in other approaches to DR such\\nas PCA [20] or ASM [7].\\nFundamental to the current study is the work of [12]. The authors perform SA on the wave resistance coefficient\\nCwof a series of ship hulls using the composition of a parametric modeler and a Cw-solver as the model f(·)\\nunder investigation. SA is performed globally via the variance-based method of Sobol’s indices as discussed above.\\nMotivated by the well studied correlation between the so-called Sectional Area Curve (SAC)3of a ship’s hull and ship\\nwave resistance (see [21]) the authors construct a geometry-based operator on ship hulls dubbed Shape Signature\\nVector (SSV) and perform an alternative SA by composing the parametric modeler with the SSV. The two SA' metadata={'source': './data/2403.06990.pdf', 'page': 2}\n",
            "page_content='Vector (SSV) and perform an alternative SA by composing the parametric modeler with the SSV. The two SA\\napproaches are studied for correlation with the expectation that the much cheaper to compute SSV would produce\\nresults related to the physics-based solver and thus could be used to expedite the preliminary design stage of hull\\ndesign. The outcome of this study does show a positive correlation between the two SA’s and this is the starting\\npoint for the work relevant to this study.\\n3 Sensitivity Analysis using Geometric Operators\\nLet be given a parametric modeller Dof 3D objects embedded in an n-dimensional design space X= [a1, b1]×\\n...×[an, bn] where each design parameter ti, i= 1, ..., n varies continuously in [ ai, bi]. Further, if P(D)∈Ris a\\nphysics-based property of objects generated by D, perform SA on DwithPbeing the Quantity of Interest (QoI),\\nto identify a subset of k≤nparameters I={i1, ..., i k}(i.e.{ti1, ..., t ik}) to which Pis most sensitive to. We\\naim at identifying a geometry-based operator Gsuch that performing SA with Gas the QoI, results in a subset of\\nk′< n sensitive parameters J={j1, ...jk′}such that the now reduced-dimensionality design space comprising of\\nthe parameters in Jcan be used for optimisation with Pwith acceptable loss of information. This is illustrated in\\nFigure 1,\\n3SAC represents the longitudinal variation of the cross-sectional area of a ship’s hull below the waterline. The integrated area under\\nthe SAC gives the displaced volume and provides an effective and simple description of global geometric properties. At the same time,\\nit is closely related to the resistance and propulsion performance of a ship.\\n3' metadata={'source': './data/2403.06990.pdf', 'page': 2}\n",
            "page_content='Dimensionality ReductionOptimisation\\nUsing a computationally inexpensive\\ngeometry-based operator Gcorrelated\\nwith the physics-based quantity P,\\nreduce the dimensionality of the design\\nspace resulting from modeller DNow with reduced\\ndimensional design space,\\noptimise the design for\\nminimum/maximum P\\nFigure 1: Illustration of the application of the geometric operator G. It’s important to note that Gis not employed in\\nthe optimisation step; rather, its use is confined to identifying the most sensitive parameters for dimension reduction.\\nThis enables an expedited physics-based optimisation process.\\nIt is important to emphasise that this process produces triplets ( D,P,G) without any guarantee that the correlation\\nbetween PandGis invariant to D. It is essential therefore to investigate the dependence of the pair ( P,G) on the\\nparametric modeller Dfor, if this dependence is weak, Gcan readily be applied to a range of similar parametric\\nmodellers. In this effort, the process above can be repeated for constant PandGacross a set of various parametric\\nmodellers {D1,D2, ...}. By verifying the compatibility of an increasing amount of triplets ( Di,P,G), the dependence\\nof (P,G) on the parametric modeller is deceased. For example, in the present case, the aim is to identify a\\ngeometric operator Gwhich is compatible with Pequal to the wave-making resistance coefficient Cwfor a set of\\nparametric modelers {D1,D2, ...}={Di:Diproduces slender hulls }. The above methodology for identifying triplets\\n({D1,D2, ...},P,G) is outlined in Figure 2.\\nFigure 2: Diagrammatic representation of the methodology for identifying a suitable geometric operator Gcompatible\\nwith a physics-based quantity Pacross a set of parametric modellers D1,D2, . . ..\\nIn this paper, the procedure outlined in Fig. 2 has been applied twice, for two different geometric operators, on a\\nsingle parametric modeller D1. Both of these geometric operators are based on the concept of geometric moments,\\nintroduced in the following three sections. Then, the chosen approach for DR as well as the approach for performing\\ncorrelation analysis between JDiandIDiare outlined.\\n3.1 Geometric Moments\\nThere is a plethora of geometric operators Gwhich have been studied in the literature, such as the so-called Fourier\\nDescriptors (FD) [22], which extract information from the geometry of the contour, Convexity Measures (CM) ([23];\\n[24]) or even Elongation Measures [25], which at their simplest form are defined as the width over the length of\\nthe bounding box of a 2D shape. In order to apply the procedure in Fig. 2, two desirable attributes on Gare the\\nfollowing:\\nGis able to extract enough information from the underlying geometry; (1a)\\nGis easy and fast to evaluate for arbitrary geometries. (1b)\\nIn view of the above two points, the two operators Gwhich have been investigated in this study are based on\\ngeometric-moments M(p, q, r ). Starting with the definition of M(p, q, r ), the sthorder moment where s=p+q+r\\nover the solid Dis defined as\\nM(p, q, r ) =ZZZ\\nDxpyqzrdxdydz. (2)\\nFrom the study of the renowned Problem of Moments [26] it is well known that the geometric moments (2) contain\\nsignificant information regarding the underlying geometry, which can be used to approximate it and, under certain\\nconditions, even uniquely reconstruct it [27, 28, 29, 30]. For example, the volume of Dis given by VD=M(0,0,0)\\nand its centroid C= (Cx, Cy, Cz) by,\\nCx=M(1,0,0)\\nM(0,0,0), C y=M(0,1,0)\\nM(0,0,0), C z=M(0,0,1)\\nM(0,0,0). (3a)\\n4' metadata={'source': './data/2403.06990.pdf', 'page': 3}\n",
            "page_content='Further, there are numerous procedures for evaluating moments of arbitrary shapes efficiently, one of which is pre-\\nsented in [31] and has been implemented in the scope of this study. It is then evident that geometric-moments easily\\nsatisfy (1a) and (1b)\\nThe utility of moments as a mathematical construction has been recognised in various disciplines, with applications\\nstarting from probability and statistics [32] and including image analysis [33], signal processing [34], feature extraction\\n[35], computing tomography [36, 37] and inverse potential theory [38, 39] among others. Most importantly, however,\\nthey have shown promising results in a context similar to this study [12, 40] via construction of so called Shape\\nSignature Vector (SSV).\\n3.2 Shape Signature Vector\\nAs mentioned above, significant geometric information carried by Dcan be captured with a large enough number\\nof its moments. It is then straightforward to consider as a potential moment-based geometric operator, the vector\\nconsisting of all moments up to a certain order. This geometric-based operator is called Shape Signature Vector\\n(SSV) and was first introduced in [12] where it was investigated in an analogous context to the present study. Before\\nintroducing the relevant notation, notice that the moments in (2) depend on the position of Din its ambient space\\nand its scale, two non-desirable characteristics. A translation-invariant form of M(p, q, r ) can readily be defined if\\nDis positioned at its centroid,\\nMT(p, q, r ) =ZZZ\\nD(x−Cx)p(y−Cy)q(z−Cz)rdxdydz. (4)\\nHowever, (4) still varies when Dis scaled uniformly by λ, for if we apply the transformation g(x, y, z ) = (λx, λy, λz )\\nto (2) over the scaled geometry λD,\\nZZZ\\nλDxpyqzrdxdydz =ZZZ\\nD(λx)p(λy)q(λz)r|detDg|dxdydz\\n=ZZZ\\nD(λx)p(λy)q(λz)rλ3dxdydz\\n=λp+q+r+3M(p, q, r ). (5)\\nThen, looking at (3) and (5) we can write,\\nZZZ\\nλD(x−Cx)p(y−Cy)q(z−Cz)rdxdydz =ZZZ\\nD(λx−λCx)p(λy−λCy)q(λz−λCz)rλ3dxdydz\\n=λp+q+r+3MT(p, q, r ). (6)\\nSince for λD,M(0,0,0) = MT(0,0,0) = VλD=λ3VD, a scaling invariant moment MS(p, q, r ) and the translation &\\nscaling invariant moment MI(p, q, r ) ofDare given by,\\nMS(p, q, r ) =M(p, q, r )\\nM(0,0,0)(p+q+r+1)/3, (7)\\nMI(p, q, r ) =MT(p, q, r )\\nMT(0,0,0)(p+q+r+1)/3. (8)\\nNow, given a solid D, consider the set Ms\\nIof all scaling and translation invariant moments of D,MI(p, q, r ) of order\\ns.\\nMs\\nI={MI(p, q, r ) :p+q+r=s} (9)\\nFor example,\\nM0\\nI={MI(0,0,0)}\\nM1\\nI={MI(1,0,0), MI(0,1,0), MI(0,0,1)}\\nM2\\nI={MI(2,0,0), MI(1,1,0), MI(1,0,1), MI(0,2,0), MI(0,1,1), MI(0,0,2)}.\\nNotice that all translation invariant moments of order 1 are equal to zero by definition. In light of this, define the\\nShape Signature Vector (SSV) of order N, as the vector consisting of all elements of Ms\\nIfors= 0,2, ..., N ,\\nSSV N= [M0\\nI, M2\\nI, ..., MN\\nI]. (10)\\nWe conclude this section by calculating the cardinality of SSV N. For each set Ms\\nI, Card( Ms\\nI) = ( s+ 1)( s+ 2)/2\\nwhich can be proven inductively. Then since all Ms\\nIare disjoint,\\nCard( SSV N) =NX\\ns=0, s̸=1Card( Ms\\nI) =NX\\ns=0, s̸=1(s+ 1)( s+ 2)\\n2=1\\n6N3+N2+11\\n6N−2, N≥1. (11)\\n5' metadata={'source': './data/2403.06990.pdf', 'page': 4}\n",
            "page_content='3.3 Slender Body Operator\\nAs will be explained in Section 4, the chosen physics-based quantity in this study is the wave resistance coefficient\\nCw. The well known correlation between Cwand the SAC of a hull is the starting point for the derivation of the\\nslender body operator. More specifically, in the context of slender body theory, Vossers proposed the following\\nexpression for estimating the wave resistance of a slender ship moving at constant velocity U, at water density ρ,\\nwith K=g/U2, forggravitational acceleration [41],\\nR\\n−0.5ρU2=ZZ\\nR2S′′(x)S′′(ξ)Y0(K|x−ξ|)dS′(x)dS′(ξ) (12)\\nwhere Y0is the Bessel function of the second kind, S(x) is the SAC of the ship’s hull with S(x) = 0 for x /∈[−L/2, L/2]\\nand integration is of Riemann–Stieltjes type. Next, under the assumption that S′(±L/2) = 0, (12) transforms to\\nthe following Riemann integral,\\nR\\n−0.5ρU2=ZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ)Y0(K|x−ξ|)dxdξ, (13)\\nRecall the definition of Y0(x) to be\\nY0(x) =2\\nπ(ln(0.5x) +γ)J0(x) +2\\nπ∞X\\nk=1(−1)k−1(0.25x2)k\\n(k!)2kX\\nj=11\\nj, (14)\\nJ0(x) =∞X\\nk=0(−0.25x2)k\\n(k!)2, (15)\\nwith J0the Bessel function of the first kind. We can then write,\\nY0(K|x−ξ|) =2\\nπ(ln(0.5K|x−ξ|) +γ)J0(K|x−ξ|) +2\\nπ∞X\\nk=1(−1)k−1(0.25(K|x−ξ|)2)k\\n(k!)2kX\\nj=11\\nj\\n=2\\nπln(|x−ξ|)J0(K|x−ξ|) +2\\nπ(ln(0.5K) +γ)J0(K|x−ξ|)\\n+2\\nπ∞X\\nk=1(−1)k−1K2k\\n22k(k!)2 kX\\nj=11\\nj!\\n(x−ξ)2k\\n=2\\nπln(|x−ξ|)J0(K|x−ξ|) +2\\nπ(ln(0.5K) +γ)∞X\\nk=0(−1)kK2k\\n22k(k!)2(x−ξ)2k\\n+2\\nπ∞X\\nk=1(−1)k−1K2k\\n22k(k!)2 kX\\nj=11\\nj!\\n(x−ξ)2k\\n=2\\nπln(|x−ξ|)J0(K|x−ξ|) +∞X\\nk=0(−1)k K2k\\n22k−1(k!)2π\\x10\\nln(0.5K) +γ−h(k)\\x11\\n(16)\\nwhere\\nh(k) =\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f30, k = 0\\nkX\\nj=11\\nj, k > 0.(17)\\nSetting\\nf(k;K) = (−1)k K2k\\n22k−1(k!)2π\\x10\\nln(0.5K) +γ−h(k)\\x11\\n, (18)\\nwe rewrite (14) in its final form\\nY0(K|x−ξ|) =2\\nπln(|x−ξ|)J0(K|x−ξ|) +∞X\\nk=0f(K, k). (19)\\nSubstituting (19) into (13),\\nZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ)Y0(K|x−ξ|)dxdξ =2\\nπZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ) ln(|x−ξ|)J0(K|x−ξ|)dxdξ\\n+∞X\\nk=0f(k;K)Ik (20)\\n6' metadata={'source': './data/2403.06990.pdf', 'page': 5}\n",
            "page_content='with\\nIk=ZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ)(x−ξ)2kdxdξ. (21)\\nWe will now show Ikcan be expressed as a quadratic polynomial in moments of the type M(n,0,0), in view of which\\nwe introduce the following notation,\\nMn≡M(n,0,0). (22)\\nTo proceed, expand ( x−ξ)2k,\\nIk=ZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ)2kX\\ni=0\\x122k\\ni\\x13\\n(−1)ix2k−iξidxdξ\\n=2kX\\ni=0\\x122k\\ni\\x13\\n(−1)i ZL/2\\n−L/2S′′(x)x2k−idx! ZL/2\\n−L/2S′′(ξ)ξidξ!\\n. (23)\\nIt is evident that we must investigate the integral\\nIIN=ZL/2\\n−L/2S′′(x)xNdx. (24)\\nIntegrating twice by parts,\\nIIN=h\\nS′(x)xNiL/2\\n−L/2−NZL/2\\n−L/2S′(x)xN−1dx\\n=h\\nS′(x)xNiL/2\\n−L/2−Nh\\nS(x)xN−1iL/2\\n−L/2+N(N−1)ZL/2\\n−L/2S(x)xN−2dx\\n=h\\nS′(x)xNiL/2\\n−L/2−Nh\\nS(x)xN−1iL/2\\n−L/2+N(N−1)MN−2, N≥2. (25)\\nSetting M−1≡M−2= 0 we extend (25) to all N≥0 without any effect on the ensuing calculations, which allows\\nfor less cumbersome notation in what follows. Now, looking at (23) it is evident that we must expand II2k−iIIi,\\nII2k−iIIi= h\\nS′(x)x2k−iiL/2\\n−L/2−(2k−i)h\\nS(x)x2k−i−1iL/2\\n−L/2+ (2k−i)(2k−i−1)M2k−i−2!\\n· h\\nS′(x)xiiL/2\\n−L/2−ih\\nS(x)xi−1iL/2\\n−L/2+i(i−1)Mi−2!\\n= h\\nS′(x)x2k−iiL/2\\n−L/2−(2k−i)h\\nS(x)x2k−i−1iL/2\\n−L/2! h\\nS′(x)xiiL/2\\n−L/2−ih\\nS(x)xi−1iL/2\\n−L/2!\\n+(2k−i)(2k−i−1) h\\nS′(x)xiiL/2\\n−L/2−ih\\nS(x)xi−1iL/2\\n−L/2!\\nM2k−i−2\\n+i(i−1) h\\nS′(x)x2k−iiL/2\\n−L/2−(2k−i)h\\nS(x)x2k−i−1iL/2\\n−L/2!\\nMi−2\\n+i(i−1)(2k−i)(2k−i−1)Mi−2M2k−i−2. (26)\\nBefore substituting (26) into (23), notice that a significant number of terms is repeated across the summation. For\\nexample, the last term in (26) at iwill be identical to that at 2 k−i. Same terms can easily be grouped together by\\nnoticing that for any g(N)\\n2kX\\ni=0\\x122k\\ni\\x13\\n(−1)ig(i)g(2k−i) = kX\\ni=k+2k−1X\\ni=0!\\x122k\\ni\\x13\\n(−1)ig(i)g(2k−i). (27)\\n7' metadata={'source': './data/2403.06990.pdf', 'page': 6}\n",
            "page_content='Then looking at (26) and (27), set\\nck= kX\\ni=k+2k−1X\\ni=0!\\x122k\\ni\\x13\\n(−1)i h\\nS′(x)x2k−iiL/2\\n−L/2−(2k−i)h\\nS(x)x2k−i−1iL/2\\n−L/2!\\n· h\\nS′(x)xiiL/2\\n−L/2−ih\\nS(x)xi−1iL/2\\n−L/2!\\n, (28a)\\nck\\ni=\\x122k\\ni+ 2\\x13\\n(−1)i(i+ 1)( i+ 2) h\\nS′(x)x2k−i−2iL/2\\n−L/2−(2k−i−2)h\\nS(x)x2k−i−3iL/2\\n−L/2!\\n, (28b)\\nck\\ni,j=\\x12i+j+ 4\\ni\\x13\\n(−1)i(i+ 1)( i+ 2)( j+ 1)( j+ 2). (28c)\\nTransforming the operator of Ikin (23) by (27) and substituting equations (26), (28),\\nIk=ck+ kX\\ni=k+2k−1X\\ni=0! \\nck\\n2k−i−2M2k−i−2\\n+ck\\ni−2Mi−2\\n+ck\\ni−2,2k−i−2Mi−2M2k−i−2!\\n=ck+ kX\\ni=k+2k−1X\\ni=0! \\nck\\n2k−i−2M2k−i−2!\\n+ kX\\ni=k+2k−1X\\ni=0! \\nck\\ni−2Mi−2!\\n+ kX\\ni=k+2k−1X\\ni=0! \\nck\\ni−2,2k−i−2Mi−2M2k−i−2!\\n=ck+ck\\nk−2Mk−2+ 2k−1X\\ni=0 \\nck\\n2k−i−2M2k−i−2!\\n+ck\\nk−2Mk−2+ 2k−1X\\ni=0 \\nck\\ni−2Mi−2!\\n+ck\\nk−2,k−2M2\\nk−2+ 2k−1X\\ni=0 \\nck\\ni−2,2k−i−2Mi−2M2k−i−2!\\n. (29)\\nTo proceed, we reverse the order in first sum that appears in (29) and change the indices in the second and third\\nsum by i→i+ 2 which transforms the summation limits to {−2, k−3}.\\nIk=ck+ck\\nk−2Mk−2+ 22k−2X\\ni=k−1 \\nck\\niMi!\\n+ck\\nk−2Mk−2+ 2k−3X\\ni=−2 \\nck\\niMi!\\n+ck\\nk−2,k−2M2\\nk−2+ 2k−3X\\ni=−2 \\nck\\ni,2k−i−4MiM2k−i−4!\\n. (30)\\n8' metadata={'source': './data/2403.06990.pdf', 'page': 7}\n",
            "page_content='Notice that in the last two sums, the terms for i={−2,−1}originate from II0, II 1respectively and can be set to\\nzero. Then,\\nIk=ck+ck\\nk−2Mk−2+ 22k−2X\\ni=k−1 \\nck\\niMi!\\n+ck\\nk−2Mk−2+ 2k−3X\\ni=0 \\nck\\niMi!\\n+ck\\nk−2,k−2M2\\nk−2+ 2k−3X\\ni=0 \\nck\\ni,2k−i−4MiM2k−i−4!\\n=ck\\n+22k−2X\\ni=0 \\nck\\niMi!\\n+ck\\nk−2,k−2M2\\nk−2+ 2k−3X\\ni=0 \\nck\\ni,2k−i−4MiM2k−i−4!\\n. (31)\\nFinally, looking at (20) and (31) we define the slender body operator Gof order nas,\\nG(n) =nX\\nk=0f(k;K)Ik, (32)\\nwhich, under the assumption that the second derivative S′′(x) of the sectional area curve is bounded, will be a finite\\npart of Vossers’ integral. A proof for this statement is included in Appendix A\\n3.4 Sobol’s Sensitivity Analysis\\nAs stated earlier, in this section Sobol’s methodology for SA is presented, it’s sampling-based counterpart which\\ncan be used to approximate the relevant sensitivity indices is provided and the approach through which parameters\\nare categorised as sensitive or insensitive is introduced. Let Xi,i={1, ..., n}be the random input variables to\\nthe function f:Rn→Rk, which produces the random vector Y=f(X1, ..., X n)∈Rk. Let ube an non-empty\\nr-subset of {1, ..., n}and let Xu= (Xi, i∈u)∈Rr,X˜ u= (Xi, i∈ {1, ..., n}/u)∈Rn−r. Then, recall the Hoeffding\\ndecomposition [42] of f:\\nf(X0, ..., X n−1) =c+fu(Xu) +f˜ u(X˜ u) +fu,˜ u(Xu,X˜ u), (33)\\nwhere c∈Rk,fu:Rr→Rk,f˜ u:Rn−r→Rkandfu,˜ u:Rn→Rkare given by:\\nc=E(Y), f u=E(Y|Xu)−c, f ˜ u=E(Y|X˜ u)−c, f u,˜ u=Y−fu−fu,˜ u−c. (34)\\nTaking the covariance of (33), and due to L2-orthogonality:\\nCov(f) =Cov(fu(Xu)) +Cov(f˜ u(X˜ u)) +Cov(fu,˜ u(Xu,X˜ u)) (35)\\nThen to project these co-variances to a scalar, two approaches are to take the derivative or take the trace. The\\nauthors in [18], take the trace of these covariances and define the following three indices:\\nSu(f) =Trace (Cov(fu(Xu)))\\nTrace (Cov(f)),sensitivity only to the inputs in u (36a)\\nS˜ u(f) =Trace (Cov(f˜ u(X˜ u)))\\nTrace (Cov(f)),sensitivity only to the inputs not in u (36b)\\nSu,˜ u(f) =Trace (Cov(fu,˜ u(Xu,X˜ u)))\\nTrace (Cov(f)),interaction between inputs of u\\nand inputs of {0, ..., n−1}/u. (36c)\\nNow, for u={i}, (36a) will be called the sensitivity index of the parameter ciwith respect to the quantity of interest\\nf, and denoted as,\\nSIf\\ni≡Su(f). (37)\\n9' metadata={'source': './data/2403.06990.pdf', 'page': 8}\n",
            "page_content='In accordance to [18], (37) can be approximated via a sampling-based algorithm. Specifically, first draw Ninde-\\npendent samples xi∈Rn, i= 1, ..., N from the entire design space which are sampled according to the respective\\nprobability density function ρof the input parameters. Proceed by evaluating the respective responses fi=f(xi).\\nThen, for each xi, construct another sample x∗\\niby fixing ciand only varying the rest of the parameters to evaluate\\nf∗\\ni=f(x∗\\ni). For fi= (fi,1, ..., f i,k)∈Rk, the sensitivity index SIf\\niofciis approximated by,\\nSIf\\ni=kX\\nj=1 NX\\ni=1fi,jf∗\\ni,j−1\\nN NX\\ni=1fi,j+f∗\\ni,j\\n2!2!\\nkX\\nj=1 NX\\ni=1(fi,j)2+ (f∗\\ni,j)2\\n2−1\\nN NX\\ni=1fi,j+f∗\\ni,j\\n2!2!. (38)\\nSince in our case c1, c2, c3are design variables, ρis chosen as the uniform PDF [14] so as not to introduce any\\nbias towards any region of the design space. As stated earlier, each parameter ciis characterised as sensitive or\\ninsensitive with respect to fby specifying a threshold value ϵ[19],\\n(\\nSIf\\ni> ϵ=⇒cisensitive parameter with respect to f,\\nSIf\\ni≤ϵ=⇒ciinsensitive parameter with respect to f.(39)\\nIn alignment with [12], ϵhas been chosen as 0 .05 for this study.\\n3.5 Physics-Geometry Correlation Measures\\nHaving evaluated the physics-based sensitivity indices SIP={SIP\\n1, ..., SIP\\nn}and their geometry-based counterparts\\nSIG={SIG\\n1, ..., SIG\\nn}two metrics will be used to quantify their correlation,\\nNRMSE =vuuutnX\\ni=1\\x10\\nSIP\\ni−SIG\\ni\\x112\\nn\\nmax( SIP)−min(SIP), (40a)\\nsimilarity =nX\\ni=1SIP\\ni·SIG\\ni\\nvuutnX\\ni=1SIP\\nivuutnX\\ni=1SIG\\ni∈[0,1], (40b)\\nwhere\\nSIP\\ni=(\\n1, SIP\\ni> ϵ\\n0,else,SIG\\ni=(\\n1, SIG\\ni> ϵ\\n0,else.(40c)\\nEquation (40a) is the Root Mean Square Error between SIPandSIGnormalised by the former, while (40b) can\\nbe thought of as a discretised correlation between only the sensitive parameters of SIPandSIG. More accurately,\\nnotice that if a parameter is insensitive in both SIPandSIGit then has no effect on (40b). However if a parameter\\nis sensitive for one of SIP,SIGbut not the other, (40b) is penalised. This penalization can be interpreted as follows:\\nif a parameter is sensitive for SIPbut not SIGthen the geometry-based DR has omitted important information\\nregarding the relevant physics-based problem; if the opposite is true, then the geometry-based DR has not sufficiently\\nreduced the dimensionality of the physics-based problem. These two metrics were used in [12] in an analogous context\\nto this study.\\n4 Test Case: Wave Resistance and the Wigley-based Modeller\\nLooking at Figure 2, Section 3 was devoted to outlining two different choices of Gas well as the approach to\\ndimensionality reduction and correlation analysis between the physics- and geometry-based sensitivity indices. This\\nsection introduces the chosen parametric modeller D1and physics-based property P.\\n4.1 Modified Wigley Hull Parametric Modeller\\nThe modeller Dinvestigated in this study is a modified Wigley hull-form, a benchmark geometry which has been\\nwidely used is the literature [43]. The half-length and half-breadth subsurface of D, is parameterised in 6 parameters\\n{L, B, T, c 1, c2, c3}and given by,\\n10' metadata={'source': './data/2403.06990.pdf', 'page': 9}\n",
            "page_content='D(ξ, ζ;L, B, T, c 1, c2, c3) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0Lξ\\n2\\nBη\\n2\\nTζ\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, ξ∈[0,1], ζ∈[0,1], (41a)\\nη= (1−ζ2)(1−ξ2)(1 + c1ξ2+c2ξ4) +c3ζ2(1−ζ8)(1−ξ2)4, (41b)\\nwhere L, B, T are the length breadth and draft respectively. While the effect of the first three parameters is clear,\\nthe same is not true for c1, c2andc3. Figures 3, 5 and 7 indicate this effect by varying each one between 0 and 1,\\nwhile keeping the others fixed. Looking at the first two figures, note that c1andc2predominantly affect the deck\\narea with negligible effect on the keel. By comparing Figures 3 and 5, which are identically scaled, it is easy to see\\nthat c1has a stronger effect on the deck when compared to c2. Varying c3mainly affects the keel of the hull, with\\nno effect to the deck shape as is evident from the ζ2factor in (41). In Figure 7 one can see the transverse sections of\\nthe hull at midships ( ξ= 0) for varying c3. Finally, the original Wigley hull is recovered by setting c1=c2=c3= 0.\\nIt should be noted that if S′(±L/2)̸= 0, then (12) diverges [44], as is the case for the Wigley-hull family (41)\\nwhere S′(±L/2) = (8 BT/3L)(1 + c1+c2)̸= 0 (see Appendix C). In this connection, we have adopted (13) as a finite\\npart of (12).\\nFigure 3: Effect of c1parameter on the modified Wigley hull. In this top-view figure, each curve is the deck-line\\ncorresponding to c1={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B = 0.0996, T=\\n0.13775 , c2= 0, c3= 0}.\\nFigure 4: Renders of the modeller instances depicted in Figure 3. Specifically, from left to right, each hull corresponds\\ntoc1={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B= 0.0996, T= 0.13775 , c2= 0, c3=\\n0}.\\n11' metadata={'source': './data/2403.06990.pdf', 'page': 10}\n",
            "page_content='Figure 5: Effect of c2parameter on the modified Wigley hull. In this top-view figure, each curve is the deck-line\\ncorresponding to c2={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B = 0.0996, T=\\n0.13775 , c1= 0, c3= 0}.\\nFigure 6: Renders of the modeller instances depicted in Figure 5. Specifically, from left to right, each hull corresponds\\ntoc2={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B= 0.0996, T= 0.13775 , c1= 0, c3=\\n0}.\\n4.1.1 Design Space Selection\\nThe maximum number of parameters supported by (41) is 6, {L, B, T, c 1, c2, c3}. Similar to [12], a design space\\nwith uniform bounding box was chosen. We will now prove that this is indeed the case when L, B andTare fixed\\nandc1×c2×c3∈[0,1]3. For fixed length, beam and draft, let Sbe the three-parameter family of surfaces defined\\nby\\nS(ξ, ζ;c1, c2, c3)≡ D(ξ, ζ;L, B, T, c 1, c2, c3). (42a)\\nIfSiis the ithcoordinate function of S, define the bounding box of Sfor a given choice of c1, c2, c3as\\nB(c1, c2, c3) ={p= (p1, p2, p3)T∈R3: min\\nDSSi≤pi≤max\\nDSSi, i= 1,2,3}, (42b)\\nwhere DS= [0,1]2is the domain of S. We will show that B(·,·,·) is constant. Looking at (41), it is evident that for\\ni= 1,3 the minimum/maximum values of Siare taken at ξ=ζ= 0 / ξ=ζ= 1 respectively and are independent\\ntoc1, c2, c3. For i= 2 it is enough to show that min\\nDSηand max\\nDSηare independent to these three parameters as\\nwell. One can easily verify that in DS,η≥0 and η(ξ,1;c1, c2, c3) = 0. Therefore, min\\nDSη= 0 which is independent\\ntoc1, c2andc3. For the maximum of ηnotice that for 0 ≤ci≤1,\\n0≤(1−ζ2)(1−ξ2)(1 + c1ξ2+c2ξ4)≤(1−ζ2)(1−ξ2)(1 + ξ2+ξ4)\\n0≤c3ζ2(1−ζ8)(1−ξ2)4≤ζ2(1−ζ8)(1−ξ2)4.\\n12' metadata={'source': './data/2403.06990.pdf', 'page': 11}\n",
            "page_content='Figure 7: Effect of c3parameter on the modified Wigley hull. In this front-view figure, each curve is the depth-line\\ncorresponding to c3={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B = 0.0996, T=\\n0.13775 , c1= 0, c2= 0}.\\nAdding the two inequalities yields\\nη(ξ, ζ;c1, c2, c3)≤η(ξ, ζ; 1,1,1), ξ×ζ×c1×c2×c3= [0,1]5, (42c)\\nwhere η(ξ, ζ; 1,1,1) is clearly independent to c1, c2andc3. We complete this proof by showing that max\\nDSη(ξ, ζ;c1, c2, c3) =\\nmax\\nDSη(ξ, ζ; 1,1,1) for any c1, c2andc3. Denote g(ξ, ζ) =η(ξ, ζ; 1,1,1) with Dg= [0,1]2. Since gis differentiable,\\nifg(ξ∗, ζ∗) is a maximum, then either ( ξ∗, ζ∗) is a singular point and/or it is on the boundary of Dg. We begin by\\ninvestigating the first case,\\n∂g\\n∂ξ=−2ξ(1−ζ2)(1 + ξ2+ξ4) + (1 −ζ2)(1−ξ2)(2ξ+ 4ξ3) +ζ2(1−ζ8)4(1−ξ2)3(−2ξ)\\n= 2ξ(1−ζ2)\\x10\\n−(1 +ξ2+ξ4) + (1 −ξ2)(1 + 2 ξ2)−ζ2(1 +ζ2)(1 + ζ4)4(1−ξ2)3\\x11\\n= 2ξ(1−ζ2)\\x10\\n−1−ξ2−ξ4+ 1 + 2 ξ2−ξ2−2ξ4−ζ2(1 +ζ2)(1 + ζ4)4(1−ξ2)3\\x11\\n= 2ξ(1−ζ2)\\x10\\n−3ξ4−ζ2(1 +ζ2)(1 + ζ4)4(1−ξ2)3\\x11\\n. (42d)\\nWhen not in the boundary of Dgsuch that ξ, ζ∈(0,1) it is evident from (42d) that ∂g/∂ξ < 0 and therefore, there\\nare no singular points in the interior of Dg. For the second case where ( ξ, ζ)∈Bd(Dg),\\ng(Bd(Dg)) =\\uf8f1\\n\\uf8f2\\n\\uf8f30 , (ξ, ζ) = (1 , ζ) or ( ξ,1)\\n1−ζ8, (ξ, ζ) = (0 , ζ)\\n1−ξ6, (ξ, ζ) = (ξ,0)\\uf8fc\\n\\uf8fd\\n\\uf8fe. (42e)\\nThen max\\nBd(Dg)g= 1 and further, by the non-singularity of gin Int( Dg), it follows that max\\nDgg= max\\nBd(Dg)g= 1. All that\\nis left now is to show that max\\nDSη(ξ, ζ;c1, c2, c3) = max\\nDSη(ξ, ζ; 1,1,1)def= max\\nDgg(ξ, ζ) = 1 for any c1, c2andc3. By (42c)\\nit is sufficient to show that there exist ξ∗andζ∗such that η(ξ∗, ζ∗;c1, c2, c3) = 1. Indeed, substitute ( ξ∗, ζ∗) = (0 ,1)\\nin equation (41) to verify this statement and complete the proof.\\n4.1.2 Closed Forms of the Geometric Moments of the Modified Wigley Hull\\nDue to the simple analytical expression (41) of the modified Wigley hull, it is possible to evaluate the moments (8)\\nanalytically with the relevant analysis carried out in Appendix B. Specifically, M(p, q, r ) and MT(p, q, r ) are given\\nin equations (68) and (72) respectively, while MS(p, q, r ) and MI(p, q, r ) can easily be computed using (7) and (8)\\n13' metadata={'source': './data/2403.06990.pdf', 'page': 12}\n",
            "page_content='Figure 8: Renders of the modeller instances depicted in Figure 7. Specifically, from left to right, each hull corresponds\\ntoc3={0.0,0.25,0.5,0.75,1.0}with all other parameters fixed at {L= 1, B= 0.0996, T= 0.13775 , c1= 0, c2=\\n0}.\\n4.2 Wave Resistance Coefficient\\nIn line with [12], Pis identified with the wave resistance coefficient, Cw. This choice can be attributed to its explicit\\nconnection to the SAC, S(x), of a hull which is a purely geometric property. This connection was initially established\\nin the context of slender body theory via the so called Vossers’ integral [41] as in equation (12). Further development\\ncontinued ([45]; [46]; [47]) which, however, resulted in a theory suffering from a number of deficiencies as pointed out\\nin [48] and [44]. Nevertheless, the SAC’s importance in resistance-related ship-design problems which originated by\\nthe so-called Lackenby transformation [49] has continued till recent times ([21]; [50])\\nTheCwof (41) was approximated using a Boundary Element Method-based solver in the context of IsoGeometric\\nAnalysis (IGA-BEM), as introduced in [51]. To utilise this solver, the input geometry must be provided in B-spline\\nform, which is not the case for (41). However, it is important to note that (41) represents a polynomial surface of\\ndegree 8 in ξand 10 in ζ. This means that only a change from the monomial to the B-spline basis is necessary or rather\\nto the simpler Bernstein basis, needing only a single 8 ×10 patch for the surface to be explicitly represented. The\\nresulting Bezier surface will be represented by (8 + 1)(10 + 1) = 99 control points bi,j,i= 0, ...,8, j= 0, ...,10 which\\nare calculated numerically: for Bn,ithe Bernstein polynomial of degree n, find bi,jsuch that for D: [0,1]2→R3\\nthe wigley hull presented in (41) it is true that\\nD(ξ, ζ) =8X\\ni=010X\\nj=0B8,i(ξ)B10,j(ζ)bi,j, ξ×ζ∈[0,1]2. (43)\\nTo find the control points in (43) it is enough to evaluate Dat 99 distinct points in [0 ,1]2,pi,jso that (43) is\\nconverted to three linear systems (one for each coordinate of bi,j) each of 99 independent equations in 99 unknowns.\\nFinally, the choice of pi,jwas done uniformly in ξ, ζ,\\npi,j= (i/8, j/10), i= 0, ...,8, j= 0, ...,10. (44)\\nHaving found bi,jit was then straightforward to supply the relevant knot sequences such that the resulting Bezier\\nsurface can be interpreted as a B-spline patch.\\n5 Results and Discussion\\nThis section presents the detailed numerical results of the methodology outlined in Figure 2 both for SSV- and\\nslender body ( G)-based geometric operators, presented in Sections 3.2 and 3.3, respectively. The approximation of\\nthe relevant sensitivity indices is carried out using a sampling-based strategy. This implies the necessity of selecting\\na sampling strategy and analysing the computed approximation (38) for convergence.\\nThe chosen sampling strategy is detailed in Section 5.1, while its convergence in case of wave resistance, SSV- and\\nG-based geometric operators is presented in Sections 5.2, 5.3, and 5.4, respectively. Finally, in Section 5.5, all the\\naforementioned results are summarised. Correlation measures (40a) and (40b) are applied to the computed indices\\nto compare the performance of SSV versus that of the slender body operator.\\n14' metadata={'source': './data/2403.06990.pdf', 'page': 13}\n",
            "page_content='5.1 Dynamic Propagation Sampling\\nThe choice of sampling algorithm is of fundamental importance to the expedited convergence of (38), therefore, a\\nDynamic Propagation Sampling (DPS) [8] is utilised in this study. The principal idea behind DPS is an optimisation\\nbased approach, which aims at creating uniformly distributed and diverse number of sample over multiple run. DPS\\nachieves this while minimising an objective function, which is a weighted combination of three sampling criteria;\\nspace-filling ,non-collapsing andrepulsion .\\nGiven a sample-set of Nsamples, SN={si, i= 1, ..., N},si∈Rnthe space-filling criterion ensures that the\\nsamples siare uniformly distributed over the entire design space. This criterion is defined as f1: (Rn)N→R\\nf1(SN) =N−1X\\ni=1NX\\nj=i+11\\n∥si−sj∥2, (45)\\nwhere ∥ · ∥is the Euclidean norm in Rk. However, minimisation of f1tends to place samples towards the boundary\\nof the design space. To address this issue, the non-collapsing criterion is introduced through the discretisation\\nof the design space into Nkdisjoint subsets and the construction of a functional that discourages samples from\\nbeing in the same subset. Specifically, if the range of each parameter ci∈[ai, bi] is divided into Nsub-intervals\\nai=t0\\ni< t1\\ni< ... < tN\\ni=bidefine the discrete position ds∈Rnof a sample s∈Rnas,\\nds= [d1, ..., d N],such that ∀i,si∈[tdi\\ni, tdi+1\\ni). (46a)\\nThen, the related functional is defined as,\\nf2(SN) =ωN−1X\\ni=1NX\\nj=i+1K(si,sj), (46b)\\nK(si,sj) =nX\\nm=1δ\\x10\\x00\\ndsi\\x01\\nm,\\x00\\ndsj\\x01\\nm\\x11\\n, (46c)\\nwhere ωis a user-controlled weight on the non-collapsing criterion,\\x00\\nds\\x01\\nmis the mthelement of the discrete position\\nofsandδ(i, j) is Kronecker’s delta.\\nFinally, DPS can be used to create a new set of samples, while ensuring that they are different from the previous\\nsampled samples. Given an already existent sample set of Msamples ˆSM={ˆsi, i= 1, ..., M }DPS uses the repulsion\\ncriterion ( f3) to ensure that the new samples SNare different from the previous sampled designs. The formulation\\noff3is similar to that of f1,\\nf3(SN) =NX\\ni=1MX\\nj=11\\n∥si−ˆsj∥2, (47)\\nwhere ∥ · ∥is the Euclidean norm in Rk. Finally, define f(SN) =f1(SN) +f2(SN) +f3(SN) as the functional to be\\nminimised.\\n5.2Cwbased SA\\nIn order to identify the number of samples Nto use for the reliable implementation of (38) the relevant convergence\\nhas to be investigated. Figure 9 show the sensitivity indices of the parameters with respect to Cwover the varying\\nnumber of samples. Evaluation of sensitivity indices commenced with 10 samples and varied to 350 samples. It\\ncan be seen that sensitivity indices are unstable until 100 samples, however, they tend to converge afterwarding,\\nindicating that c1, c2are sensitive parameters while c3is an insensitive parameter. This is to be expected due to\\nthe nature of the wave-making phenomenon: due to the wave effect exponential decaying with respect to depth,\\nc3which as shown in Figure (7) predominantly affects the keel of the hull, is expected to have less of an effect on\\nCwwhen compared to c1, c2which predominantly affect the deck. Moreover, notice that the effect of c1onCwis\\nsignificantly greater than that of c2which can be again justified by comparing Figures 3 and 5: the c1parameter\\nhas a significantly greater effect on the deck-geometry, compared to c2.\\n15' metadata={'source': './data/2403.06990.pdf', 'page': 14}\n",
            "page_content='0 0.2 0.4 0.6 0.8 1\\n 0  50  100  150  200  250  300  350SI\\nNumber of Samplesc1\\nc2\\nc3\\nThreshooldFigure 9: Convergence of Cw-based sensitivity indices. The horizontal axis is the number of DPS-generated samples\\nwhile the vertical axis shows the sensitivity indices (SI)\\n5.3 SSV based SA\\nIn contrast to the Cw-based SA, performing sensitivity analysis with respect to SSV necessitates the choice of a\\nright order n. In [12], the authors investigated up to n= 4, as high-order geometric moments can be sensitive to\\nnoise while at the same time, numerical inaccuracies are ever-present when evaluating high-order terms. However,\\ndue to the simple expression of (41), the evaluation of high-order moments is computationally inexpensive and less\\npronounce to noise and inaccuracies. Therefore, in this study, the chosen range of nis{2,···,15}. Consequently,\\neach order nwill yield its respective parameter sensitivity indices, necessitating a convergence analysis for all orders\\nand parameters. This results in a total of 42 distinct convergence graphs, which can be found in Figures 10 through\\n12. Each graph corresponds to one of the three parameters: c1,c2, and c3.\\nSeveral noteworthy observations should be made about these results. Firstly, it is apparent that the number of\\nsamples required for convergence is significantly higher compared to Cwin Figure 9. This disparity can be attributed\\nto certain sensitivity indices being closer to the threshold value set at 0.05. Additionally, indices evaluated with\\nrespect to higher-order SSVs tend to exhibit greater stability with regard to the number of samples. In other words,\\nthey require fewer samples to achieve convergence compared to lower-order SSVs. This trend is particularly evident\\nin parameters c1andc2, which were sensitive to Cw. Moreover, note the similarity in sensitivity indices related to\\nconsecutive orders of the form 2 kand 2 k+ 1. This behaviour can be attributed to the length and breadth-wise\\nsymmetry of the Wigley hull (41), resulting in the vanishing of odd-index moments in their first and/or second\\npositions, as seen in (68).\\n 0 0.05 0.1 0.15 0.2\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c1 Sensitivity Index\\nNumber of SamplesSSV 2, SSV 3\\nSSV 4, SSV 5\\nSSV 6, SSV 7\\nSSV 8, SSV 9\\nSSV 10, SSV 11\\nSSV 12, SSV 13\\nSSV 14, SSV 15\\nThreshold\\nFigure 10: Convergence of the SSV-based sensitivity index of parameter c1with respect to the number of samples\\nacross varying order from 2 to 15.\\n16' metadata={'source': './data/2403.06990.pdf', 'page': 15}\n",
            "page_content='-0.04-0.02 0 0.02 0.04 0.06 0.08\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c2 Sensitivity Index\\nNumber of SamplesSSV 2, SSV 3\\nSSV 4, SSV 5\\nSSV 6, SSV 7\\nSSV 8, SSV 9\\nSSV 10, SSV 11\\nSSV 12, SSV 13\\nSSV 14, SSV 15\\nThresholdFigure 11: Convergence of the SSV-based sensitivity index of parameter c2with respect to the number of samples\\nacross varying order from 2 to 15.\\n 0.7 0.75 0.8 0.85 0.9 0.95 1\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c3 Sensitivity Index\\nNumber of SamplesSSV 2, SSV 3\\nSSV 4, SSV 5\\nSSV 6, SSV 7\\nSSV 8, SSV 9\\nSSV 10, SSV 11\\nSSV 12, SSV 13\\nSSV 14, SSV 15\\nFigure 12: Convergence of the SSV-based sensitivity index of parameter c3with respect to the number of samples\\nacross varying order from 2 to 15.\\nFigures 13 through 15 show the comparative results of sensitivity indices with respect to SSV and Cw, where each\\nfigure is related to one of the parameters. In the case of parameter c1, which is clearly sensitive to CW, its sensitivity\\nindex increases with the order nof SSV. Interestingly, it is insensitive until SSV 7and then tends to increase steadily.\\nHowever, in Figure 14, c2indicates a behaviour opposite to c1, i.e., its sensitivity reduces as the order of SSV\\nincreases. Based on the threshold of 0.05, it tends to be sensitive up to SSV 5. Note that in Figure 14, some of the\\nindices are negative, indicating convergence, but some are numerically zero. Since their confidence intervals include\\nzero, we can safely treat them as zero. Finally, in the case of c3, SSV nandCware in complete disagreement, with\\nthis parameter being insensitive to Cwwhile highly sensitive to SSV n. This misalignment in the c3highlights one\\nof the shortcomings of SSV—its inability to differentiate between keel-wise and deck-wise changes to the geometry,\\nwhich are crucial for wave-making.\\nFigure 13: Cw- and SSV-based sensitivity indices of parameter c2evaluated with 8000 samples over SSV orders\\nvarying from 2 to 15.\\n17' metadata={'source': './data/2403.06990.pdf', 'page': 16}\n",
            "page_content='Figure 14: Cw- and SSV-based sensitivity indices of parameter c2evaluated with 8000 samples over SSV orders\\nvarying from 2 to 15.\\nFigure 15: Cw- and SSV-based sensitivity indices of parameter c2evaluated with 8000 samples over SSV orders\\nvarying from 2 to 15.\\n5.4 Slender Body Operator based SA\\nConsidering (58) and in contrast to the SSV-based sensitivity analysis, it would not yield meaningful results to the\\nreader by deriving outcomes for a range of orders nas depicted in Figures 10 through 15. This would unnecessarily\\nlengthen the paper. Instead, it is more reasonable to identify a singular sufficiently high nwhere the corresponding\\nsensitivity indices have reached convergence. To ensure the convergence of (38) concerning both the sample size and\\nthe order n. These two variables are incrementally adjusted in the following manner: for each order n, the necessary\\nsample size to guarantee the convergence of G(n) is determined before examining whether the consecutive differences\\n|G(k+ 1)− G(k)|have sufficiently diminished. Figures 16 through 18 depict the G-based sensitivity indices c1,c2,\\nandc3respectively, as the sample size increases for G-orders ranging from 0 to 15.\\n 0.1 0.2 0.3 0.4 0.5 0.6\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c1 Sensitivity Index\\nNumber of SamplesG0\\nG1\\nG2\\nG3\\nG4\\nG5\\nG6\\nG7\\nG8G9\\nG10\\nG11\\nG12\\nG13\\nG14\\nG15\\nThreshold\\nFigure 16: Convergence of the G-based sensitivity index of parameter c1with respect to the number of samples\\nacross different orders of G.\\n18' metadata={'source': './data/2403.06990.pdf', 'page': 17}\n",
            "page_content='0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c2 Sensitivity Index\\nNumber of SamplesG0\\nG1\\nG2\\nG3\\nG4\\nG5\\nG6\\nG7\\nG8G9\\nG10\\nG11\\nG12\\nG13\\nG14\\nG15\\nThresholdFigure 17: Convergence of the G-based sensitivity index of parameter c2with respect to the number of samples\\nacross different orders of G.\\n-0.04-0.02 0 0.02 0.04 0.06 0.08\\n 0  1000  2000  3000  4000  5000  6000  7000  8000c3 Sensitivity Index\\nNumber of SamplesG0\\nG1\\nG2\\nG3\\nG4\\nG5\\nG6\\nG7\\nG8G9\\nG10\\nG11\\nG12\\nG13\\nG14\\nG15\\nThreshold\\nFigure 18: Convergence of the G-based sensitivity index of parameter c3with respect to the number of samples\\nacross different orders of G.\\n 0 0.2 0.4 0.6 0.8 1\\n 0  2  4  6  8  10  12  14Sensitivity Index\\nG Orderc1\\nc2\\nc3\\nThreshold\\nFigure 19: Convergence of G-based sensitivity indices with respect to order from 0 to 15 at 8000 samples.\\nNotice in Figures 16 through 18 that the graphs of G12,G13,G14, and G15 are practically identical, providing\\nevidence for the validity of (58). To better illustrate this, sensitivity indices for all parameters and orders were\\nretrieved at 8000 samples from Figures 16, 17, and 18 and replotted, this time with the Gorders on the horizontal\\naxis (see Figure 19). Evidently, for orders greater than 11, the sensitivity indices for c1, c2,andc3show negligible\\nvariation.\\nFurther insights were gained by retrieving the sensitivity indices of the three parameters at order 15 and plotting\\nthem in comparison to their Cw-based counterparts (see Figure 20). The results indicate agreement between Gand\\nCwwith respect to the threshold. Additionally, the relative importance of c1, c2,andc3is maintained during the\\ntransition between CwandG. In both cases, it holds true that SIc1> SI c2> SI c3.\\n19' metadata={'source': './data/2403.06990.pdf', 'page': 18}\n",
            "page_content='0 0.2 0.4 0.6 0.8 1\\nc1 c2 c3Sensitivity IndexG15\\nCw\\nThresholdFigure 20: G15-based sensitivity indices comparison to Cw-based sensitivity indices at 8000 samples\\n5.5 Summary of SA Results\\nThe resulting sensitivity indices with respect to SSV, G, and Cwfor parameters c1,c2, and c3are summarized in\\nFigures 21, 22, and 23, respectively. Notice that in all figures, the behaviour of G15is significantly more aligned with\\nCwcompared to that of SSV n, especially in the case of c3.\\n 0 0.2 0.4 0.6 0.8 1\\nSSV2\\nSSV3\\nSSV4\\nSSV5\\nSSV6\\nSSV7\\nSSV8\\nSSV9\\nSSV10\\nSSV11\\nSSV12\\nSSV13\\nSSV14\\nSSV15\\nG15Cwc1 Sensitivity IndexThreshold\\nFigure 21: Sensitivity indices of c1parameter based on SSV of orders varying from 2 to 15, Gof order 15, and Cw\\n 0 0.2 0.4 0.6 0.8 1\\nSSV2\\nSSV3\\nSSV4\\nSSV5\\nSSV6\\nSSV7\\nSSV8\\nSSV9\\nSSV10\\nSSV11\\nSSV12\\nSSV13\\nSSV14\\nSSV15\\nG15Cwc2 Sensitivity IndexThreshold\\nFigure 22: Sensitivity indices of c2parameter based on SSV of orders varying from 2 to 15, Gof order 15, and Cw\\n20' metadata={'source': './data/2403.06990.pdf', 'page': 19}\n",
            "page_content='0 0.2 0.4 0.6 0.8 1\\nSSV2\\nSSV3\\nSSV4\\nSSV5\\nSSV6\\nSSV7\\nSSV8\\nSSV9\\nSSV10\\nSSV11\\nSSV12\\nSSV13\\nSSV14\\nSSV15\\nG15Cwc3 Sensitivity IndexThresholdFigure 23: Sensitivity indices of c3parameter based on SSV of orders varying from 2 to 15, Gof order 15, and Cw.\\nThe correlation measures (40a) and (40b) can be applied to Figures 21 through 23 so that the performance of\\nSSV relative to that of Gcan be more easily compared. Doing so for (40a) and (40b) results in Figures 24 and\\n25, respectively. Notice in Figure 25 that the slender body operator G15is able to capture 100% of the sensitive\\nparameters.\\n 0 0.2 0.4 0.6 0.8 1\\nSSV2\\nSSV3\\nSSV4\\nSSV5\\nSSV6\\nSSV7\\nSSV8\\nSSV9\\nSSV10\\nSSV11\\nSSV12\\nSSV13\\nSSV14\\nSSV15\\nG15NRMSE\\nFigure 24: Error (40a) between SSV- and Cw-based sensitivity indices compared to error between G-based and Cw-\\nbased sensitivity indices.\\n 0 0.2 0.4 0.6 0.8 1\\nSSV2\\nSSV3\\nSSV4\\nSSV5\\nSSV6\\nSSV7\\nSSV8\\nSSV9\\nSSV10\\nSSV11\\nSSV12\\nSSV13\\nSSV14\\nSSV15\\nG15Similarity\\nFigure 25: Similarity (40b) between SSV- and Cw-based sensitivity indices compared to similarity between G- and\\nCw-based sensitivity indices.\\nNext, in Table 1, one can identify the sensitive parameters from Figures 21, 22, 23, in order of importance, as well\\nas the normalised error and percentage similarity from Figures 24 and 25. Notice that not only does G15outperform\\nSSV nin terms of error (40a) and similarity (40b), but also the relative importance of the parameters is preserved, so\\nthatc1can be correctly identified as more influential than c2. These results illustrate that the inclusion of additional\\ninformation regarding the underlying geometry with the SSV noperators, in contrast to the restriction to moments\\nof the form M(N,0,0), as is the case with G, produces results consistently less aligned with that of the physics\\noperator Cw.\\n21' metadata={'source': './data/2403.06990.pdf', 'page': 20}\n",
            "page_content='Quantity of Interest Sensitive Parameters NRMSE Similarity\\nCw c1> c2 N/A N/A\\nG15 c1> c2 0.267455 100%\\nSSV 15 c3> c1 0.766259 50%\\nSSV 14 c3> c1 0.766271 50%\\nSSV 13 c3> c1 0.801278 50%\\nSSV 12 c3> c1 0.801279 50%\\nSSV 11 c3> c1 0.839496 50%\\nSSV 10 c3> c1 0.839486 50%\\nSSV 9 c3 0.877892 0%\\nSSV 8 c3 0.877874 0%\\nSSV 7 c3 0.908029 0%\\nSSV 6 c3 0.908006 0%\\nSSV 5 c3 0.911654 0%\\nSSV 4 c3 0.911578 0%\\nSSV 3 c3> c2 0.865169 50%\\nSSV 2 c3> c2 0.864787 50%\\nTable 1: Comparison among CW, SSV, and G-based sensitivity analyses. The second column displays the order of\\nsensitive parameters concerning the respective quantity of interest. The third and fourth columns present the error\\n(40a) and discrete similarity (40b) between sensitivity indices evaluated with respect to G15and SSV ncompared to\\nthe indices obtained with Cw.\\n5.6 Computational cost\\nThe computational cost to perform sensitivity analysis with geometric operators is significantly less than performing\\nthese analysis with Cw. On a PC with Intel(R) Xeon(R) Gold 6226 CPU with 2.70 GHz and 2.69 GHz processors\\nand 128 GB of memory on average, each run of the IGA-BEM solver to evaluate Cwtook about 540 seconds, totalling\\naround 210 hours to produce Figure 9. In contrast, both the SSV-based analysis and the G-based analysis required\\ncomputational time in the order of seconds. To compare the per-run computational cost of Cwversus SSV (10) and\\nG(32), a benchmark across 50 random samples was performed for all operators to establish, for a given order, the\\nnecessary computational cost. These results can be seen in Figure 26, where one can readily notice that for the\\ntested orders, SSV is at least four orders of magnitude faster than Cw, andGis about 1 to 2 orders of magnitude\\nfaster than SSV. It is noteworthy that as the order increases, the computational cost of SSV rises much faster than\\nthat of G, which can be attributed to the number of new moments that each order introduces.\\n 10 100 1000 10000 100000 1x106 1x107 1x108 1x109\\n 5  10  15  20  25  30Microseconds\\nOrderSSV\\nG\\nCw\\nFigure 26: Per-run computational cost of evaluating Cw, SSV and Gverses the order of SSV and G.\\n6 Conclusion and Future Work\\nThis study proposes a geometry-based operator to support computationally demanding physical models for reducing\\nthe dimensionality of the problem through sensitivity analysis. A general framework for matching physics-based\\nquantities Pto geometry-based ones is outlined in Figure 2. Two geometry-based operators are proposed: the SSV,\\npreviously tested in [12], and the slender body operator G, derived in this paper based on slender body theory.\\nChoosing P=Cwas the wave resistance coefficient and Das the modified Wigley hull parametric modeller, the\\nframework is applied twice: once to investigate the compatibility between Cwand SSV, and once for CwandG.\\nThe results indicate that Goutperforms SSV significantly, as summarised in Table 1. In this table, one can observe\\nthat the slender body operator outperforms SSV for all displayed orders, achieving 100% similarity to Cwwhile\\nalso preserving the relative importance of the sensitive parameters. Furthermore, the computational cost of the\\n22' metadata={'source': './data/2403.06990.pdf', 'page': 21}\n",
            "page_content='G-based operator was significantly less than that of Cw, with the former taking seconds and the latter taking days.\\nAdditionally, as shown in Figure 26, the computational cost of Gis significantly lower than that of SSV, a difference\\nthat becomes more pronounced as the order increases.\\nWhile the slender body operator has proven to be a promising candidate geometric operator for the problem of wave\\nresistance when paired with the parametric modeller (41), several avenues have not been explored in this study. First\\nand foremost, the framework of Figure 2 was applied only for one parametric modeller, which also has a small number\\nof parameters. Further experimentation with richer parametric modellers is necessary to expand the applicability of\\nthe pair G − P . The two parametric modellers tested in [12], for which SSV performed satisfactorily, are promising\\ncandidates in this regard, allowing for a more in-depth investigation into the seemingly better performance of G\\ncompared to SSV.\\nNext, the relatively short computational times of the G-based analysis can be attributed in part to the closed-form\\nexpression for the moments (72), made possible by the simple analytical expression of (41). However, it is acknowl-\\nedged that this may not be possible with more complicated geometries. Nevertheless, relevant experimentation has\\nshown that the increase in computational time is not drastic in the general-surface case, still providing a significant\\nreduction when compared to the Cw-based sensitivity analysis.\\nFinally, there is potential to enhance Gfurther by considering the generalised formulation of Vossers’ integral, as\\nprovided in [45]. This has the potential to increase the performance of the slender body operator.\\nACKNOWLEDGEMENTS\\nThis work received funding from the European Union’s Horizon 2020 research and innovation programme under the\\nMarie Sk lodowska-Curie grant agreement No 860843, PI for the University of Strathclyde: P.D. Kaklis.\\nReferences\\n[1] L. Sun, H. Gao, S. Pan, J.-X. Wang, Surrogate modeling for fluid flows based on physics-constrained deep\\nlearning without simulation data, Computer Methods in Applied Mechanics and Engineering 361 (2020) 112732.\\n[2] M. K¨ oppen, The curse of dimensionality, in: 5th online world conference on soft computing in industrial appli-\\ncations (WSC5), Vol. 1, 2000, pp. 4–8.\\n[3] S. Khan, M. J. Awan, A generative design technique for exploring shape variations, Advanced Engineering\\nInformatics 38 (2018) 712–724.\\n[4] S. Chen, J. Montgomery, A. Boluf´ e-R¨ ohler, Measuring the curse of dimensionality and its effects on particle\\nswarm optimization and differential evolution, Applied Intelligence 42 (2015) 514–526.\\n[5] Z. Masood, S. Khan, L. Qian, Machine learning-based surrogate model for accelerating simulation-driven opti-\\nmisation of hydropower kaplan turbine, Renewable Energy 173 (2021) 827–848.\\n[6] D. D’Agostino, A. Serani, E. F. Campana, M. Diez, Nonlinear methods for design-space dimensionality reduction\\nin shape optimization, in: International Workshop on Machine Learning, Optimization, and Big Data, Springer,\\n2017, pp. 121–132.\\n[7] P. G. Constantine, E. Dow, Q. Wang, Active subspace methods in theory and practice: Applications to kriging\\nsurfaces, SIAM Journal on Scientific Computing 36 (4) (2014) A1500–A1524.\\n[8] S. Khan, P. Kaklis, From regional sensitivity to intra-sensitivity for parametric analysis of free-form shapes:\\nApplication to ship design, Advanced Engineering Informatics 49 (2021) 101314.\\n[9] A. Saltelli, M. Ratto, T. Andres, F. Campolongo, J. Cariboni, D. Gatelli, M. Saisana, S. Tarantola, Global\\nSensitivity Analysis: The Primer, Wiley, 2008.\\n[10] A. Saltelli, P. Annoni, How to avoid a perfunctory sensitivity analysis, Environmental Modelling & Software\\n25 (12) (2010) 1508–1517.\\n[11] D. Cacuci, Sensitivity & Uncertainty Analysis, Volume 1: Theory, no. v. 1, CRC Press, 2003.' metadata={'source': './data/2403.06990.pdf', 'page': 22}\n",
            "page_content='25 (12) (2010) 1508–1517.\\n[11] D. Cacuci, Sensitivity & Uncertainty Analysis, Volume 1: Theory, no. v. 1, CRC Press, 2003.\\n[12] S. Khan, P. Kaklis, A. Serani, M. Diez, Geometric moment-dependent global sensitivity analysis without simu-\\nlation data: Application to ship hull form optimisation, Computer-Aided Design 151 (2022) 103339.\\n[13] K. Cheng, Z. Lu, C. Ling, S. Zhou, Surrogate-assisted global sensitivity analysis: an overview, Structural and\\nMultidisciplinary Optimization 61 (2020) 1187–1213.\\n[14] B. Iooss, P. Lemaˆ ıtre, A Review on Global Sensitivity Analysis Methods, Springer US, Boston, MA, 2015, pp.\\n101–122.\\n23' metadata={'source': './data/2403.06990.pdf', 'page': 22}\n",
            "page_content='[15] H. W. Bode, et al., Network analysis and feedback amplifier design (1945).\\n[16] E. Borgonovo, E. Plischke, Sensitivity analysis: A review of recent advances, European Journal of Operational\\nResearch 248 (3) (2016) 869–887.\\n[17] I. M. Sobol’, On sensitivity estimation for nonlinear mathematical models, Matematicheskoe modelirovanie 2 (1)\\n(1990) 112–118.\\n[18] F. Gamboa, A. Janon, T. Klein, A. Lagnoux, Sensitivity indices for multivariate outputs, Comptes Rendus\\nMathematique 351 (7) (2013) 307–310.\\n[19] F. Sarrazin, F. Pianosi, T. Wagener, Global sensitivity analysis of environmental models: Convergence and\\nvalidation, Environmental Modelling & Software 79 (2016) 135–152.\\n[20] C. O. S. Sorzano, J. Vargas, A. P. Montano, A survey of dimensionality reduction techniques, arXiv preprint\\narXiv:1403.2877 (2014).\\n[21] S. Han, Y.-S. Lee, Y. B. Choi, Hydrodynamic hull form optimization using parametric models, Journal of marine\\nscience and technology 17 (2012) 1–17.\\n[22] D. Zhang, G. Lu, et al., A comparative study of fourier descriptors for shape representation and retrieval, in:\\nProc. 5th Asian Conference on Computer Vision, Citeseer, 2002, p. 35.\\n[23] E. Rahtu, M. Salo, J. Heikkila, A new convexity measure based on a probabilistic interpretation of images,\\nIEEE Transactions on Pattern Analysis and Machine Intelligence 28 (9) (2006) 1501–1512.\\n[24] P. Corcoran, P. Mooney, A. C. Winstanley, A convexity measure for open and closed contours (2011).\\n[25] M. Stojmenovi´ c, J. ˇZuni´ c, Measuring elongation from shape boundary, Journal of Mathematical Imaging and\\nVision 30 (2008) 73–85.\\n[26] J. A. Shohat, J. D. Tamarkin, The problem of moments, Vol. 1, American Mathematical Society (RI), 1950.\\n[27] A. Cuyt, G. Golub, P. Milanfar, B. Verdonk, Multidimensional integral inversion, with applications in shape\\nreconstruction, SIAM Journal on Scientific Computing 27 (3) (2005) 1058–1070. arXiv:https://doi.org/10.\\n1137/030601703 ,doi:10.1137/030601703 .\\nURL https://doi.org/10.1137/030601703\\n[28] P. Milanfar, M. Putinar, J. Varah, B. Gustafsson, G. H. Golub, Shape reconstruction from moments: theory,\\nalgorithms, and applications, in: Advanced signal processing algorithms, architectures, and implementations X,\\nVol. 4116, SPIE, 2000, pp. 406–416.\\n[29] A. Kousholt, J. Schulte, Reconstruction of convex bodies from moments, Discrete & Computational Geometry\\n65 (1) (2021) 1–42.\\n[30] F. Ghorbel, S. Derrode, S. Dhahbi, R. Mezhoud, et al., Reconstructing with geometric moments, in: Proc. int.\\nconf. on machine intelligence: Acidca-icmi, 2005, pp. 5–7.\\n[31] J. M. Pozo, M.-C. Villa-Uriol, A. F. Frangi, Efficient 3d geometric and zernike moments computation from\\nunstructured surface meshes, IEEE Transactions on Pattern Analysis and Machine Intelligence 33 (3) (2010)\\n471–484.\\n[32] P. Diaconis, Application of the method of moments in probability and statistics, Moments in mathematics 37\\n(1987) 125–142.\\n[33] C.-H. Teh, R. T. Chin, On image analysis by the methods of moments, IEEE Transactions on pattern analysis\\nand machine intelligence 10 (4) (1988) 496–513.\\n[34] M. I. Sezan, H. Stark, Incorporation of a priori moment information into signal recovery and synthesis problems,\\nJournal of mathematical analysis and applications 122 (1) (1987) 172–186.\\n[35] S. Khan, P. Kaklis, A. Serani, M. Diez, K. Kostas, Shape-supervised dimension reduction: Extracting geometry\\nand physics associated features with geometric moments, Computer-Aided Design 150 (2022) 103327.\\n[36] P. Milanfar, G. C. Verghese, W. C. Karl, A. S. Willsky, Reconstructing polygons from moments with connections\\nto array processing, IEEE Transactions on signal processing 43 (2) (1995) 432–443.\\n[37] P. Milanfar, W. C. Karl, A. S. Willsky, A moment-based variational approach to tomographic reconstruction,\\nIEEE Transactions on Image Processing 5 (3) (1996) 459–470.\\n[38] V. Strakhov, M. Brodsky, On the uniqueness of the inverse logarithmic potential problem, SIAM Journal on' metadata={'source': './data/2403.06990.pdf', 'page': 23}\n",
            "page_content='IEEE Transactions on Image Processing 5 (3) (1996) 459–470.\\n[38] V. Strakhov, M. Brodsky, On the uniqueness of the inverse logarithmic potential problem, SIAM Journal on\\nApplied Mathematics 46 (2) (1986) 324–344.\\n24' metadata={'source': './data/2403.06990.pdf', 'page': 23}\n",
            "page_content='[39] M. Brodsky, E. Panakhov, Concerning a priori estimates of the solution of the inverse logarithmic potential\\nproblem, Inverse problems 6 (3) (1990) 321.\\n[40] S. Khan, K. Goucher-Lambert, K. Kostas, P. Kaklis, Shiphullgan: A generic parametric modeller for ship hull\\ndesign using deep convolutional generative model, Computer Methods in Applied Mechanics and Engineering\\n411 (2023) 116051.\\n[41] G. Vossers, Some applications of the slender body theory in ship hydrodynamics (1962).\\n[42] W. Hoeffding, A class of statistics with asymptotically normal distribution, Breakthroughs in Statistics: Foun-\\ndations and Basic Theory (1992) 308–334.\\n[43] I. J. Journ´ ee, Experiments and calculations on four wigley hullforms, Delft University 909 (1992).\\n[44] J. V. Wehausen, The wave resistance of ships, Advances in applied mechanics 13 (1973) 93–245.\\n[45] H. Maruo, Calculation of the wave resistance of ships, the draught of which is as small as the beam, Journal of\\nZosen Kiokai 1962 (112) (1962) 21–37.\\n[46] E. Tuck, On vossers’ integral, in: International Seminar on Theoretical Wave Resistance, Ann Arbor, Michigan,\\n1963, 1963.\\n[47] E. Tuck, A systematic asymptotic expansion procedure for slender ships, Journal of Ship Research 8 (03) (1964)\\n15–23.\\n[48] J. Kotik, Various wave resistance theories for slender ships, Schiffstechnik 10 (54) (1963) 178–186.\\n[49] H. Lackenby, On the systematic geometrical variation of ship forms, vol. 92, Transaction of Royal Institute of\\nNaval Architects (1950).\\n[50] M. Tasrief, M. Kashiwagi, Improvement of Ship Geometry by Optimizing the Sectional Area Curve With Binary-\\nCoded Genetic Algorithms (BCGAs), Vol. All Days of International Ocean and Polar Engineering Conference,\\n2013.\\n[51] K. Belibassakis, T. Gerostathis, K. Kostas, C. Politis, P. Kaklis, A. Ginnis, C. Feurer, A bem-isogeometric\\nmethod for the ship wave-resistance problem, Ocean Engineering 60 (2013) 53–67.\\nA Convergence of the Slender Body Operator\\nIn this section we show the convergence of (32), for which a comparison and a ratio test suffice. For S(x) the sectional\\narea curve of the hull at the longitudinal position xassume that there exists an M > 0 with,\\n|S′′(x)| ≤M,∀x. (48)\\nThen,\\nS′′(x)S′′(ξ)(x−ξ)2k≤M2(x−ξ)2k,∀x, ξ. (49)\\nLooking at (21) and since the right-hand side of (49) is certainly integrable,\\nIk=ZL/2\\n−L/2ZL/2\\n−L/2S′′(x)S′′(ξ)(x−ξ)2kdxdξ\\n≤M2ZL/2\\n−L/2ZL/2\\n−L/2(x−ξ)2kdxdξ\\n=M2ZL/2\\n−L/2\"\\n(x−ξ)2k+1\\n2k+ 1#L/2\\n−L/2dξ\\n=M2\\n2k+ 1\"\\n−(L/2−ξ)2k+2+ (−L/2−ξ)2k+2\\n2k+ 2#L/2\\n−L/2\\n=2M2L2k+2\\n(2k+ 1)(2 k+ 2). (50)\\nUsing (50) and looking at (32),\\n|f(k;K)Ik| ≤ |f(k;K)2M2L2k+2\\n(2k+ 1)(2 k+ 2)|, (51)\\n25' metadata={'source': './data/2403.06990.pdf', 'page': 24}\n",
            "page_content='which means that it is enough to show that the following series is absolutely convergent,\\n∞X\\nk=0βk, β k=f(k;K)2M2L2k+2\\n(2k+ 1)(2 k+ 2). (52)\\nThis can be done by application of the ratio test,\\n|βk+1|\\n|βk|=f(k+ 1;K)\\nf(k;K)L2(2k+ 1)(2 k+ 2)\\n(2k+ 3)(2 k+ 4)\\n=|f(k+ 1;K)|\\n|f(k;K)|O(1), k→ ∞ . (53)\\nThen, recalling the definition of f(k;K) from (18),\\n|f(k+ 1;K)|\\n|f(k;K)|=K222k−1(k!)2\\n22k+1((k+ 1)!)2·ln(0.5K) +γ−h(k+ 1)\\nln(0.5K) +γ−h(k)\\n=K2\\n4(k+ 1)2·ln(0.5K) +γ−h(k+ 1)\\nln(0.5K) +γ−h(k)\\n=ln(0.5K) +γ−h(k+ 1)\\nln(0.5K) +γ−h(k)O(k−2), k→ ∞ . (54)\\nNow, for the harmonic series h(k), it is true that h(k) =O(ln(k)), k→ ∞ . In light of this,\\nln(0.5K) +γ−h(k+ 1)\\nln(0.5K) +γ−h(k)=h(k+ 1)\\nh(k)·1−ln(0.5K) +γ\\nh(k+ 1)\\n1−ln(0.5K) +γ\\nh(k)\\n=k+1X\\ni=01\\ni\\nkX\\ni=01\\ni·1−O((ln(k+ 1))−1)\\n1−O((ln(k))−1)\\n= \\n1 +1\\n(k+ 1)h(k)!\\n·1 +o(1)\\n1 +o(1)\\n= (1 + O(k−1(ln(k))−1))(1 + o(1))\\n= 1 + o(1), k→ ∞ . (55)\\nSubstituting (55) into (54),\\n|f(k+ 1;K)|\\n|f(k;K)|= (1 + o(1))O(k−2) =o(1), k→ ∞ . (56)\\nSubstituting (56) into (53),\\n|βk+1|\\n|βk|=o(1), k→ ∞ , (57)\\nwhich means that {βi}is an absolutely convergent series. By comparison, (51) implies that\\nlim\\nn→∞G(n) exists and ∈R. (58)\\nB Closed Forms of the Geometric Moments of the Modified Wigley\\nHull\\nIn this section, we express the moments (2) and (4) for (41) analytically. We begin with M(p, q, r ) by substituting\\n(41) into (2),\\nM(p, q, r ) =ZL/2\\n−L/2ZT\\n0ZBη/2\\n−Bη/2xpyqzrdydzdx\\n=ZL/2\\n−L/2ZT\\n0xpzr\"\\nyq+1\\nq+ 1#Bη/2\\n−Bη/2dzdx\\n=\\x10B\\n2\\x11q+1(1−(−1)q+1)\\nq+ 1ZL/2\\n−L/2ZT\\n0xpzrηq+1dzdx. (59)\\n26' metadata={'source': './data/2403.06990.pdf', 'page': 25}\n",
            "page_content='Notice that if qis odd then M(p, q, r ) = 0 which can be attributed to the breadth-wise symmetry of the Wigley hull.\\nAssuming qto be even, we write\\nM(p, q, r ) =\\x10B\\n2\\x11q+12\\nq+ 1ZL/2\\n−L/2ZT\\n0xpzrηq+1dzdx, q = even . (60)\\nTo proceed, we expand ηq+1,\\nηq+1=\\x10\\n(1−ζ2)(1−ξ2)(1 + c1ξ2+c2ξ4) +c3ζ2(1−ζ8)(1−ξ2)4\\x11q+1\\n=q+1X\\ni1=0\\x12q+ 1\\ni1\\x13\\x10\\n(1−ζ2)(1−ξ2)(1 + c1ξ2+c2ξ4)\\x11q+1−i1\\x10\\nc3ζ2(1−ζ8)(1−ξ2)4\\x11i1\\n=q+1X\\ni1=0\\x12q+ 1\\ni1\\x13\\n(1−ζ2)q+1−i1(1−ξ2)q+1+3 i1(1 +c1ξ2+c2ξ4)q+1−i1ci1\\n3ζ2i1(1−ζ8)i1\\n=q+1X\\ni1=0\\x12q+ 1\\ni1\\x13 q+1−i1X\\ni2=0\\x12q+ 1−i1\\ni2\\x13\\n(−1)i2ζ2i2! q+1+3 i1X\\ni3=0\\x12q+ 1 + 3 i1\\ni3\\x13\\n(−1)i3ξ2i3!\\n· q+1−i1X\\ni4=0\\x12q+ 1−i1\\ni4\\x13\\n(c1ξ2+c2ξ4)i4!\\nci1\\n3ζ2i1 i1X\\ni6=0\\x12i1\\ni6\\x13\\n(−1)i6ζ8i6!\\n=q+1X\\ni1=0\\x12q+ 1\\ni1\\x13 q+1−i1X\\ni2=0\\x12q+ 1−i1\\ni2\\x13\\n(−1)i2ζ2i2! q+1+3 i1X\\ni3=0\\x12q+ 1 + 3 i1\\ni3\\x13\\n(−1)i3ξ2i3!\\n· q+1−i1X\\ni4=0\\x12q+ 1−i1\\ni4\\x13i4X\\ni5=0\\x12i4\\ni5\\x13\\nci5\\n1ξ2i5ci4−i5\\n2ξ4i4−4i5!\\nci1\\n3ζ2i1 i1X\\ni6=0\\x12i1\\ni6\\x13\\n(−1)i6ζ8i6!\\n. (61)\\nGrouping together all the sum-operators and combining common factors,\\nηq+1=q+1X\\ni1=0q+1−i1X\\ni2=0q+1+3 i1X\\ni3=0q+1−i1X\\ni4=0i4X\\ni5=0i1X\\ni6=0\\x12q+ 1\\ni1\\x13\\x12q+ 1−i1\\ni2\\x13\\x12q+ 1 + 3 i1\\ni3\\x13\\n·\\x12q+ 1−i1\\ni4\\x13\\x12i4\\ni5\\x13\\x12i1\\ni6\\x13\\n(−1)i2+i3+i6ci5\\n1ci4−i5\\n2ci1\\n3ζ2i1+2i2+8i6ξ2i3+4i4−2i5. (62)\\nSubstituting (62) into (60) and factoring all constants out of the double integral,\\nM(p, q, r ) =\\x10B\\n2\\x11q+12\\nq+ 1q+1X\\ni1=0q+1−i1X\\ni2=0q+1+3 i1X\\ni3=0q+1−i1X\\ni4=0i4X\\ni5=0i1X\\ni6=0\"\\n\\x12q+ 1\\ni1\\x13\\x12q+ 1−i1\\ni2\\x13\\x12q+ 1 + 3 i1\\ni3\\x13\\x12q+ 1−i1\\ni4\\x13\\x12i4\\ni5\\x13\\x12i1\\ni6\\x13\\n·(−1)i2+i3+i6ci5\\n1ci4−i5\\n2ci1\\n3ZL/2\\n−L/2ξ2i3+4i4−2i5xpdxZT\\n0ζ2i1+2i2+8i6zrdz#\\n, q = even . (63)\\nSince ξ= 2x/L,\\nZL/2\\n−L/2ξ2i3+4i4−2i5xpdx= (2/L)2i3+4i4−2i5ZL/2\\n−L/2xp+2i3+4i4−2i5dx\\n= (2/L)2i3+4i4−2i5\"\\nxp+2i3+4i4−2i5+1\\np+ 2i3+ 4i4−2i5+ 1#L/2\\n−L/2\\n=\\x10L\\n2\\x11p+1 (1−(−1)p+1)\\np+ 2i3+ 4i4−2i5+ 1. (64)\\nAgain, if pis odd, M(p, q, r ) = 0 which can be attributed to the length-wise symmetry of the Wigley hull. Assuming\\nthatpis even,\\nZL/2\\n−L/2ξ2i3+4i4−2i5xpdx=\\x10L\\n2\\x11p+1 2\\np+ 2i3+ 4i4−2i5+ 1, p = even . (65)\\n27' metadata={'source': './data/2403.06990.pdf', 'page': 26}\n",
            "page_content='Similarly for ζ=z/T,\\nZT\\n0ζ2i1+2i2+8i6zrdz=Tr+1 1\\nr+ 2i1+ 2i2+ 8i6+ 1. (66)\\nSubstituting (65) and (66) into (63),\\nM(p, q, r ) =\\x10L\\n2\\x11p+1\\x10B\\n2\\x11q+1\\nTr+14\\nq+ 1q+1X\\ni1=0q+1−i1X\\ni2=0q+1+3 i1X\\ni3=0q+1−i1X\\ni4=0i4X\\ni5=0i1X\\ni6=0\"\\n\\x12q+ 1\\ni1\\x13\\x12q+ 1−i1\\ni2\\x13\\x12q+ 1 + 3 i1\\ni3\\x13\\x12q+ 1−i1\\ni4\\x13\\x12i4\\ni5\\x13\\x12i1\\ni6\\x13\\n·(−1)i2+i3+i6ci5\\n1ci4−i5\\n2ci1\\n3(p+ 2i3+ 4i4−2i5+ 1)−1(r+ 2i1+ 2i2+ 8i6+ 1)−1#\\n, (67)\\nwhere p, q= even .\\nThe presentation of (67) can be simplified considerably by noticing that the sums can be split into three groups: the\\nfirst group consists only of the sum with index i1, the second group of the sums with indexes {i3, i4, i5}and the\\nfinal group with {i2, i6}. Then,\\nM(p, q, r ) =\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\x10L\\n2\\x11p+1\\x10B\\n2\\x11q+1\\nTr+14\\nq+ 1q+1X\\ni1=0\\x12q+ 1\\ni1\\x13\\nci1\\n3Xi1Zi1, p, q are even\\n0,else,(68a)\\nwhere\\nXi1=q+1+3 i1X\\ni3=0q+1−i1X\\ni4=0i4X\\ni5=0\\x12q+ 1 + 3 i1\\ni3\\x13\\x12q+ 1−i1\\ni4\\x13\\x12i4\\ni5\\x13\\n(−1)i3ci5\\n1ci4−i5\\n2(p+ 2i3+ 4i4−2i5+ 1)−1, (68b)\\nand\\nZi1=q+1−i1X\\ni2=0i1X\\ni6=0\\x12q+ 1−i1\\ni2\\x13\\x12i1\\ni6\\x13\\n(−1)i2+i6(r+ 2i1+ 2i2+ 8i6+ 1)−1. (68c)\\nHaving (68a) it is easy to evaluate an analogous expression for the translation invariant moment MT(p, q, r ). as\\ngiven in (4),\\nMT(p, q, r ) =ZL/2\\n−L/2ZT\\n0ZBη/2\\n−Bη/2(x−Cx)p(y−Cy)q(z−Cz)rdydzdx. (69)\\nFirst, notice that by (68a), M(1,0,0) = M(0,1,0) = 0. Then, looking at (3), Cx=Cy= 0,\\nMT(p, q, r ) =ZL/2\\n−L/2ZT\\n0ZBη/2\\n−Bη/2xpyq(z−Cz)rdydzdx. (70)\\nIn the derivation of (68a), the zrfactor (now ( z−Cz)r) was integrated at (66). One can then easily be convinced\\nthat the only difference between MT(p, q, r ) and (68a) will be in Zi1. Specifically, looking at (66)\\nZT\\n0ζ2i1+2i2+8i6(z−Cz)rdz=\\x10\\nT2i1+2i2+8i6\\x11−1ZT\\n0z2i1+2i2+8i6rX\\nj=0\\x12r\\nj\\x13\\nzr−j(−1)jCj\\nzdz\\n=\\x10\\nT2i1+2i2+8i6\\x11−1rX\\nj=0\\x12r\\nj\\x13\\n(−1)jCj\\nz\"\\nzr+2i1+2i2+8i6−j+1\\nr+ 2i1+ 2i2+ 8i6−j+ 1#T\\n0\\n=Tr+1rX\\nj=0\\x12r\\nj\\x13\\n(−1)j\\x10Cz\\nT\\x11j\\n(r+ 2i1+ 2i2+ 8i6−j+ 1)−1. (71)\\nSo that finally,\\nMT(p, q, r ) =\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\x10L\\n2\\x11p+1\\x10B\\n2\\x11q+1\\nTr+14\\nq+ 1q+1X\\ni1=0\\x12q+ 1\\ni1\\x13\\nci1\\n3Xi1Zi1, p, q are even\\n0,else,(72a)\\n28' metadata={'source': './data/2403.06990.pdf', 'page': 27}\n",
            "page_content='with\\nZi1=q+1−i1X\\ni2=0i1X\\ni6=0rX\\nj=0\\x12q+ 1−i1\\ni2\\x13\\x12i1\\ni6\\x13\\x12r\\nj\\x13\\n(−1)i2+i6+j\\x10Cz\\nT\\x11j\\n(r+ 2i1+ 2i2+ 8i6−j+ 1)−1, (72b)\\nwhere Czis given by (3) and Xi1by (68b). It should be noted that the validity of equations (68) and (72) were\\nverified via the computer algebraic system in MAPLE©4.\\nC Closed Form of the Sectional Area Curve of the Modified Wigley\\nHull\\nIn this section, a closed form for the sectional area of (41) at a given longitudinal position x∈[−L/2, L/2],S(x), is\\nderived. We can directly write\\nS(x) =ZT\\n0ZBη/2\\n−Bη/2dydz =BZT\\n0ηdz=B(1−ξ2)(1 + c1ξ2+c2ξ4)ZT\\n0(1−ζ2)dz+Bc3(1−ξ2)4ZT\\n0(ζ2−ζ10)dz\\n(73)\\nwhere ξ= 2x/Landζ=z/T. Evaluating the integrals in (73),\\nS(x) =BT \\n2\\n3(1−ξ2)(1 + c1ξ2+c2ξ4) +8c3\\n33(1−ξ2)4!\\n(74)\\nDifferentiating (74) with respect to x,\\nS′(x) =2BT\\nL \\n−4ξ\\n3(1 +c1ξ2+c2ξ4) +2\\n3(1−ξ2)(2c1ξ+ 4c2ξ3−32c3ξ(1−ξ2)2/11)!\\n(75)\\nForx=±L/2,ξ(x=±L/2) =±1 we get,\\nS′(±L/2) =∓8BT\\n3L(1 +c1+c2) (76)\\nwhich attains its maximum at c1=c2= 1,|S′(±L/2;c1=c2= 1)|= 8BT/L\\n4https://www.maplesoft.com/products/Maple/\\n29' metadata={'source': './data/2403.06990.pdf', 'page': 28}\n",
            "Processing  ./data/2403.06991.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='A multilayer shallow water model for polydisperse reactive\\nsedimentation\\nJulio Careaga†,∗and V´ıctor Osores‡\\nAbstract\\nA three-dimensional model of polydisperse reactive sedimentation is developed by means of a multi-\\nlayer shallow water approach. The model consists of a variety of solid particles of different sizes and\\ndensities, and substrates diluted in water, which produce biochemical reactions while the sedimen-\\ntation process occurs. Based on the Masliyah–Lockett–Bassoon settling velocity, compressibility of\\nthe sediment and viscosity of the mixture, the system of governing equations is composed by non-\\nhomogeneous transport equations, coupled to a momentum equation describing the mass-average\\nvelocity. Besides, the free-surface depicted by the total height of the fluid column is incorpo-\\nrated and fully determined through the multilayer approach. A finite volume numerical scheme on\\nCartesian grids is proposed to approximate the model equations. Numerical simulations of the den-\\nitrification process exemplify the performance of the numerical scheme and model under different\\nscenarios and bottom topographies.\\nKey words: Reactive sedimentation, multilayer shallow water model, polydisperse velocities, finite\\nvolume methods, viscous flow, sedimentation.\\nMathematics subject classifications (2000): 65N06, 76T20.\\n1 Introduction\\nReactive sedimentation is defined as the process in which multiple types of solid particles suspended\\non a fluid settle due to gravity, while reacting with substrates diluted in water. The solid phase is then\\ncomposed by organic or inert matter, including bacteria considered as solid particles, and the fluid\\nphase is constituted by substrates. An example of reactive sedimentation can be found in wastewater\\ntreatment industries, when the so-called activated sludge is treated in the Secondary Settling Tank.\\nActivated sludge models (ASMs) have been widely developed in terms of biokinetic reactions and or-\\ndinary differential equations, see e.g. [29, 31, 32, 34, 35, 36]. Models accounting space variations based\\non partial differential equations (PDEs) have been studied to a lesser extent. The one-dimensional\\nPDE-based model presented in [8], which has evolved from the previous work [9], constitutes one of the\\nfew works including the complexity of nonlinear effects appearing in the sedimentation-consolidation\\nprocess. Furthermore, this model is tailored to handle mixtures of arbitrary number of solid species\\nand liquid substrates. Extensions to Sequencing Batch Reactors, in which the main complication is re-\\nlated to a moving-boundary problem, can be found in [10, 11, 13], and comparisons with experimental\\ndata are presented in [12].\\n∗Corresponding author\\n†Departamento de Matem´ atica, Universidad del B´ ıo B´ ıo, Chile, email: jcareaga@ubiobio.cl .\\n‡Departamento de Matem´ atica, Universidad Cat´ olica del Maule, Chile, email: vosores@ucm.cl .\\n1arXiv:2403.06991v1  [math.NA]  26 Feb 2024' metadata={'source': './data/2403.06991.pdf', 'page': 0}\n",
            "page_content='In terms of advancements in PDE-based models of mono-dispersed and non-reactive sedimentation,\\nincluding the development of suitable numerical schemes, a variety of works in different space dimen-\\nsions can be found in the literature. One-dimensional approaches for batch and continuous sedimen-\\ntation such as [15, 16, 19, 27, 28, 30, 47] are mainly approximated by finite differences or finite volume\\nschemes, with focus on non-linear flux approximations. The special case of quasi-one-dimensional ap-\\nproaches [1, 6, 7, 14, 23], treat the container geometry as a coefficient function. Already for two space\\ndimensions, most of the models need to be supplemented with momentum balances, increasing the\\nnumber of equations to solve and adding a substantial degree of difficulty. Different variants appear\\ndepending on the velocity field considered: volume-average velocity [20, 21], mass-average velocity [44],\\nor solid-phase velocity [22, 33]. In [20], a multiresolution finite volume scheme is used to approximate\\nthe two-dimensional model, while a combination of finite volumes and mixed finite element methods\\nis employed in [22]. A sophisticated finite volume element method was tailored in [21] to approximate\\ntheir developed axisymmetric equations. For suspensions composed by solid species with different\\ndensities, an adequate description of the process comes from a polydisperse sedimentation description.\\nIn this line, the work in [18] addresses a multilayer Saint-Venant (MSV) polydisperse model in three\\ndimensional domains having a free-boundary. This dynamic multilayer approach combines the theory\\nof mixtures with the ideas presented in [2, 3, 4], so that the final system of equations couples a type\\nof multilayer shallow water equations with the polydisperse sedimentation model developed in [5, 45].\\nThe multilayer method provides then an efficient alternative for numerically solving the transport and\\nflow equations by layers, reducing the problem by one space dimension. An additional gain in the\\nformulation proposed in [18] is that the vertical components of the velocity fields are computed by\\nmeans of a post-processing procedure. The numerical scheme proposed is based on the specialized\\nmethods for hyperbolic equations with non-conservative products [25, 26], which employ the theory\\nintroduced by Dal Maso et al. [42] to properly approximate the non-conservative products.\\nThe aim of this work is to extend the reactive sedimentation model in [8] to the three-dimensional\\npolydisperse case following the line of [18], this is, employing an MSV approach. In contrast to the\\none-dimensional scenario, in which only mass balances are required, we need to take into account mo-\\nmentum equations. The latter contemplates the inclusion of the viscous stress tensor of the mixture\\nin terms of a mass-average velocity. Hence, we derive a polydisperse multidimensional reactive sedi-\\nmentation model, that couples convection-diffusion-reaction transport equations with flow equations.\\nIn turn, the MSV approach, which requires the assumption of shallow water, is employed in order\\nto reduce the complexity that three-dimensional evolutionary PDEs mean. With this, we aspire to\\ngive a full description of the governing equations, making distinction between the vertical coordinate\\n(pointing in the opposite direction to gravity) and the horizontal ones, so that the final equations are\\nwritten in layers that subdivide the vertical axis. Moreover, we seek to develop a numerical scheme\\nfor the approximation of the model equations combining the Harten–Lax–van Leer (HLL)-type path-\\nconservative numerical method from [24] with upwind flux approximations for the substrates.\\n1.1 Outline of the paper\\nThe paper is organized as follows. In Section 2, we introduce the continuity equations for the solids\\nand fluid phases, and momentum balance for the mixture. The governing PDEs for three-dimensional' metadata={'source': './data/2403.06991.pdf', 'page': 1}\n",
            "page_content='The paper is organized as follows. In Section 2, we introduce the continuity equations for the solids\\nand fluid phases, and momentum balance for the mixture. The governing PDEs for three-dimensional\\ndomains are written as a coupled system in terms of the solids and substrates concentrations, concen-\\ntration of the mixture, mass-average velocity and pressure. In Section 3, we develop the multilayer or\\nshallow water version of the governing equations. We start in Section 3.1 introducing notation and\\nspecifications related to the layers of our multilayer approach. Then, in Section 3.2, we formulate\\nthe multilayer form of the governing equations, including a description of the normal mass fluxes.\\nIn Section 4, we propose a finite volume numerical scheme to approximate the multilayer version of\\nour model. Numerical simulations of the denitrification of activated sludge process are presented in\\n2' metadata={'source': './data/2403.06991.pdf', 'page': 1}\n",
            "page_content='Section 5. Finally, concluding remarks are summarized in Section 6.\\n2 Model derivation\\nWe begin by considering a bounded domain Ω ⊂R3andx:= (x1, x2, z) the vector of coordinates of\\na point in Ω, where ex= (x1, x2) corresponds to its horizontal coordinates, and Ω T:= Ω×(0, T] for\\nT >0. The vertical direction, denoted by the z-coordinate, is assumed to be pointing in the opposite\\ndirection to gravity, and its unit vector is denoted by k= (0,0,1) or simply k= (0,1) with 0= (0,0).\\nFor ease in notation when describing the multiple particulate components, substrates and layers, we\\ndefine for each k∈Nthe following sets on integers\\nI(k) :={1,2, . . . , k }andI0(k) :={0,1, . . . , k }.\\nWe consider nc∈Nspecies of spherical solid particles dispersed in a viscous fluid whose vector of\\nvolume fractions is ϕ:=\\x00\\nϕ(1), ϕ(2), . . . , ϕ(nc)\\x01t∈Rnc, where the i-th phase volume fraction is denoted\\nbyϕ(i):=ϕ(i)(x, t) for all ( x, t)∈ΩT. Furthermore, we assume that each solid phase i∈ I(nc) has\\ndensity ϱi>0 and particle diameter di>0, such that d1≥d2≥ ··· ≥ dN. The volume fraction and\\ndensity of the fluid phase is defined by ϕf:=ϕf(x, t) and ϱf(constant) for all ( x, t)∈ΩT, respectively.\\nThe velocity field of each solid phase i∈ I(nc) is denoted by\\nvi:=vi(x, t) := ( ui(x, t), vi(x, t), wi(x, t))∈R3,∀(x, t)∈ΩT,\\nwhere evi:= (ui, vi)∈R2corresponds to its horizontal velocity and wiis referred as the vertical com-\\nponent. Without loss of generality, we will also use the compact notation vi= (evi, wi) to differentiate\\nbetween the horizontal and vertical components of the velocity fields. We denote the fluid velocity by\\nvf:=vf(x, t) and define the vector of solid concentrations by c:=\\x00\\nc(1), c(2), . . . , c(nc)\\x01t∈Rncsuch\\nthat c(i)(x, t) :=ϱiϕ(i)(x, t) for all i∈ I(nc) and ( x, t)∈ΩT. The total solids volume fraction and\\ntotal concentration (including the fluid phase) at each point ( x, t)∈ΩTare given by\\nϕ:=ϕ(ϕ) :=ncX\\ni=1ϕ(i)=ncX\\ni=1c(i)/ρi, ρ :=ρ(Φ(x, t)) =ϱfϕf(x, t) +ncX\\ni=1ϱiϕ(i)(x, t),\\nrespectively, where Φ:=\\x00\\nϕf, ϕ(1), . . . , ϕ(nc)\\x01tis the vector of volume fractions including the one of the\\nfluid phase. Using the constitutive assumption ϕf+ϕ= 1, we can express ϕfin terms of cas follows\\nϕf(c) = 1−ncX\\ni=11\\nϱic(i),\\nso that we can write Φin terms of c, therefore Φ=Φ(c), the same, therefore, can be done to write ϕ\\nandρin terms of c. In turn, we assume that the fluid phase is composed by ns∈Nsoluble substrates\\ndiluted in water, and define the vector of substrate concentrations by s:=\\x00\\ns(1), s(2), . . . , s(ns)\\x01t∈Rns,\\nwhere each component is a function of ( x, t)∈ΩT.\\nTaking into account that the solid phase may react with the fluid phase (substrates), each solid\\nspecie satisfies the continuity equation or mass conservation\\n∂c(i)\\n∂t+ div\\x00\\nc(i)vi\\x01\\n=R(i)\\nc(c,s)∀(x, t)∈ΩT,∀i∈ I(nc), (1)\\nwhere R(i)\\nc:=R(i)\\nc(c,s) is a (nonlinear) function involving the kinetic reactions between solid and\\nfluid components. We also define the total solid reactions as eRc:=R(1)\\nc+R(2)\\nc+···+R(nc)\\nc. The\\n3' metadata={'source': './data/2403.06991.pdf', 'page': 2}\n",
            "page_content='fluid phase, on the other hand, is composed by ns∈Nsubstrates and water, whose concentration is\\ndenoted by sw:=sw(x, t), so that the following equation holds\\nϱfϕf=s(1)+s(2)+···+s(ns)+sw.\\nSimilarly to (1), the fluid phase fulfills the mass conservation equation\\nϱf∂ϕf\\n∂t+ div( ϱfϕfvf) =eRs(c,s)∀(x, t)∈ΩT,∀i∈ I(ns), (2)\\nwhere the function eRs:=eRs(c,s) corresponds to the sum of functions describing the reactions of\\neach substrate component, defined by R(i)\\ns:=Rs(c,s) for i∈ I(ns), therefore\\neRs:=R(1)\\ns+R(2)\\ns+···+R(ns)\\ns.\\nIn addition, assuming that the substrate components move with the same velocity vfas the entire\\nfluid phase, and that the water does not react, we can establish from (2) the continuity equation for\\neach substrate component and water concentration\\n∂s(i)\\n∂t+ div( s(i)vf) =R(i)\\ns(c,s)∀(x, t)∈ΩT,∀i∈ I(ns), (3a)\\n∂sw\\n∂t+ div ( swvf) = 0 ∀(x, t)∈ΩT, (3b)\\ns(1)+···+s(ns)+sw=ϱfϕf ∀(x, t)∈ΩT.\\nWe observe that Equation (3b), which determines sw, does not need to be solved since this variable\\ncan be recovered directly using the third equation as sw=ϱfϕf(c)−\\x00\\ns(1)+···+s(ns)\\x01\\n.\\nIn what follows, we are going to introduce constitutive assumptions over the velocity vectors in\\nequations (1) and (3a) such that we can reformulate them in terms of the slip velocities ui:=vi−vf\\nfori∈ I(nc) and the so-called mass-average velocity, defined by\\nv:=1\\nρ(c)\\x12\\nϱfϕfvf+nsX\\nj=1c(j)vj\\x13\\n.\\nStraightforwardly from the definition of the slip and mass-average velocities, we can obtain the fol-\\nlowing expressions for the velocity of the solid and fluid phase\\nvi=v+ui−1\\nρ(c)ncX\\nj=1c(j)uj∀i∈ I(nc), (4)\\nvf=v−1\\nϱfϕfncX\\ni=1c(i)\\x12\\nui−1\\nρ(c)ncX\\nj=1c(j)uj\\x13\\n. (5)\\nNext, introducing the constitutive assumption given in [18, Equation 2.9], we can write each slip\\nvelocity as a function of the vector of solid concentrations and its gradient, i.e., ui=ui(c,∇c) for\\ni∈ I(nc), so that we have\\nc(i)\\x12\\nui−1\\nρ(c)ncX\\nj=1c(j)uj\\x13\\n=fi(c)k−Ai(c,∇c)∀i∈ I(nc), (6)\\nwhere fi(c) :=c(i)vMLB\\ni(c) is the batch flux function of the solid component i∈ I(nc), with vMLB\\ni(c)\\nthei-th Masliyah–Lockett–Bassoon (MLB) settling velocity function [41, 40], and Ai:=Ai(c,∇c) is\\n4' metadata={'source': './data/2403.06991.pdf', 'page': 3}\n",
            "page_content='the compression term corresponding to the i-th solid specie. For each solid component i∈ I(nc), the\\nMLB settling velocity and compression functions are given by\\nvMLB\\ni(c) :=−gd2\\n1\\n18µfvhs(ϕ(c))ϑi(c),\\nAi(c,∇c) :=d2\\n1\\n18µfvhs(ϕ(c))(\\n(1−ϕ(c))c(i)\\nϕ(c)\\x00\\nδi−δTc/ρ(c)\\x01\\n∇σe(ϕ(c))\\n+σe(ϕ(c))\"\\nδi∇ \\nc(i)\\nϕ(c)!\\n−c(i)NX\\nj=1δj\\nρ(c)∇ \\nc(j)\\nϕ(c)!#)\\n,\\nwhere µfis the viscosity of the pure fluid, vhs:=vhs(ϕ) is the hindered settling velocity, σe:=σe(ϕ)≥0\\nis the effective solid stress function with compact support contained in [0 , cmax]. For each i∈ I(nc),\\nthe function ϑiis given by\\nϑi(c) :=δi\\x00\\nϱi+ϕf(c)ϱf−ctot\\x01\\n−ncX\\nk=1c(k)\\nρ(c)δk\\x00\\nϱk+ϕf(c)ϱf−ctot\\x01\\n,\\nwith ctot:=c(1)+c(2)+···+c(nc). Then, replacing the relation for the slip velocities (6) into the\\nexpressions for the solid and fluid velocities, (4) and (5), respectively, we can finally write the fluxes\\ncorresponding to (1) and (3a) in terms of v,cand∇c, as follows\\nc(i)vi=c(i)v+fi(c)k−Ai(c,∇c) ∀i∈ I(nc),\\ns(l)vf=s(l)v−s(l)\\nϱfϕf(c)ncX\\nj=1\\x10\\nfj(c)k−Aj(c,∇c)\\x11\\n∀l∈ I(ns).\\nFollowing the dimensional analysis performed in [18], for all i∈ I(nc), the horizontal components of\\nAiare neglected, so that we can replace this vector by a(i)(c, ∂zc)k, where vector a= (a(i))i∈I(nc),\\nwhich collects the new compression functions of each solid specie is defined as\\na:=a(c, ∂zc) :=D(c)∂c\\n∂z∈Rnc,\\nand the matrix D(c)∈Rnc×ncis determined by the coefficients\\n[D(c)]i,l:=d2\\n1vhs(ϕ(c))\\n18µfϱlϕ(c) \\n\\x00\\n1−ϕ(c)\\x01c(l)\\nϱlρ(c)\\x00\\nδlρ(c)−δtc\\x01\\nσ′\\ne(ϕ(c))\\n+σe(ϕ(c))\\x12\\nδlˆδil−c(l)ϱiδi\\nϱlρ(c)−c(l)\\nϱlϕ(c)ρ(c)\\x00\\nδlρ(c)−δtc\\x01\\x13!\\n,∀i, l∈ I(nc),(7)\\nwithδ= (δ1, δ2, . . . , δ nc)t, and ˆδildefined as the Kronecker delta, which equals to 1 if i=land zero\\notherwise. The total mass conservation of the mixture is obtained by adding up each component of\\nequations (1) and (3a), and this is given by\\n∂ρ\\n∂t+ div\\x00\\nρv\\x01\\n=eRρ(c,s)∀(x, t)∈ΩT,\\nwhere eRρ(c,s) :=eRc(c,s) +eRs(c,s). To determine the mass-average velocity v, we introduce the\\nmomentum balance equation of the mixture\\n∂\\n∂t(ρv) +div\\x00\\nρv⊗v−µ(ϕ)e(v)\\x01\\n+∇p=−ρgk∀(x, t)∈ΩT,\\n5' metadata={'source': './data/2403.06991.pdf', 'page': 4}\n",
            "page_content='where pis the pressure, µ:=µ(ϕ) is the viscosity of the mixture, and e(v) :=1\\n2\\x00\\n∇v+ (∇v)t\\x01\\nis the\\nsymmetric part of the gradient. Then, the final system of equations is the following: Find ( c,s,(v, p))\\nsuch that for each x∈Ω and t >0\\n∂ρ\\n∂t+ div\\x00\\nρv\\x01\\n=eRρ(c,s), (8a)\\n∂\\n∂t(ρv) +div\\x00\\nρv⊗v−µ(ϕ)e(v)\\x01\\n+∇p=−ρgk, (8b)\\n∂c(i)\\n∂t+ div\\x10\\nc(i)v+\\x00\\nfi(c)−a(i)(c, ∂zc)\\x01\\nk\\x11\\n=R(i)\\nc(c,s) ∀i∈ I(nc), (8c)\\n∂s(l)\\n∂t+ div\\x12\\ns(l)v−s(l)ncX\\nj=1\\x00\\nfj(c)−a(j)(c, ∂zc)\\x01\\nk\\nϱfϕf(c)\\x13\\n=R(l)\\ns(c,s)∀l∈ I(ns). (8d)\\nThis system is supplemented with initial conditions and zero flux boundary conditions. In Section 3,\\nwe are going to introduce the multilayer approach of (8), where an extra unknown corresponding to\\nthe height of each layer is included. Nevertheless, an additional assumption on the pressure pwill\\nallow us maintaining the same number of equations as in system (8).\\nWe end this section by establishing some assumptions on the reaction terms. We assume that there\\nexist constant stoichiometric matrices σc∈Rℓ×ncandσs∈Rℓ×ns, where ℓ >0 is the number of\\nnon-negative reaction rates. Considering that the ℓreaction rates are given by the vector function\\nκ:=κ(c,s)∈Rℓ, the reaction terms are defined as\\nRc(c,s) =σcκ(c,s),Rs(c,s) =σsκ(c,s).\\nIn addition, to guarantee that each component of cdo not exceed cmax:= min {ϱi:i∈ I(nc)}ϕmax,\\nwith ϕmax>0 the maximum volume fraction, we require the reaction terms tending to zero as ctot\\napproaches cmax: There exists an ε >0 such that Rc(c,s) =0for all ctot≥cmax−ε.\\n3 Multilayer formulation\\n3.1 Preliminaries\\nWith the aim of providing a proper free-boundary description, we consider that for each t∈[0, T], the\\ndomain varies with time, this is Ω := Ω( t). Moreover, in order to implement a multilayer approach\\nof (8) we divide the domain Ω( t) along the vertical direction kintoM∈Nlayers. More precisely,\\nfor each α∈ I(M), we consider surfaces z:=zα+1/2(ex, t) with ex∈R2, such that layer αat time t\\ncorresponds to the set comprehended between z=zα−1/2(ex, t) and z=zα+1/2(ex, t), i.e., zlies in the\\ninterval\\nIα(ex, t) :=\\x00\\nzα−1/2(ex, t), zα+1/2(ex, t)\\x01\\n⊂R.\\nThen, the layers and interfaces are defined by the sets\\nΩα(t) :=n\\n(ex, z) : z∈Iα(ex, t),ex∈Π(t)o\\n, ∀t∈(0, T],∀α∈ I(M),\\nΓα+1/2(t) :=n\\n(ex, z) : z=zα+1/2(ex, t),ex∈Π(t)o\\n,∀t∈(0, T],∀α∈ I0(M),\\nwhere Π(t) maps the points in Ω( t) to their projections into the horizontal plane, i.e., ex∈Π(t) if\\nonly if there exists z∈Rsuch that ( ex, z)∈Ω(t). The set of layers {Ωα(t)}α∈I(M)fulfills the following\\n6' metadata={'source': './data/2403.06991.pdf', 'page': 5}\n",
            "page_content='x1x2z\\nz1/2z3/2zM+1/2\\nzM−1/2\\nzM−3/2\\nh1hM−1hMFigure 1: Illustration of the domain Ω and layers. The bottom surface (ground level) corresponds to\\nzB=z1/2and the free-boundary is at zS=zM+1/2.\\nidentities\\nΩ(t) =M[\\nα=1Ωα(t), ∂Ωα(t) = Γ α−1/2(t)∪Γα+1/2(t)∪n\\n(ex, z) :ex∈∂Π(t), z∈Iα(ex, t)o\\n.\\nIn addition, we assume that the Minterfaces are of class C1in time and space. The thickness of layer\\nαis defined by the function hα:=hα(ex, t) :=zα+1/2(ex, t)−zα−1/2(ex, t) which will vary with respect\\nto the horizontal position exand time t, see Figure 1. The top and bottom surfaces are denoted by\\nzB:=z1/2andzS:=zM+1/2, and the height of the fluid is h:=zS−zB, and there hold\\nh=MX\\nα=1hα, z M+1/2=zB+hand zα+1/2=zB+αX\\nj=1hj,∀α=I(M).\\nFurthermore, we assume that the layer thicknesses are small enough to neglect the dependence of the\\nhorizontal velocities and the concentrations on the vertical variable inside each layer. Finally, for each\\nα∈ I(M) and function φ: Ω→R, we define the one-sided limits\\nφ−\\nα+1/2:= (φ|Ωα)|Γα+1/2, φ+\\nα+1/2:= (φ|Ωα+1)|Γα+1/2.\\nIfφis continuous across Γ α+1/2(t), we simply set φα+1/2:=φ|Γα+1/2.\\n3.2 Multilayer model equations\\nIn what follows, we are going to derive a multilayer approach of system (8), so that the balance\\nequations are established on each layer Ω α(t) for all α∈ I(M). In this regard, to properly write every\\nequation in (8) over each α-layer, we need to take into account the flux transmissions at the interfaces\\nΓα+1/2. Before delving deeper, we introduce the divergence and gradient operators in the horizontal\\ncoordinates as div xand∇x, respectively, this is\\ndivx:=∂\\n∂x1+∂\\n∂x2,∇x:= (∂x1, ∂x2)t,\\nand define divxas the row-wise tensorial version of the divergence div x. For all α∈ I(M), we\\nemploy the subscript αto denote the restriction of each function to the layer α, i.e., cα=c|Ωα(t),\\n7' metadata={'source': './data/2403.06991.pdf', 'page': 6}\n",
            "page_content='sα=s|Ωα(t)andvα=v|Ωα(t), and similarly with the rest of scalar, vector and tensor variables. In\\naddition, we split the volume-average velocity at the α-layer into its horizontal component evαand\\nvertical velocity wα, this is vα:= (evα, wα). Among the assumptions required by this approach, we\\nassume the layer thicknesses are small enough to neglect the dependence of the horizontal velocities,\\nand the concentration of each species on the vertical coordinate within each layer. This means that\\n∂zevα=0and ∂zcα=∂zsα=0∀α∈ I(M).\\nIn addition, we assume that the vertical volume-average velocity wαis piecewise linear in z, and\\npossibly discontinuous. Under this assumption the vertical velocity and (hydrostatic) pressure are\\npiecewise linear in z, i.e.,\\n∂zwα= (∂zwα)(x, t) and ∂zpα= (∂zpα)(x, t)∀α∈ I(M).\\nMore precisely, the assumption of a hydrostatic pressure means that\\npα(x, z, t) =pα+1/2(x, t) +ραg(zα+1/2−z), p α+1/2(x, t) :=pS(x, t) +gMX\\nβ=α+1ρβhβ(x, t),\\nwith pSthe pressure at the free surface.\\nWe begin by describing the multilayer version of equation (8a) for the total density of the mixture\\nραat the layer α∈ I(M). Integrating (8a) with respect to the vertical coordinate over the interval\\nIα, we obtain\\n∂t(ραhα) + div x(ραhαevα) =G−\\nα+1/2− G+\\nα−1/2+hα\\x00eRc(cα,sα) +eRs(cα,sα)\\x01\\n, (9)\\nwhere the normal mass fluxes across the interfaces, defined by Gα+1/2, are assumed to satisfy the\\ncontinuity condition G−\\nα+1/2=G+\\nα+1/2=Gα+1/2for each α∈ I(M), with\\nGα+1/2:=ρα\\x00\\n∂tzα+1/2+evt\\nα· ∇xzα+1/2−wα\\x01\\n. (10)\\nAn explicit expression for this flux without including wαwill be provided later on in this section.\\nSimilarly, the multilayer version of equation (8c), which gives the vector of concentrations of solid\\nspecies cαat the layer α∈ I(M), is obtained by integration on the vertical axis\\n∂t(c(i)\\nαhα) + div x(c(i)\\nαhαevα) =G(i)\\nα+1/2− G(i)\\nα−1/2+hαR(i)\\nc(cα,sα),∀i∈ I(nc), (11)\\nwhere the normal mass fluxes of each i-th solid specie, which are denoted by G(i)\\nα+1/2, are defined in\\nthe same way as in [17], such that they are continuous across the interface Γ α+1/2and\\nncX\\ni=1G(i)\\nα+1/2=Gα+1/2.\\nThen, the following relation between the normal mass fluxes is derived\\nG(i)\\nα+1/2= ˜c(i)\\nα+1/2Gα+1/2−˜f(i)\\nα+1/2+ ˜a(i)\\nα+1/2∀i∈ I(nc),∀α∈ I0(M), (12)\\nwhere the averages ˜ c(i)\\nα+1/2,˜f(i)\\nα+1/2, and ˜ a(i)\\nα+1/2are defined by\\n˜c(i)\\nα+1/2:=1\\n2 \\nc(i)\\nα+1\\nρα+1+c(i)\\nα\\nρα!\\n,˜f(i)\\nα+1/2:=1\\n2\\x10\\nf(i),+\\nα+1/2+f(i),−\\nα+1/2\\x11\\n,˜a(i)\\nα+1/2:=1\\n2\\x10\\na(i),+\\nα+1/2+a(i),−\\nα+1/2\\x11\\n.\\n8' metadata={'source': './data/2403.06991.pdf', 'page': 7}\n",
            "page_content='Employing vector notation, we define the vector of normal mass fluxes for the solid components as\\nGα+1/2:=\\x00\\nG(i)\\nα+1/2\\x01\\ni∈I(nc)and the vectors of the average variables as\\n˜cα+1/2:=\\x00\\n˜c(i)\\nα+1/2\\x01\\ni∈I(nc), ˜fα+1/2:=\\x00˜f(i)\\nα+1/2\\x01\\ni∈I(nc), ˜aα+1/2:=\\x00\\n˜a(i)\\nα+1/2\\x01\\ni∈I(nc).\\nThe multilayer approach of equation (8d) given for the vector of substrate concentrations sαat each\\nlayer α∈ I(M), is obtained performing similar calculations as before, this is\\n∂t(s(l)\\nαhα) + div x(s(l)\\nαhαevα) =S(l)\\nα+1/2− S(l)\\nα−1/2+hαR(l)\\ns(cα,sα)∀l∈ I(ns), (13)\\nwhere the normal mass flux corresponding to the l-substrate is denoted by S(l)\\nα+1/2, which is defined as\\nS(l)\\nα+1/2:= ˜s(l)\\nα+1/2Gα+1/2+1\\n2ϱf\\x12s(l)\\nα\\nϕf,α+s(l)\\nα+1\\nϕf,α+1\\x13ncX\\nj=1\\x00˜f(j)\\nα+1/2−˜a(j)\\n3,α+1/2\\x01\\n, (14)\\nwith ˜ s(l)\\nα+1/2:=1\\n2\\x00\\ns(l)\\nα/ρα+s(l)\\nα+1/ρα+1\\x01\\n. In the same way as we did for the solid phase, we introduce\\nthe vectors ˜sα+1/2:=\\x00\\n˜s(l)\\nα+1/2\\x01\\nl∈I(ns)andSα+1/2:=\\x00\\nS(i)\\nα+1/2\\x01\\ni∈I(ns). For the multilayer version of the\\nmomentum equation (8b), we observe that such a formulation can be obtained in the same way as in\\n[17], so that we skip its derivation and directly introduce it as follows\\n∂t(hαραevα) +∇x·\\x00\\nhαραevα⊗evα\\x01\\n+hα\\x00\\n∇x¯pα+gρα∇x¯zα\\x01\\n=1\\n2Gα+1/2\\x00evα+1−evα\\x01\\n−1\\n2Gα−1/2\\x00evα−evα−1\\x01\\n+Kα+1/2−Kα−1/2,(15)\\nwhere the vector fluxes Kα+1/2forα∈ I0(M), which appears after neglecting terms on the viscous\\nstress tensor, is given by\\nKα+1/2:=µ(ϕα+1/2)\\n2evα+1−evα\\nhα+1/2, (16)\\nwith hα+1/2is the distance between the midpoints of layers αandα+ 1/2. On the other hand ¯ pαand\\n¯zαare defined by (18) and (19), respectively.\\nNext, in order to close the system, we further assume that the thickness of each layer hαis a fixed\\nfraction of the total height h, such that hα=lαhforα∈ I(M), where l1, . . . , l Mare positive numbers\\nthat add up to 1. Then, defining new variables mα:=ραh,qα:=ραhevα,rα:=cαhandζα:=sαhfor\\nα∈ I(M) and replacing them into (9), (15), (11) and (13), we can write the multilayer formulation\\nof (8) as: For all α∈ I, find ( mα,qα,rα,ζα) such that for each ex∈Π(t) and t >0\\n∂tmα+ div x(qα) =1\\nlα\\x00\\nGα+1/2− Gα−1/2\\x01\\n+heRρ,α, (17a)\\n∂tqα+ div x\\x12qα⊗qα\\nmα\\x13\\n+h\\x00\\n∇x¯pα+gρα∇x¯zα\\x01\\n=1\\nlα\\x00\\n˜qα+1/2Gα+1/2−˜qα−1/2Gα−1/2) +1\\nlα\\x00\\nKα+1/2−Kα−1/2\\x01\\n,(17b)\\n∂tr(i)\\nα+ div x\\x12r(i)\\nαqα\\nmα\\x13\\n=1\\nlα\\x00\\nG(i)\\nα+1/2− G(i)\\nα−1/2\\x01\\n+hR(i)\\nc,α ∀i∈ I(nc), (17c)\\n∂tζ(i)\\nα+ div x\\x12ζ(i)\\nαqα\\nmα\\x13\\n=1\\nlα\\x00\\nS(i)\\nα+1/2− S(i)\\nα−1/2\\x01\\n+hR(i)\\ns,α ∀i∈ I(ns), (17d)\\n9' metadata={'source': './data/2403.06991.pdf', 'page': 8}\n",
            "page_content='where ¯ pα, ¯zα, and average ˜qα+1/2are defined by\\n¯pα:=pS+1\\n2glαmα+MX\\nβ=α+1glβmβ, (18)\\n¯zα:=zB+1\\n2lαh+α−1X\\nβ=1lβh , (19)\\n˜qα+1/2:=1\\n2\\x12qα+1\\nmα+1+qα\\nmα\\x13\\n.\\nUsing definitions (18) and (19), together with l1+···+lM= 1 and mα=ραh, the third term on the\\nleft-hand side of (17b), accounting for the pressure gradient, can be written as\\nψα:=h\\x00\\n∇x¯pα+gρα∇x¯zα\\x01\\n=gmα∇x(zB+h) +gh2\\x121\\n2lα+MX\\nβ=α+1lβ\\x13\\n∇xρα\\n+ghMX\\nβ=α+1lβ∇x\\x00\\nmβ−mα\\x01\\n.\\nThe conservation equation for the total mass of the solid phase is obtained by adding up equation\\n(17a) (after multiplying by lαin both sides of the equation) from α= 0 to α=M, this is\\n∂t¯m+ div x MX\\nβ=1lβqβ!\\n=GM+1/2− G1/2+MX\\nβ=1lβheRρ,β, (20)\\nwhere ¯ m:=PM\\nβ=1lβmβis the total mass of the solid phase, and GM+1/2andG1/2represent the mass\\ntransfer on the free surface and at the bottom, respectively. We observe that the mass at each layer\\nand height can be written in terms of ¯ mand the concentration of solid species as follows:\\nmα= ¯m+MX\\nβ=1ncX\\nl=1ϱl−ϱf\\nϱllβ\\x00\\nr(l)\\nα−r(l)\\nβ\\x01\\n,\\nh=1\\nϱf\\x12\\n¯m−MX\\nβ=1ncX\\nl=1ϱl−ϱf\\nϱllβr(l)\\nβ\\x13\\n.\\nFor each α∈ I(M), the normal mass flux Gα+1/2, as it was defined in (10), depends explicitly on the\\nvertical velocities wα, which we are going to be determined on a post processing procedure. To avoid\\nthis dependency, we reformulate these fluxes in the same way as in [18]. First, we define the following\\nvector and scalar variables\\nRα:=qα−ncX\\nj=1r(j)\\nαqα\\nmαϱj−ϱf\\nϱj,¯R:=MX\\nβ=1lβRβ,˜ρα+1/2:=2ραρα+1\\nρα+ρα+1, L α:=αX\\nβ=1hβ,\\nfor all α∈ I(M). Then, following the same calculations as in [18] we obtain\\nGα+1/2=˜ρα+1/2\\nϱfαX\\nβ=1lβdivx(Rβ−¯R) +˜GR\\nα+1/2+˜Gα+1/2∀α∈ I0(M), (21)\\n10' metadata={'source': './data/2403.06991.pdf', 'page': 9}\n",
            "page_content='where ˜GR\\nα+1/2corresponds to the contribution made by the reaction terms, given by\\n˜GR\\nα+1/2:=ncX\\nj=1(ϱj−ϱf)˜ρα+1/2\\nϱjϱf\\x12αX\\nβ=1lβhR(j)\\nc,β−LαMX\\nγ=1lγhR(j)\\nc,γ\\x13\\n−˜ρα+1/2\\nϱf\\x12αX\\nβ=1lβheRm,β−LαMX\\nγ=1lγheRρ,γ\\x13\\n.\\nWe assume that the reaction terms affect only locally on each layer, so that we neglected this vertical\\ncontribution and simply set ˜GR\\nα+1/2≡0 for all α∈ I0(M). The third term in (21), which is related to\\nthe nonlinear hindered-settling fluxes and compression functions, is defined as\\n˜Gα+1/2:=ncX\\nj=1(ϱj−ϱf)˜ρα+1/2\\nϱjϱf\\x12\\n−\\x10\\n˜f(j)\\nα+1/2−˜a(j)\\nα+1/2\\x11\\n+ (1−Lα)\\x10\\n˜f(j)\\n1/2−˜a(j)\\n1/2\\x11\\n+Lα\\x10\\n˜f(j)\\nM+1/2−˜a(j)\\nM+1/2\\x11\\x13\\n+ (1−Lα)˜ρα+1/2\\n˜ρ1/2G1/2+Lα˜ρα+1/2\\n˜ρM+1/2GM+1/2.(22)\\nIn the special case of zero flux boundary conditions at z=zBandz=h, we have G1/2=GM+1/2= 0,\\nand the formula above reduces to\\n˜Gα+1/2:=−ncX\\nj=1(ϱj−ϱf)˜ρα+1/2\\nϱjϱf\\x10\\n˜f(j)\\nα+1/2−˜a(j)\\nα+1/2\\x11\\n.\\nComputation of the vertical velocity . The multilayer formulation presented in this section does not\\ninvolve the vertical velocity of the mixture w. Nevertheless, this variable can be computed by means\\nof a recursive post-procesing procedure, as the one presented in [18]. In this way, the procedure reads\\nas follows. The first step is to recursively calculate the vertical velocities at the interfaces for all\\nα∈ I(M), this is\\nw+\\n1/2=∂tzB+evt\\n1· ∇xzB−G1/2\\nρ1,\\nw−\\nα+1/2=w+\\nα−1/2−hα\\nρα\\x00\\n∂tρα+ div x(ραevα)−eRρ,α\\x01\\n,\\nw+\\nα+1/2=1\\nρα\\x10\\n(ρα−ρα−1)∂tzα−1/2+ (ρα+1evα+1−ραevα)t· ∇xzα−1/2+ρα−1w−\\nα−1/2\\x11\\n.\\nThen, for α∈ I(M) and z∈(zα−1/2, zα+1/2) we compute the vertical velocities by:\\nwα(x, z, t) =w+\\nα−1/2−1\\nρα\\x00\\n∂tρα+ div x(ραevα)−eRρ,α\\x01\\n(z−zα−1/2).\\n4 Numerical scheme\\nIn this section, we describe the finite volume numerical scheme designed to approximate system (17)\\nfor the case of Ω ⊂R3and two-dimensional horizontal domain projections. We employ the numerical\\nscheme described in [17] to our model, which uses the HLL-PVM-1U method developed in [24] com-\\nbined with upwind flux approximations. Besides, the viscous terms in (17b) are computed through a\\nsplitting procedure.\\nIn a Cartesian grid, we consider Ncontrol volumes Vi⊂R2fori∈ I(N), given by squares of size\\n∆x×∆ywith ∆ xand ∆ ypositive, such that T:={Vi}N\\ni=1is a partition of the horizontal domain.\\n11' metadata={'source': './data/2403.06991.pdf', 'page': 10}\n",
            "page_content='We assume that the horizontal domain remains constant, so that the partition Tremains constant\\nover time. In addition, we define ei,jas the edge between two adjacent control volume ViandVj, and\\nηi,jas the unitary normal vector at ei,jpointing from VitoVj. In turn, the set of indexes of control\\nvolumes neighboring Viis defined by Ji. The center of mass of the i-th control volume is denoted\\nbyxiwhile |Vi|and|ei,j|stand for the area of Viand length of ei,j, respectively. Given a function\\nφ, we define { {φ} }i,j:= (φi+φj)/2, the average of φat the edge ei,j, which is defined analogously for\\nvector and tensor variables. For a time span specified by tn=n∆twith ∆ t >0 being a time step and\\nn∈N, we approximate each unknown by its volume average. Therefore, mαis approximated at the\\ni-th control volume Viand time t=tnby\\nmn\\ni,α=1\\n|Vi|Z\\nVimα(·, tn) dx ∀α∈ I(M),\\nand analogously with the rest of unknowns. We start by describing the approximation of the horizontal\\nfluxes of the system, which are related to the horizontal divergence and gradient operators in (17). For\\nthe first two equations (17a) and (17b), given i∈ I(N) and j∈Ji, we approximate their respective\\nhorizontal fluxes on the interface ei,jby\\nFm,n\\ni,j,α=\\x08 \\x08\\nqn\\nα\\t \\tt\\ni,j·ηi,j−1\\n2\\x10\\nθn\\n0,i,j\\x00\\nmn\\nj,α−mn\\ni,α+bn\\ni,j,α\\x01\\n+θn\\n1,i,j(qn\\nj,α−qn\\ni,α)t·ηi,j\\x11\\n,\\nFq,n\\ni,j,α=\\x08 \\x08\\nm−1\\nαqα⊗qα\\t \\t\\ni,jηi,j−1\\n2θn\\n0,i,j\\x00\\nqn\\nj,α−qn\\ni,α\\x01\\n−1\\n2θn\\n1,i,jψn\\ni,j,α\\n−1\\n2θn\\n1,i,j\\x121\\nmn\\nj,αqn\\nj,α⊗qn\\nj,α−1\\nmn\\ni,αqn\\ni,α⊗qn\\ni,α\\x13\\nηi,j,\\nwhere bn\\ni,j,α:={ {ρn\\nα} }i,j(zB,j−zB,i) is included to preserve the well-balance and the stationary solution\\ndeduced in [18, Proposition 1], and ψn\\ni,j,αis approximated by averages and differences as follows\\nψn\\ni,j,α:=g \\n\\x08 \\x08\\nmn\\nα\\t \\t\\ni,j\\x10\\nzB,j−zB,i+hn\\nj−hn\\ni\\x11\\n+\\x08 \\x08\\n(hn)2\\t \\t\\ni,j\\x12lα\\n2+MX\\nβ=α+1lβ\\x13\\x10\\nρn\\nj,α−ρn\\ni,α\\x11\\n+\\x08 \\x08\\nhn\\t \\t\\ni,jMX\\nβ=α+1lβ\\x10\\nmn\\nj,β−mn\\ni,β−(mn\\nj,α−mn\\ni,β)\\x11!\\nηi,j.\\nThe coefficients θn\\n0,i,jandθn\\n1,i,jare respectively defined by\\nθn\\n0,i,j=σn\\nR,i,j|σn\\nL,i,j| −σn\\nL,i,j|σn\\nR,i,j|\\nσn\\nR,i,j−σn\\nL,i,j, θn\\n1,i,j=|σn\\nR,i,j| − |σn\\nL,i,j|\\nσn\\nR,i,j−σn\\nL,i,j,\\nwith σn\\nL,i,jandσn\\nR,i,jbeing the characteristic velocities, which are global approximations of the mini-\\nmum and maximum wave speed of the viscosity matrix, respectively. We make use of the eigenvalues\\nobtained in the one-dimensional case in [17], given by\\nσn\\nL,i,j:= ¯vn\\ni,j−(χΨn\\ni,j)1/2, σn\\nR,i,j:= ¯vn\\ni,j+ (χΨn\\ni,j)1/2,\\nwhere χ= 4−2/Mis a positive constant and\\n¯vn\\ni,j:=1\\nMMX\\nβ=1\\x08 \\x08evn\\nβ\\t \\tt\\ni,jηi,j, Ψn\\ni,j:=MX\\nβ=1(¯vn\\ni,j−vn\\nβ,i,j)2+g\\n2\\x08 \\x08\\nhn\\t \\t\\ni,j\\x12\\n1 +MX\\nβ=1(2β−1)\\nMϱf\\x08 \\x08\\nρn\\nβ\\t \\t\\ni,j\\x13\\n,\\n12' metadata={'source': './data/2403.06991.pdf', 'page': 11}\n",
            "page_content='where vn\\nβ,i,j:={ {vn\\nβ} }t\\ni,j·ηi,j. The horizontal fluxes corresponding to equations (17c) and (17d) are\\nindividually approximated using an upwind type scheme, these are respectively given by\\nFr,n\\ni,j,α:= Upw\\x12\\nFm,n\\ni,j,α;rn\\ni,α\\nmn\\ni,α,rn\\nj,α\\nmn\\nj,α\\x13\\n,Fζ,n\\ni,j,α:= Upw\\x12\\nFm,n\\ni,j,α;ζn\\ni,α\\nmn\\ni,α,ζn\\nj,α\\nmn\\nj,α\\x13\\n,\\nwhere Upw represents the upwind operator, which for a scalar quantity ν, and vectors βandγof the\\nsame length is defined by\\nUpw( ν;β,γ) :=ν\\x121 + sgn( ν)\\n2β+1−sgn(ν)\\n2γ\\x13\\n,\\nwith sgn represents the sign function. In order to approximate the vertical fluxes of system (17), which\\nare determined by (12), (14) and (21), we need to handle the convective and diffusive nonlinear fluxes\\n˜fα+1/2and˜aα+1/2forα∈ I(M), respectively. To do so, we use the following approximations\\n˜f(i),n\\nj,α+1/2:=1\\n2\\x10\\nfi\\x00\\ncn\\nj,α\\x01\\n+fi\\x00\\ncn\\nj,α+1\\x01\\x11\\n−ωj,α+1\\n2\\x00\\nc(i),n\\nj,α+1−c(i),n\\nj,α\\x01\\n−c(i),n\\nj,α\\n2\\x0c\\x0cvMLB\\ni(cn\\nj,α+1)−vMLB\\ni(cn\\nj,α)\\x0c\\x0csgn\\x00\\nc(i),n\\nj,α+1−c(i),n\\nj,α\\x01\\n,fori∈ I(nc),\\n˜an\\nj,α+1/2:=1\\n2hαlα\\x00\\nD(cn\\nj,α) +D(cn\\nj,α+1)\\x01\\n(cn\\nj,α+1−cn\\nj,α),\\nwhere ωα+1:= max i=1,...,n c|vMLB\\ni(cα+1)|, and the matrix function Dis defined by its coefficients in\\n(7). Then, for the case of zero flux boundary conditions at z=handz=zB, the fluxes in (22) and\\n(21) are respectively approximated by\\n˜Gn\\ni,j,α+1/2:=−{ {˜ρn\\nα+1/2} }i,j\\nϱfncX\\nl=1(ϱl−ϱf)\\n2ϱl\\x10\\n˜f(l),n\\ni,α+1/2+˜f(l),n\\nj,α+1/2−(˜a(l),n\\ni,α+1/2+ ˜a(l),n\\nj,α+1/2)\\x11\\n,\\nGn\\ni,j,α+1/2=|ei,j|\\n|Vi|{ {˜ρn\\nα+1/2} }i,j\\nϱfαX\\nβ=1lβ\\x10\\n(Rn\\nj,β−¯Rn\\nj)t−(Rn\\ni,β−¯Rn\\ni)t\\x11\\n·ηi,j+˜Gn\\ni,j,α+1/2.\\nThen, the approximation of the vertical fluxes of equations (17b), (17c) and (17d) for all α∈ I0(M),\\nare respectively given by\\nQn\\ni,α+1/2:=X\\nj∈Ji{ {˜qn\\nα+1/2} }i,jGn\\ni,j,α+1/2,\\nGn\\ni,α+1/2:=X\\nj∈Ji{ {˜cn\\nα+1/2} }i,jGn\\ni,j,α+1/2−\\x10\\n˜fn\\ni,α+1/2−˜an\\ni,α+1/2\\x11\\n,\\nSn\\ni,α+1/2:=X\\nj∈Ji{ {˜sn\\nα+1/2} }i,jGn\\ni,j,α+1/2\\n+ Upw ncX\\nl=1\\x10\\n˜f(l),n\\ni,α+1/2−˜a(l),n\\ni,α+1/2\\x11\\n;sn\\ni,α\\nϱfϕf,i,α,sn\\ni,α+1\\nϱfϕf,i,α+1!\\n.\\nFinally, instead of approximating equation (17a), we determine the total mass ¯ mat each control\\nvolume from (20), therefore the marching formula of the fully discrete finite volume scheme for all\\n13' metadata={'source': './data/2403.06991.pdf', 'page': 12}\n",
            "page_content='Vi∈ T,α∈ I(M) and n∈Nreads as follows\\n¯mn+1\\ni= ¯mn\\ni−∆t\\n|Vi|X\\nj∈JiMX\\nβ=1|ei,j|lβFm,n\\ni,j,β+ ∆tMX\\nβ=1lβhn\\nieRn\\nρ,i,β, (23a)\\nqn+1/2\\ni,α =qn\\ni,α−∆t\\n|Vi|X\\nj∈Ji|ei,j|\\x00\\nFq,n\\ni,j,α+ψn\\ni,j,α\\x01\\n+∆t\\nlα\\x00\\nQn\\ni,α+1/2−Qn\\ni,α−1/2\\x01\\n, (23b)\\nrn+1\\ni,α=rn\\ni,α−∆t\\n|Vi|X\\nj∈Ji|ei,j|Fr,n\\ni,j,α+∆t\\nlα\\x00\\nGn\\ni,α+1/2−Gn\\ni,α−1/2\\x01\\n+ ∆thn\\niRn\\nc,i,α, (23c)\\nζn+1\\ni,α=ζn\\ni,α−∆t\\n|Vi|X\\nj∈Ji|ei,j|Fζ,n\\ni,j,α+∆t\\nlα\\x00\\nSn\\ni,α+1/2−Sn\\ni,α−1/2\\x01\\n+ ∆thn\\niRn\\ns,i,α, (23d)\\nwhere mn+1\\ni,αandhn+1\\niare computed by\\nmn+1\\ni,α= ¯mn+1\\ni+MX\\nβ=1ncX\\nl=1ϱl−ϱf\\nϱllβ\\x00\\nr(l),n+1\\ni,α −r(l),n+1\\ni,β\\x01\\n, (24)\\nhn+1\\ni=1\\nϱf\\x12\\n¯mn+1\\ni−MX\\nβ=1ncX\\nl=1ϱl−ϱf\\nϱllβr(l),n+1\\ni,β\\x13\\n. (25)\\nTo determine the velocity field at time tn+1, we need to include the discretization of the viscous terms\\n(16), this is done by solving the following linearly implicit equation\\nqn+1\\ni,α=qn+1/2\\ni,α +∆t\\nlα\\x10\\nKn+1\\ni,α+1/2−Kn+1\\ni,α−1/2\\x11\\n∀α∈ I(M), (26)\\nwhere\\nKn+1\\ni,α+1/2:=µ(ϕn+1\\ni,α+1/2)\\n2(1\\n2lα+1+1\\n2lα)hn\\ni\\x12qn+1\\ni,α+1\\nmn+1\\ni,α+1−qn+1\\ni,α\\nmn+1\\ni,α\\x13\\n∀α∈ I0(M),\\nwith ϕn\\ni,α+1/2:=1\\n2\\x00\\nϕ(cn\\ni,α+1) +ϕ(cn\\ni,α)\\x01\\n. Note that equation (26) can be written as a linear system\\ncomposed by an invertible tri-diagonal matrix. The numerical scheme is supplemented with the CFL\\ncondition provided in [18], which corresponds to an adaptive time-stepping given by\\nmax\\x1a|λi,j|\\ndi,j:i∈ I(N), j∈Ji\\x1b\\n∆t≤CFL,\\nwhere λi,jare bounds of the eigenvalues of the viscosity matrix, di,j=||xj−xi||2is the distance\\nbetween centers of the volume iandj. For all examples, we have set the Courant number as CFL = 0 .5.\\n5 Numerical simulations\\nWe have implemented the numerical scheme composed by (23), (24), (25) and (26) in the programming\\nlanguage Fortran 90, and the tri-diagonal linear system arising from (26) is computed using the library\\nSuperLU [39]. For the numerical examples, we consider the reduced denitrification process [9], which\\nconsists of converting nitrate into nitrogen gas. The solid phase is composed by two species, the\\nordinary heterotrophic organisms c(1)and undegradable organics c(2), and the substrates are three,\\n14' metadata={'source': './data/2403.06991.pdf', 'page': 13}\n",
            "page_content='the diluted nitrate s(1), the readily biodegradable substrate s(2), and the diluted nitrogen s(3). The\\nvectors of concentration are c=\\x00\\nc(1), c(2)\\x01\\nands=\\x00\\ns(1), s(2), s(3)\\x01\\n, which in [9] are denoted by\\nc=\\x00\\nXOHO, XU\\x01\\nands=\\x00\\nSNO 3, SS, SN2\\x01\\n. The vector of reaction rates and stoichiometric matrices\\nare respectively given by\\nκ(c,s) =c(1)\\x14\\nµ(s(1), s(2))\\nb\\x15\\n,σc=\\x141−1\\n0fp\\x15\\n,σs=\\x14−¯Y−1/Y ¯Y\\n0 0 .8 0\\x15\\n,\\nwhere b= 6.94×10−5s−1is the decay rate of heterotrophic organisms, fp= 0.2 is the portion of these\\nthat decays to undegradable organics, Y= 0.67 and ¯Y= 0.172216 are (dimensionless) yield factors.\\nThe specific growth rate function is defined by\\nµ(s) =µmaxs(1)\\nκ1+s(1)s(2)\\nκ2+s(2),\\nwith µmax= 5.56×10−4s−1being the maximum growth rate, and κ1= 5×10−4kg/m3andκ2=\\n0.02 kg /m3are saturation constants. Note that constants µmaxandbare chosen 10 times larger than\\nthose used in the reduced denitrification model [9, 8], this is in order to amplify the effect produced\\nby the reaction terms in a shorter simulation time. Then, the reaction terms read as follows\\nRc(c,s) =c(1)\"\\nµ(s)−b\\nfpb#\\n,Rs(c,s) =c(1)\\uf8ee\\n\\uf8ef\\uf8f0−¯Y µ(s)\\n−µ(s)/Y+ 0.8b\\n¯Y µ(s)\\uf8f9\\n\\uf8fa\\uf8fb,\\nThe hindered settling, effective solid stress and viscosity functions used in all simulations are provided\\nrespectively as follows\\nvhs(ϕ) :=\\x12\\n1−ϕ\\nϕmax\\x134.7\\n, σ e(ϕ) :=(\\nσ0\\x00\\n(ϕ/ϕ c)5.0−1\\x01\\nifϕ≥ϕc,\\n0 if ϕ < ϕ c,\\nand\\nµ(ϕ) :=µ0\\x12\\n1−ϕ\\n0.95\\x13−2.5\\n,\\nwhere σ0= 0.02 m2/s2,µ0= 0.01 Pa s, and the maximal and critical volume fractions are set to\\nϕmax= 0.02 and ϕc:= 0.003, respectively. Other parameters are ϱ1=ϱ2= 2000 kg /m3,ϱf=\\n998 kg /m3,δ1= 4×10−4,δ2= 2.5×10−4andg= 9.81 m/s2. In all simulations we use the following\\nhomogeneous initial conditions for the substrates\\ns(x,0) = (0 .006,0.0009,0.0)tkg/m3∀x∈Ω,\\nand for all simulations except the second one, we use the solids initial condition\\nc(x,0) = (3 ,2.5)tkg/m3∀x∈Ω.\\nFinally, we implemented zero flux-boundary conditions so that evt\\nα·ηα= 0 on ∂Ω(t), for all α∈ I(M),\\nwhere ηαis the outward-facing normal to the boundary ∂Ωα.\\n5.1 Simulation 1: Inclined walls settling\\nWe consider a closed vessel with an inclined bottom wall, enclosed on a rectangle of 4 m wide ( x1\\ndirection) and 6 m long ( x2direction), such that the projected domain corresponds to Π(0) = (0 ,4)×\\n(0,6). The vessel, shown in Figures 2 to 5, has a bottom wall described by\\nzB(ex) = max\\x1a\\n±6.4125\\n22.8(x1−2) +2.25\\n22.8(x2−6) + 0 ,5625,0\\x1b\\n,ex∈Π(0),\\n15' metadata={'source': './data/2403.06991.pdf', 'page': 14}\n",
            "page_content='Ordinary heterotrophic organisms ( XOHO)\\nc(1)[kg/m3]\\n t= 20 s\\nc(1)[kg/m3]\\n t= 40 s\\nUndegradable organics ( XU)\\nc(2)[kg/m3]\\n t= 20 s\\nc(2)[kg/m3]\\n t= 40 s\\nMass-average velocity\\n∥v∥[m/s]\\n t= 20 s\\n∥v∥[m/s]\\n t= 40 s\\nFigure 2: Simulation 1. Concentration of solid components and magnitude of the mass-average velocity\\nat a middle time t= 20 s (first column), and final time t= 40 s (second column).\\nwhich is initially filled with mixture up to z= 1.5 m, the initial height is h(ex,0) = 1 .5 m, for ex∈Π(0).\\nWe set the mesh sizes ∆ x= 0.1 m and ∆ y= 0.15 m, and M= 40 layers, which results in a total of\\n64000 cells, counting all layers. Figure 2 shows the simulated concentration of solid species c(1)and\\nc(2), and norm of the mass-average velocity at times t= 20 s and t=T= 40 s (the final simulation\\ntime). Both components c(1)andc(2)settle down on the bottom wall, and due to the inclination of\\nthe vessel, the sedimentation continues towards x2= 0 reaching maximum values of concentration\\nnear the center line x1= 2. The latter proofs that the model and numerical scheme properly address\\nthe movements of the particles in all three directions. Moreover, the mass-average velocity, shown in\\nthe third row of Figure 2, assumes its maximum at the bottom layers, remaining almost constant in\\nthe rest of the domain. The above is explained by the fact that a greater amount of solid particles\\nare being concentrated in the first layers, inducing the movement along them. The concentration\\nof substrate components s(1),s(2)ands(3)at both times are presented in Figure 3. For the nitrate\\nsubstrate s(1)(first row, Figure 3), we observe that the maximum concentration is assumed at regions\\nwhere c(1)is higher. However, at time t= 20 s a thin layer of this substrate is still remaining at the\\ntop of the vessel. On the other hand, the readily biodegradable substrate s(2)(second row of Figure 3),\\nat time t= 20 s has increased in the entire domain, and at time t= 40 s exhibit a decreasing behavior\\nnear the top of the wall at x2= 0. Furthermore, the maximum concentration is located near the\\ncorners at x2= 6. The nitrogen substrate s(3)(third row, Figure 3), which is entirely produced by the\\nchemical reactions, is accumulated with greater abundance at the bottom wall towards x2= 0, and it\\nincreases over time.\\n16' metadata={'source': './data/2403.06991.pdf', 'page': 15}\n",
            "page_content='Nitrate substrate ( SNO 3)\\ns(1)[kg/m3]\\n t= 20 s\\ns(1)[kg/m3]\\n t= 40 s\\nReadily biodegradable substrate ( SS)\\ns(2)[kg/m3]\\n t= 20 s\\ns(2)[kg/m3]\\n t= 40 s\\nNitrogen substrate ( SN2)\\ns(3)[kg/m3]\\n t= 20 s\\ns(3)[kg/m3]\\n t= 40 s\\nFigure 3: Simulation 1. Concentration of substrates at a middle time t= 20 s (first column), and final\\ntime t= 40 s (second column).\\n5.2 Simulation 2: Higher compression effect\\nWe consider the same set of parameters and domain as in Simulation 1, but amplify the compression\\neffect by setting σ0= 0.5 m2/s2andϕc= 0.002. We also use a higher initial condition for the solids\\nc(x,0) = (10 ,8)tkg/m3∀x∈Ω.\\nIn Figure 4, we report the simulated concentration of the solid components c(1)andc(2)at times t= 20 s\\nandt= 40 s. As expected, sedimentation evolves at a slower rate, with smoother space variations\\nwithin the vessel. In both components, solid particles are accumulated downward and towards the\\nwall at x2= 0 as in Simulation 1, which shows that higher compression does not prevent horizontal\\nmovements. The velocity field (third row, Figure 4) differs considerably to the one in Simulation 1.\\nAtt= 20 s, two regions of higher velocity appear at the top and bottom of the vessel, and then at\\nt= 40 s, the velocity is maximal in the top layer. The later is explained since the vessel is full of\\nhighly concentrated solid particles and no separation from clear water has occurred yet. Compared to\\nSimulation 1, the three substrates show an increase in their concentrations (Figure 5), where nitrate\\ns(1)presents the greatest variation. The three substrates assume their maximum values near the top\\nof the wall at x2= 6.\\n17' metadata={'source': './data/2403.06991.pdf', 'page': 16}\n",
            "page_content='Ordinary heterotrophic organisms ( XOHO)\\nc(1)[kg/m3]\\n t= 20 s\\nc(1)[kg/m3]\\n t= 40 s\\nUndegradable organics ( XU)\\nc(2)[kg/m3]\\n t= 20 s\\nc(2)[kg/m3]\\n t= 40 s\\nMass-average velocity\\n∥v∥[m/s]\\n t= 20 s\\n∥v∥[m/s]\\n t= 40 s\\nFigure 4: Simulation 2. Concentration of solid components and magnitud of the mass-average velocity\\nat a middle time t= 20 s (first column), and final time t= 40 s (second column).\\n5.3 Simulation 3: Concave bathymetry\\nWe assume now that the sedimentation process is carried out in a domain with concave and smooth\\nbathymetry. For this end, we consider the square projected domain Π(0) = (0 ,1)2and the following\\nbathymetry\\nzB(ex) = 0 .2 exp\\x10\\n−40\\x00\\n(x1−0.5)2+ (x2−0.5)2\\x01\\x11\\n∀ex∈Π(0).\\nFor this simulation we set ∆ x= ∆y= 0.025 m, final simulation time T= 20 s, total number of layers\\nM= 60 s, and initial height h(ex,0) = 0 .4 m. In Figure 6, we present the numerical results of the\\nconcentrations of each solid species and substrates, and velocity field in a domain that is cut off at\\nx1= 0.5 (for 0 ≤x1≤0.5). We observe that the concentration of particles c(1)accumulates at the\\nbottom and towards the side walls, while particles c(2)remain floating at the interface between the\\nsolid and liquid phase. The velocity field assumes its maximum near the top of the curved part of\\nzBand in the region of lower height h, and recirculation of the fluid is observed. Concentration s(1)\\naccumulates at the top near ex= (0,0) and at the bottom-central part of the side walls. Component\\ns(2)is less concentrated at the top of the domain, with the exception of a central region of highly\\nconcentrated substrate near ex= (0,0). Substrate s(3)exhibits behavior more similar to that of c(1)\\nwith a thinner layer of concentrated substrate and maximum towards the bottom of the side walls.\\n18' metadata={'source': './data/2403.06991.pdf', 'page': 17}\n",
            "page_content='Nitrate substrate ( SNO 3)\\ns(1)[kg/m3]\\n t= 20 s\\ns(1)[kg/m3]\\n t= 40 s\\nReadily biodegradable substrate ( SS)\\ns(2)[kg/m3]\\n t= 20 s\\ns(2)[kg/m3]\\n t= 40 s\\nNitrogen substrate ( SN2)\\ns(3)[kg/m3]\\n t= 20 s\\ns(3)[kg/m3]\\n t= 40 s\\nFigure 5: Simulation 2. Concentration of substrates at a middle time t= 20 s (first column), and final\\ntime t= 40 s (second column).\\n6 Conclusions\\nWe have extended the one-dimensional reactive sedimentation model provided in [8] to the three-\\ndimensional case combining the multilayer shallow water approach for polydisperse settling presented\\nin [17, 18]. We consider solid particles featuring varying diameters and densities which settle with\\ndifferent velocities, while the substrates or liquid components move with the same velocity. The\\ngoverning equations are written in terms of concentrations of solid species and substrates, quantities\\nthat are interconnected through nonlinear reaction terms, as well as by their respective velocity fields.\\nMoreover, the model includes nonlinear convective velocities involving the hindered settling function,\\nand the compressibility of the sediment, both acting on the vertical component. For the multilayer\\nversion of the model, the momentum equation which is written in terms of the horizontal components\\nof the mass-average velocity, we have assumed hydrostatic pressure and a reduced version of the viscous\\nstress tensor. One of the advantages of introducing this multilayer approach, is that at each layer,\\nwe only have horizontal space derivatives, while vertical fluxes and reaction terms are considered as\\nsource terms. In addition, the total height of the fluid column allows determining the evolution of the\\nfree surface.\\nRegarding the numerical scheme, we approximate the vertical fluxes in a slightly different way\\nthan in [18], where the approximation of the vertical fluxes corresponding to the substrates depend\\non the numerical fluxes of each solid species. Numerical examples show the good performance of\\n19' metadata={'source': './data/2403.06991.pdf', 'page': 18}\n",
            "page_content='Solid XOHO Substrate SNO 3\\nc(1)[kg/m3]\\ns(1)[kg/m3]\\nSolid XU Substrate SS\\nc(2)[kg/m3]\\ns(2)[kg/m3]\\nVelocity Substrate SN2\\n∥v∥[m/s]\\ns(3)[kg/m3]\\nFigure 6: Simulation 3. Concentration of the solid and substrate components, and velocity field at\\nthe end time t= 20 s on the sub-domain for 0 ≤x1≤0.5. The plot of the velocity field includes the\\narrows of its direction.\\nthe model and numerical scheme in scenarios varying the compression effect and bathymetry for the\\ndenitrification model [15]. We emphasize that our work can be used to simulate more general processes\\ninvolving concentrations of solid particles and substrates, such as the ASM1 [35] or eutrophication of\\nshallow lakes [37, 38, 43, 46].\\nFinally, there remain unresolved mathematical questions concerning the model equations, including\\nthe hyperbolicity of the system and the existence of entropy solutions. Additionally, questions persist\\nregarding the analysis of the numerical scheme, encompassing aspects such as positivity preserving,\\nstability, and convergence analysis. Further studies can be done in order to determine the appropri-\\nateness of neglecting the vertical flux contributions coming from the reaction terms, and in the line\\nof considering alternative numerical schemes such as the discontinuous Galerkin and entropy stable\\nmethod proposed in [48].\\n20' metadata={'source': './data/2403.06991.pdf', 'page': 19}\n",
            "page_content='Acknowledgments\\nJC is supported by ANID-Chile through Fondecyt Postdoctoral project No. 3230553.\\nReferences\\n[1] G. Anestis. Eine eindimensionale Theorie der Sedimentation in Absetzbeh¨ altern ver¨ anderlichen\\nQuerschnitts und in Zentrifugen . PhD thesis, TU Vienna, Austria, 1981.\\n[2] E. Audusse. A multilayer saint-venant model: Derivation and numerical validation. Discrete\\nContinuous Dyn. Syst. Ser. B , 5(2):189–214, 2005.\\n[3] E. Audusse, M.-O. Bristeau, M. Pelanti, and J. Sainte-Marie. Approximation of the hydrostatic\\nnavier–stokes system for density stratified flows by a multilayer model: Kinetic interpretation\\nand numerical solution. J. Comput. Phys. , 230(9):3453–3478, 2011.\\n[4] E. Audusse, M.-O. Bristeau, B. Perthame, and J. Sainte-Marie. A multilayer saint-venant sys-\\ntem with mass exchanges for shallow water flows. derivation and numerical validation. ESAIM:\\nMath. Model. Numer. Anal. , 45(1):169–200, 2010.\\n[5] D.K. Basson, S. Berres, and R. B¨ urger. On models of polydisperse sedimentation with particle-\\nsize-specific hindered-settling factors. Appl. Math. Model. , 33(4):1815–1835, 2009.\\n[6] R. B¨ urger, J. Careaga, and S. Diehl. Entropy solutions of a scalar conservation law modeling\\nsedimentation in vessels with varying cross-sectional area. SIAM J. Appl. Math. , 77(2):789–811,\\n2017.\\n[7] R. B¨ urger, J. Careaga, and S. Diehl. A simulation model for settling tanks with varying cross-\\nsectional area. Chem. Eng. Commun. , 204(11):1270–1281, 2017.\\n[8] R. B¨ urger, J. Careaga, and S. Diehl. A method-of-lines formulation for a model of reactive settling\\nin tanks with varying cross-sectional area. IMA J. Appl. Math. , 86(3):514–546, 2021.\\n[9] R. B¨ urger, J. Careaga, S. Diehl, C. Mej´ ıas, I. Nopens, E. Torfs, and P. A. Vanrolleghem. Simula-\\ntions of reactive settling of activated sludge with a reduced biokinetic model. Computers Chem.\\nEng., 92:216–229, 2016.\\n[10] R. B¨ urger, J. Careaga, S. Diehl, and R. Pineda. A moving-boundary model of reactive settling in\\nwastewater treatment. Part 1: Governing equations. Appl. Math. Modelling , 106:390–401, 2022.\\n[11] R. B¨ urger, J. Careaga, S. Diehl, and R. Pineda. A moving-boundary model of reactive settling\\nin wastewater treatment. Part 2: Numerical scheme. Appl. Math. Modelling , 111:247–269, 2022.\\n[12] R. B¨ urger, J. Careaga, S. Diehl, and R. Pineda. A model of reactive settling of activated sludge:\\nComparison with experimental data. Chem. Eng. Sci. , 267:118244, March 2023.\\n[13] R. B¨ urger, J. Careaga, S. Diehl, and R. Pineda. Numerical schemes for a moving-boundary\\nconvection-diffusion-reaction model of sequencing batch reactors. ESAIM: Math. Model. Nu-\\nmer. Anal. , 57(5):2931–2976, 2023.\\n[14] R. B¨ urger, J. J. R. Damasceno, and K. H. Karlsen. A mathematical model for batch and con-\\ntinuous thickening of flocculated suspensions in vessels with varying cross-section. Int. J. Miner.\\nProcess. , 73:183–208, 2004.\\n21' metadata={'source': './data/2403.06991.pdf', 'page': 20}\n",
            "page_content='[15] R. B¨ urger, S. Diehl, S. Far˚ as, and I. Nopens. On reliable and unreliable numerical methods\\nfor the simulation of secondary settling tanks in wastewater treatment. Computers Chem. Eng. ,\\n41:93–105, 2012.\\n[16] R. B¨ urger, S. Evje, K. H. Karlsen, and K.-A. Lie. Numerical methods for the simulation of the\\nsettling of flocculated suspensions. Chem. Eng. J. , 80:91–104, 2000.\\n[17] R. B¨ urger, E.D. Fern´ andez-Nieto, and V. Osores. A dynamic multilayer shallow water model for\\npolydisperse sedimentation. ESAIM: Math. Model. Numer. Anal. , 53(4):1391–1432, 2019.\\n[18] R. B¨ urger, E.D. Fern´ andez-Nieto, and V. Osores. A multilayer shallow water approach for poly-\\ndisperse sedimentation with sediment compressibility and mixture viscosity. J. Sci. Comput. ,\\n85(2), 2020.\\n[19] R. B¨ urger, K. H. Karlsen, and J. D. Towers. A model of continuous sedimentation of flocculated\\nsuspensions in clarifier-thickener units. SIAM J. Appl. Math. , 65:882–940, 2005.\\n[20] R. B¨ urger, R. Ruiz-Baier, K. Schneider, and H. Torres. A multiresolution method for the simu-\\nlation of sedimentation in inclined channels. Int. J. Numer. Anal. Model. , 9(3):479–504, 2012.\\n[21] R. B¨ urger, R. Ruiz-Baier, and H. Torres. A stabilized finite volume element formulation for\\nsedimentation-consolidation processes. SIAM J. Sci. Comput. , 34:B265–B289, 2012.\\n[22] J. Careaga and G.N. Gatica. Coupled mixed finite element and finite volume methods for a solid\\nvelocity-based model of multidimensional sedimentation. ESAIM: Math. Model. Numer. Anal. ,\\n57:2529–2556, 2023.\\n[23] J.-Ph. Chancelier, M. Cohen de Lara, and F. Pacard. Analysis of a conservation PDE with\\ndiscontinuous flux: a model of settler. SIAM J. Appl. Math. , 54(4):954–995, 1994.\\n[24] M.J. Castro D´ ıaz and E. Fern´ andez-Nieto. A class of computationally fast first order finite volume\\nsolvers: PVM methods. SIAM Journal on Scientific Computing , 34(4):A2173–A2196, 2012.\\n[25] M.J. Castro D´ ıaz, E.D. Fern´ andez-Nieto, T. Morales de Luna, G. Narbona-Reina, and C. Par´ es.\\nA HLLC scheme for nonconservative hyperbolic problems. application to turbidity currents with\\nsediment transport. ESAIM: Math. Model. Numer. Anal. , 47(1):1–32, 2012.\\n[26] M.J. Castro D´ ıaz, E.D. Fern´ andez-Nieto, A.M. Ferreiro, and C. Par´ es. Two-dimensional sed-\\niment transport models in shallow water equations. a second order finite volume approach on\\nunstructured meshes. Comput. Methods Appl. Mech. Eng. , 198(33-36):2520–2538, 2009.\\n[27] S. Diehl. Dynamic and steady-state behavior of continuous sedimentation. SIAM J. Appl. Math. ,\\n57(4):991–1018, 1997.\\n[28] S. Diehl. The solids-flux theory – confirmation and extension by using partial differential equa-\\ntions. Water Res. , 42(20):4976–4988, 2008.\\n[29] R. Dupont and M. Henze. Modelling of the secondary clarifier combined with the activated sludge\\nmodel no. 1. Water Sci. Tech. , 25(6):285–300, 1992.\\n[30] G.A. Ekama, J.L. Barnard, F.W. G¨ unthert, P. Krebs, J.A. McCorquodale, D.S. Parker, and E.J.\\nWahlberg. Secondary Settling Tanks: Theory, Modelling, Design and Operation . IAWQ scientific\\nand technical report no. 6. International Association on Water Quality, England, 1997.\\n[31] X. Flores-Alsina, I. Rodriguez-Roda, G. Sin, and K.V. Gernaey. Multi-criteria evaluation of\\nwastewater treatment plant control strategies under uncertainty. Water Res. , 42(17):4485–4497,\\n2008.\\n22' metadata={'source': './data/2403.06991.pdf', 'page': 21}\n",
            "page_content='[32] K.V. Gernaey, M.C.M. van Loosdrecht, M. Henze, M. Lind, and S.B.Jørgensen. Activated sludge\\nwastewater treatment plant modelling and simulation: state of the art. Environ. Model. Software ,\\n19(9):763–783, 2004.\\n[33] K. Gustavsson and J. Oppelstrup. Consolidation of concentrated suspensions – numerical simu-\\nlations using a two-phase fluid model. Comput. Visual. Sci. , 3(1–2):39–45, 2000.\\n[34] M. Henze, C.P.L. Grady, W. Gujer, G.V.R. Marais, and T. Matsuo. A general model for single-\\nsludge wastewater treatment systems. Water Res. , 21(5):505–515, 1987.\\n[35] M. Henze, W. Gujer, T. Mino, and M.C.M. van Loosdrecht. Activated Sludge Models ASM1,\\nASM2, ASM2d and ASM3 . IWA Publishing, London, UK, 2000. IWA Scientific and Technical\\nReport No. 9.\\n[36] Z.R. Hu, M.C. Wentzel, and G.A. Ekama. Modelling biological nutrient removal activated sludge\\nsystems – a review. Water Res. , 37(14):3430–3444, 2003.\\n[37] M. Jayaweera and T. Asaeda. Modeling of biomanipulation in shallow, eutrophic lakes: An\\napplication to lake bleiswijkse zoom, the netherlands. Ecol. Model. , 85(2–3):113–127, 1996.\\n[38] S.E. Jørgensen. Modelling Eutrophication of Shallow Lakes , page 177–188. Elsevier, 1988.\\n[39] Xiaoye S. Li. An overview of SuperLU: Algorithms, implementation, and user interface. ACM\\nTransactions on Mathematical Software , 31(3):302–325, 2005.\\n[40] M.J. Lockett and K.S. Bassoon. Sedimentation of binary particle mixtures. Powder Technology ,\\n24(1):1–7, 1979.\\n[41] J.H. Masliyah. Hindered settling in a multi-species particle system. Chemical Engineering Science ,\\n34(9):1166–1168, 1979.\\n[42] G. Dal Maso, P. Le Floch, and F. Murat. Definition and weak stability of nonconservative\\nproducts. J. Math. Pures Appl. , 74:483–548, 1995.\\n[43] J. Pauer. Nitrification in the water column and sediment of a hypereutrophic lake and adjoining\\nriver system. Water Res. , 34(4):1247–1254, 2000.\\n[44] R. Rao, L. Mondy, A. Sun, and S. Altobelli. A numerical and experimental study of batch\\nsedimentation and viscous resuspension. Int. J. Numer. Methods Fluids. , 39(6):465–483, 2002.\\n[45] E.M. Tory, K.H. Karlsen, R. B¨ urger, and S. Berres. Strongly degenerate parabolic-hyperbolic\\nsystems modeling polydisperse sedimentation with compression. SIAM J. Appl. Math. , 64(1):41–\\n80, 2003.\\n[46] D.T. van der Molen, F.J. Los, L. van Ballegooijen, and M.P. van der Vat. Mathematical\\nmodelling as a tool for management in eutrophication control of shallow lakes. Hydrobiologia ,\\n275–276(1):479–492, 1994.\\n[47] R.W. Watts, S.A. Svoronos, and B. Koopman. One-dimensional modeling of secondary clarifiers\\nusing a concentration and feed velocity-dependent dispersion coefficient. Water Res. , 30(9):2112–\\n2124, 1996.\\n[48] N. Wintermeyer, A.R. Winters, G.J. Gassner, and D.A. Kopriva. An entropy stable nodal dis-\\ncontinuous galerkin method for the two dimensional shallow water equations on unstructured\\ncurvilinear meshes with discontinuous bathymetry. J.Comput. Phys. , 340:200–242, 2017.\\n23' metadata={'source': './data/2403.06991.pdf', 'page': 22}\n",
            "Processing  ./data/2403.06988.pdf\n",
            "[[-0.02716577 -0.02484674  0.01183671 ... -0.01191953 -0.00112414\n",
            "   0.01370022]]\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nLuca Beurer-Kellner1Marc Fischer1Martin Vechev1\\nAbstract\\nTo ensure that text generated by large language\\nmodels (LLMs) is in an expected format, con-\\nstrained decoding proposes to enforce strict for-\\nmal language constraints during generation. How-\\never, as we show in this work, not only do such\\nmethods incur performance overhead during gen-\\neration, but many of them also significantly impair\\ntask accuracy, if they do not correctly align the\\nunderlying LLM sub-word vocabularies with ex-\\nternal constraints. To address this, we present a\\nnovel decoding algorithm, DOMINO, that can en-\\nforce constraints in a fully subword-aligned fash-\\nion, while leveraging pre-computation and specu-\\nlative decoding to achieve virtually no overhead\\nand in some cases even almost 2 ×speedup over\\nunconstrained decoding – thereby outperforming\\nexisting approaches by a wide margin.\\n1. Introduction\\nThe recent success of Large Language Models (LLMs)\\n(Brown et al., 2020; Chen et al., 2021; OpenAI, 2023; Tou-\\nvron et al., 2023a;b; Anil et al., 2023; Jiang et al., 2024) has\\nlead to the development of various methods that facilitate\\nconstrained generation, a method that lets users tailor the\\noutput of an LLM to a specific task or format.\\nConstrained Decoding To ensure that text generated by\\nan LLM adheres to syntactic constraints, these methods\\nrestrict the decoding procedure of an LLM in a way that\\nonly permits syntactically valid tokens at each sampling step.\\nDoing so, the generated text can be ensured to adhere to\\nconstraints like high-level templates (Beurer-Kellner et al.,\\n2023; Lundberg et al.), regular expressions (Beurer-Kellner\\net al., 2023; Lundberg et al.; Willard & Louf, 2023) or\\ncontext-free grammars (Willard & Louf, 2023; Lundberg\\net al.). Constrained decoding provides numerous upsides.\\n*Equal contribution1Department of Computer Science,\\nETH Zurich, Switzerland. Correspondence to: Luca\\nBeurer-Kellner < luca.beurer-kellner@inf.ethz.ch>, Marc Fischer\\n<marc.fischer@inf.ethz.ch>.Prompt: A person encoded as JSON object:\\nUnconstrained Decoding:\\n{\\\\n↓\\n····\"name\":·\"John·Doe\",\\\\n\\n····\"age\":·32,\\\\n\\n····\"gender\":·\"male\",. . .\\nValid Tokens in Greedily Constrained JSON:\\n[·\\\\n\\\\t] (whitespace) \"(quote) }(closing brace)\\nGreedy Constraining induces sub-optimal tokenization:\\n{\\\\n↓\\n···\\\\t\"name\"\\\\t:\\\\t\"John·Doe\"\\\\n\\n\\\\t,\\\\t\"age\"\\\\t:\\\\t35\\\\n\\n\\\\t,\\\\t\"·occupation \"\\\\t:\\\\t\"Software . . .\\nFigure 1. Greedy (overly-invasive) constraining of LLMs can dis-\\ntort tokenization, leading to different output than with uncon-\\nstrained decoding, even in the case where unconstrained generation\\nwould produce valid output for the same prompt. Gray boxes rep-\\nresent vocabulary tokens, orange hue is proportional to perplexity.\\nFor instance, it guarantees that generated output will always\\nadhere to the specified syntactic constraints, and reduces\\nthe need for ad-hoc parsing, retrying and prompting on\\ntop of LLMs. This facilitates the use of LLMs as part of\\nlarger pipelines or very specific tasks, without the need for\\nfine-tuning or additional post-processing.\\nKey Challenge: Token Misalignment Since LLM sub-\\nword token do not align directly with most given syntactic\\nconstraints, the key challenge in constrained decoding is to\\ninterface the LLM vocabulary with a syntactic constraint\\nlikeall output should be valid JSON . We showcase this in\\nFig. 1: While in an unconstrained setting, the LLM picks\\n·\"as the fourth token during generation, naively restricting\\nthe LLM to only immediatly valid JSON grammar termi-\\nnals like just \"or·, leads to the less optimal choice of \\\\t\\ninstead. By introducing such sub-optimal tokens, the distri-\\nbution of a badly-constrained LLM can easily diverge from\\nthe unconstrained case, leading to a significant decrease in\\nreasoning performance and therefore downstream accuracy.\\nHere, the naively constrained LLM produces various high\\nperplexity tokens, indicating that the model likely would not' metadata={'source': './data/2403.06988.pdf', 'page': 0}\n",
            "page_content='reasoning performance and therefore downstream accuracy.\\nHere, the naively constrained LLM produces various high\\nperplexity tokens, indicating that the model likely would not\\nhave chosen them otherwise (highlighted in Fig. 1). This\\nis because naive constraining does not account for bridge\\n1arXiv:2403.06988v1  [cs.LG]  7 Feb 2024' metadata={'source': './data/2403.06988.pdf', 'page': 0}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\ntokens that may span multiple parser terminals in the un-\\nderlying grammar (e.g., whitespace and double quotes, in\\nthis example). While existing work on code generation has\\nmade this observation before (Poesia et al., 2022), solving\\nthis problem efficiently remains challenging, as the online\\ncomputation of all bridge tokens at each decoding step, can\\nbe too costly in high-throughput environments.\\nThis work: Efficient, Minimally-Invasive Constraining\\nIn this work, we study the token misalignment problem out-\\nlined above, and examine its consequences for constrained\\ndecoding, showing empirically that misalignment can lead\\nto a significant decrease in downstream accuracy. Based\\non this observation, we propose the notion of minimally\\ninvasive constrained decoding : A form of constraining that\\nenforces a grammar, but also intervenes as little as possi-\\nble during generation, avoiding token misalignment and\\noptimizing for faithful, low-perplexity model output.\\nBased on this, we propose a novel constrained decoding\\nalgorithm, D OMINO , which can enforce context-free gram-\\nmars in a minimally-invasive way. In contrast to existing\\nmethods, DOMINO is highly efficient and incurs little to no\\noverhead, and in many cases even increases the throughput\\nof LLM inference over unconstrained generation, by lever-\\naging pre-computation (Willard & Louf, 2023) and a novel\\nspeculative decoding procedure for constrained decoding.\\nWe compare D OMINO with other approaches in Table 1.\\nMain contributions In summary, our key contributions are:\\n•We identify the challenges of constrained decoding,\\nmost notably the correct and efficient alignment of\\nsub-word tokens and grammar terminals (§2).\\n•We propose DOMINO , a novel constrained decoding\\nalgorithm, that addresses token misalignment and lever-\\nages pre-computation and speculative decoding for\\nvery low overhead generation (§3).\\n•An extensive evaluation thats shows that DOMINO is\\nminimally-invasive, low-overhead, significantly outper-\\nforms other methods, and even exceeds unconstrained\\ngeneration throughput in many cases (§4).\\n2. Challenges of Constrained Decoding\\nWe first introduce the required background and highlight\\nthe challenges of efficient and token-aligned constraining.\\nLarge Language Models (LLMs) are machine learning\\nmodels trained to complete a given text prompt. The current\\ngeneration of these models operate on sub-word tokens, such\\nas the commonly used Byte-Pair Encoding (BPE) (Sennrich\\net al., 2016; Kudo & Richardson, 2018). Conditioned on a\\nsequence of input tokens l1, . . . , l nthe model computes aTable 1. Overview of different constrained decoding methods\\nRegex CFGPre-\\nComputedMinimally\\nInvasive\\nLMQL ✓ ✗ ✗ ✗\\nGUIDANCE ✓ (✓) ✗ (✓)∗\\nOUTLINES ✓ ✓ (✓)†✗\\nPICARD ✓ ✓ ✗ ✗\\nSYNCHROMESH ✓ ✓ ✗ ✓\\nLLAMA .CPP ✓ ✓ ✗ ✓+\\nGCD ✓ ✓ ✗ ✓\\nDOMINO (ours) ✓ ✓ ✓ ✓\\n∗Boundary token healing,+Up to implementation. We observe\\nviolations in some cases,†For regex.\\nAlgorithm 1 Constrained Decoding\\nInput: Checker C, LLM f, Tokenized Prompt x\\nOutput: Completion oadhering to C\\n1:o←[]\\n2:C.init()\\n3:loop\\n4: C.update (o) // advance state of C\\n5:m←C.mask() // compute mask\\n6:v←f(x+o) // compute logits\\n7:v′←m⊙v′\\n8: t←decode (α′) // e.g., argmax or sample\\n9: ift=EOS then break\\n10: o.append (t)\\n11:end loop\\n12:return o // optionally detokenize\\nprobability distribution over the next token, from which the\\nnext token is decoded, i.e. chosen or sampled. This process\\nis repeated for each token. In this context, we denote the set\\nof all tokens, the vocabulary, as V.\\nConstrained Decoding Algorithm 1 shows the general\\noutline used by most constrained decoding approaches. A\\nchecker C, e.g., a parser or regex checker, is used to ensure\\nthat given an input xthe generated output oadheres to the\\nconstraint. For each new token, Cis first updated with the\\nlatest generated sequence and then used to generate a mask\\nm. This mask enforces that the next token must be a valid' metadata={'source': './data/2403.06988.pdf', 'page': 1}\n",
            "page_content='constraint. For each new token, Cis first updated with the\\nlatest generated sequence and then used to generate a mask\\nm. This mask enforces that the next token must be a valid\\ncontinuation. Each time the mask in Algorithm 1 rejects\\na token that would otherwise have been chosen, we say\\nthat the constrained decoding algorithm intervenes in the\\ndecoding process. We can therefore say that an algorithm\\nthat intervenes as little as possible is minimally invasive :\\nDefinition 2.1 (Minimally invasive) We consider a con-\\nstrained decoding method minimally invasive , if every valid\\noutput that can be generated by an unconstrained model\\n(for any given prompt), is also generated by the constrained\\nmodel, given the same prompt.\\n2' metadata={'source': './data/2403.06988.pdf', 'page': 1}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nReplacing the naive constraining in Fig. 1 with a minimally\\ninvasive method, would lead to the same output as with\\nunconstrained generation, up to the final closing }bracket\\nof the JSON object. After that, an unconstrained model\\ncontinues to generate text, which will not part of the JSON\\nobject. At this point, a minimally invasive decoder, however,\\nstops decoding by forcing an the EOStoken, to ensure that\\nthe output remains valid JSON.\\nAs this example shows, non-minimally naive constrained\\ndecoding (as in Fig. 1) can lead to different overall output,\\ncompared to unconstrained generation. This in turn, can\\naffect the model’s task accuracy as we will show in §4, for a\\nJSON-encoded version of the GSM8K dataset (Cobbe et al.,\\n2021). There, naive constraining can reduce accuracy from\\n41.5%in the unconstrained case to 30.8%, while minimally\\ninvasive constraining actually achieves a slight increase in\\naccuracy to 41.8%(five-shot, Mistral 7B Jiang et al. (2023)).\\nNext, we discuss the broad categories of existing constrained\\ndecoding approaches. Current approaches implement one\\nor multiple of the following paradigms: i) Regex-Based , ii)\\nOnline Parser-Guided and iii) Template-Based approaches.\\nRegex-Based Decoding limited to regular expressions is\\nsupported by many frameworks (e.g., LMQL (Beurer-Kellner\\net al., 2023), OUTLINES (Willard & Louf, 2023), GUID -\\nANCE (Lundberg et al.), LLAMA .CPP(Gerganov & et. al.))\\nand typically does not suffer from the token misalignment\\nproblem as it is simpler to check if a token is a legal continu-\\nation or not. For this, Willard & Louf (2023) also proposed\\nan algorithm to pre-compute a regex checker for the model\\nvocabulary offline, to be more efficient during inference.\\nOnline Parser-Guided refers to running a parser and\\nscanner in lock-step with an LLM, and then computing\\nonline, which tokens are valid continuations in each step.\\nSuch algorithms ( PICARD (Scholak et al., 2021), GCD (Geng\\net al., 2023a), LLAMA .CPP) can support full CFGs and as\\nPoesia et al. (2022) ( SYNCHROMESH ) demonstrated, can\\nbe built to support bridge tokens and thus to be minimally\\ninvasive. However, all of these approaches produce com-\\nparatively high inference overhead, since, in the worst case,\\nthey have to check the entire model vocabulary at each step.\\nTemplate-Based Approaches Since constrained genera-\\ntion can add additional overhead during inference, GUID -\\nANCE and LMQL , propose a template-based approach, where\\nsome structure is fixed, and only parts of the output are sam-\\npled under a regex constraints. For instance, templated\\ngeneration can be used to implement schema-driven JSON,\\nwhere fields are fixed. Template-based decoding is efficient\\nas templated tokens can be added deterministically during\\ngeneration, without invoking the LLM, thereby requiring\\nless model forward passes. However, this form of accelera-Prompt: Tell me one sentence about Thomas Cha-\\npais.\\\\n\\\\nA: (Response In JSON)\\n(1a) Templated, Multi-Line Perplexity: 24.50\\n\\\\n{\\\\n····\"reasoning\":·\"Thomas·Chapais·is·a\\n·Canadian ·politician . . .\\n(1b) Templated, Single-Line Perplexity: 26.75\\n·{·\"reasoning\":·\"I·don’t·know·who·Thomas ·Chap\\nais·is. . .\\n(2) Naturalized Template Output Perplexity: 49.39\\n·{·\"reasoning\":·\"I·don’t·know·who·Thomas ·Chap\\nais·is. . .\\n(3) Output with Naturalized Template Perplexity: 4.17\\n·{·\"reasoning\":⇝·\"I·don’t·know·him\"·}\\nFigure 2. Template-based tokens, marked as , force unnatural\\ntokenization and formatting, which can lead to different outputs\\nand increased perplexity. Gray boxes represent vocabulary tokens,\\nhue is proportional to perplexity.\\ntion also has its downside, as discussed next.\\nTemplate-Induced Misalignment To insert templated to-\\nkens without invoking the model, an external tokenizer has\\nto be used to translate template text into tokens. We show-\\ncase this in Fig. 2, where we use the template-based GUID -' metadata={'source': './data/2403.06988.pdf', 'page': 2}\n",
            "page_content='kens without invoking the model, an external tokenizer has\\nto be used to translate template text into tokens. We show-\\ncase this in Fig. 2, where we use the template-based GUID -\\nANCE with Mistral-7B , to generate text in JSON format,\\nand compare it to unconstrained generation. (1a) and (1b)\\nalready show that depending on the concrete phrasing of\\na template (e.g., whitespace, formatting), output can vary\\nsignificantly. To compare to unconstrained generation, we\\nuse the model to also generate the template text, but without\\nimposing a fixed tokenization (algorithm in App. B). We\\nthen generate output with this naturalized template (3) to\\ncontrast it with template-induced tokenization.\\nOverall, template-based outputs exhibit clearly different\\noutputs and much higher perplexities (perplexities 24.50−\\n26.75), when compared to unconstrained generation (per-\\nplexity 4.17). Further, we observe that naturalizing the full\\ntemplate-based output under the model-preferred tokeniza-\\ntion (2) in Fig. 2, results in a form of perplexity explosion\\n(49.39), indicating that without invasive constraining, the\\nmodel is highly unlikely to generate such output. While\\nperplexity and output differences are not directly indicative\\nof output quality, our evaluation in §4.1 extends on this\\nexperiment, and indeed shows that invasive template-based\\ngeneration can lead to a significant drop in task accuracy,\\ncompared to unconstrained and less invasive approaches.\\nToken Healing To reduce the impact of template-induced\\nmisalignment, GUIDANCE implements token healing (Lund-\\nberg & Ribeiro), a method that attempts to improve tok-\\nenization at the transition points between templated and\\n3' metadata={'source': './data/2403.06988.pdf', 'page': 2}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nnon-templated tokens. Token healing truncates the prompt\\nto the second-to-last token boundary, and enforces the rest\\nof the prompt as a prefix on generation. This can be effec-\\ntive at avoiding some tokenization issues and integrating\\nbridge tokens, yet, most templated tokens remain fixed as\\ndemonstrated in Fig. 2, where token healing is already in\\neffect, and still leads to significant differences in output.\\nKey Challenge This section, as summarized in Table 1,\\ndemonstrates that the key challenge in constrained decod-\\ning is to find a method that allows i) expressivity for regex,\\ncontext-free grammars and templated decoding, ii) is min-\\nimally invasive in all settings and iii) has low inference\\noverhead. Next, we discuss DOMINO , a novel constrained\\ndecoding algorithm, that fits these requirements.\\n3. Efficient Aligned Constrained Decoding\\nIn this section, we address the above challenges by introduc-\\ningDOMINO , a fast minimally-invasive constrained decod-\\ning algorithm, showcased in Fig. 3. First, §3.1 discusses the\\nnecessary preliminaries. Then, in §3.2-§3.5 we present the\\nmain algorithm and then discuss further details.\\n3.1. Preliminaries\\nFormal Languages A formal language is a set Lof finite\\nstrings over a given alphabet Σ. A language is called regular\\nif it can be described by a regular expression. A language is\\ncontext-free if it can be described by a context-free grammar\\n(CFG). Such a grammar is described by a set of production\\nrules A=αBγ , where upper case names ( A,B) are non-\\nterminals that are recursively extended and lower case greek\\ncharacters (e.g., α,β) are terminals that are part of the\\nlanguage. For an example, see the grammar in Fig. 3 (a),\\nwith the non-terminal Eand terminals int,(,),+, which\\nare defined either by a regex or a literal string.\\nAll regular languages can be recognized by a non-\\ndeterministic finite automata (NFA). A NFA is a set of states\\nwith a start state and a set of accepting states. Connected\\nwith transitions that are labeled with a character or the empty\\nstring ε. We traverse or execute an NFA by starting in the\\nstart state and whenever we read a character follow the\\nappropriate transitions or any ε-transitions. For a regular ex-\\npression, we can construct a NFA (McNaughton & Yamada,\\n1960; Thompson, 1968), that when fed a string, character\\nby character, ends in a set of state including at least one\\naccepting state, if and only if the string matches the regular\\nexpression. To illustrate this approach consider the grammar\\ngiven in Fig. 3 (a). The terminal intis given by a regex that\\nallows any positive integer (not starting with leading zeros)\\nor one or more zeros. Fig. 4 shows an NFA for this regex.3.2. Character Scanner\\nLike classical parsers, DOMINO separates the CFG recog-\\nnition into a parser and a scanner (or lexer). The parser\\nenforces the high-level structure of the language, e.g., the\\nrules in a context-free grammar, and the scanner enforces the\\nlow-level structure, i.e., the regular expressions of the termi-\\nnals. The key idea is that any legal program in a context-free\\ngrammar is a sequence of terminals, or formally:\\nLemma 3.1 LetLGbe the language described by a CFG\\nG. Further, let r1, . . . , r nbe the regular expressions of the\\nterminals of Gand the rEOS= $. Then, it holds that:\\n•The union of these regular expressions r=\\nr1|. . .|rn|rEOSmatches any terminal in G.\\n•The regular expression R=r+matches all non-empty\\nsequences of terminals in the language.\\n•The language LRdescribed by Rcontains all legal\\nprograms in G, i.e., LG⊆LR, but also\\n•LRcontains some illegal programs, i.e., LR̸⊆LG.\\nTo construct an NFA for LR, we construct the NFA for\\nthe regex rifor each terminal in the grammar. From the\\naccepting states of the invidual terminal NFAs, we add an\\nε-transition to a single NFA accepting state qa. Similarly,\\nwe add a transition from the start state q0to the start state' metadata={'source': './data/2403.06988.pdf', 'page': 3}\n",
            "page_content='accepting states of the invidual terminal NFAs, we add an\\nε-transition to a single NFA accepting state qa. Similarly,\\nwe add a transition from the start state q0to the start state\\nof each DFA. Finally, we add a transition from q0toqa,\\nto allow for the chaining of multiple tokens. This is the\\nstandard disjuction construction for regex NFAs, however,\\nwe do this explictly to track the sub-automata correspoding\\nto each terminal. This construction is showcased in Fig. 3 (b)\\nfor the grammar in Fig. 3 (a). There, the boxes in the middle\\ncorrespond to individual NFAs like intas shown in Fig. 4.\\n3.3. Vocabulary-Aligned Subterminal Tree\\nUsing a scanner Sat generation time can enforce that the\\ngenerated string will be in LR, but not neccesarily in LG.\\nTo ensure this, we need to also run a parser Pon the output\\nas it is generated and dynamically allow and disallow some\\nof the transitions in S, according to the current parser state.\\nFor this, we first lift Sfrom the character level to the\\n(sub)terminal level used by the parser. To do so, for each\\nnode in Swe follow the transitions for each token in the vo-\\ncabulary and enumerate all reachable states. Particularly, we\\ntrack which terminal NFAs are partially or fully traversed.\\nAs terminals are not neccesarily aligned with the token vo-\\ncabulary, we introduce the notion of a subterminal as a part\\nof a terminal NFA. In particular, for terminal α, e.g.,int,\\nwe say that Sreads a:\\n•Full terminal if it passes q0and reaches an accepting\\nstateqαin the NFA for the terminal. We denote this\\nas. If we end in an accepting state, that also allows\\n4' metadata={'source': './data/2403.06988.pdf', 'page': 3}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\n(a) Example Grammar\\nE = E | E + E | (E) | int\\nint = ([1-9][0-9] *)|(0+)\\n(b) Character Scanner\\nq0 ( qa\\n)\\n+\\nint\\nws\\nEOS(c) Vocabulary\\nV={0,1,2,12,),(,+,+11(,EOS}\\n(d) Vocabulary-aligned Subterminal Tree (offline, per node)\\nq[0-9]*\\nint\\n0\\n1\\n2\\n12\\n(\\n)\\n+\\n+1\\n1(\\nEOS( )+ EOS 0,1,\\n2,12\\n1(+1(e) Parser (online)\\n( )+ EOS 0,1,\\n2,12\\n1(+1(12\\nk= 0\\n012\\n1+\\n2+1\\n(1+\\n)EOS\\nk= 1\\n012\\n1+\\n2+1\\n(1+\\n)EOS\\nk= 2\\n012\\n1+\\n2+1\\n(1+\\n)EOS\\nFigure 3. Running example and overview of DOMINO . (a) shows an example grammar, (b) the character level NFA for this language, (d)\\none of the per-state subterminal trees for the grammar in (c). (e) shows how a parser can be used to prune this tree at inference time and\\nobtain token masks efficiently by traversing the tree.\\nint 0* [0-9]*0 [1-9]\\n0 [0-9]*\\nFigure 4. NFA for the intterminal from Fig. 3 (a). Traversed from\\nnodeintthis NFA accepts all legal inputs for the terminal.\\nfurther transitions, such as both accepting states in\\nFig. 4, we also consider this a full terminal, but allow\\nfurther subterminals within the terminal NFA ( ).\\n•Start subterminal if starting from q0we reach a non-\\naccepting but valid state qαfor the NFA of the terminal.\\nWe denote this as .\\n•End subterminal if starting from a state qαwithin\\nthe NFA for αwe reach an accepting state for α. We\\ndenote this as .\\n•Continuation subterminal if starting from a state qα\\nwithin the NFA for αwe reach another non-accepting\\nstate for α. We denote this as .\\nWe visit every state qinS, obtain the current (sub)terminal\\nαand, for each vocabulary token l∈ V, enumerate all possi-\\nble subterminal sequences {αj\\n1, . . . , αj\\nmj}j. Typically there\\nis only one such sequence unless there is ambiguity in the\\ngrammar, e.g., in C-style languages we can not be sure if\\nwe are reading a variable name of a keyword. Ignoring these\\nedge cases, we show an example of this on the left hand side\\nof Fig. 3 (d) for the vocabulary given in Fig. 3 (c), where\\nwe visualize the subterminals in the previously introduced\\nbox notation with colors correspoding to the terminals in\\nFig. 3 (b). Note that there may be tokens in the vocabulary\\nfor which no continuation is possible, e.g., if we add an a\\ntoken to the language. After enumerating all these subtermi-\\nnal sequences, we organize them into a prefix tree Tq, where\\nwe attach the corresponding vocabulary tokens las values\\nto the nodes. We formalize this procedure in Algorithm 2.Algorithm 2 Construct Terminal Tree\\nInput: CFG G, Alphabet Σ, V ocabulary V\\nOutput: Scanner S\\n1:T={}\\n2:for all q∈S.states ()do\\n3: α←q.subterminal ()// get current (sub)terminal\\n4: for all l∈ V do\\n5: {αj\\n1, . . . , αj\\nmj}j←q.traverse (l)\\n6: T ← T ∪ { (αj\\n1, . . . , αj\\nmj), l}j\\n7: end for\\n8: Tq←PrefixTree (T)\\n9:end for\\n3.4. Parser\\nWhile subterminal trees can be pre-computed, we still need\\na parser at inference time, to disallow illegal continuations.\\nFor example, consider the prefix tree Fig. 3 (d). If so far, we\\nhave observed the sequence (12and correctly advanced the\\nparser and scanner to this state, we are in an intterminal ,\\nthat can still be extended further. Following the prefix tree\\nin Fig. 3 (d) would permit continuations such as (, which\\nare clearly illegal in the grammar. Thus, we need to disallow\\nthese continuations dynamically by consulting the parser.\\nAt inference time traversing the so-far generated sequence o\\nthrough the scanner and parser will result in a scanner state\\nSand parser state P. The active state of Swill be a set\\nof states q1, . . . , s m. The active state of Pwill be a parser\\nthat tracks rules that can match the output oso far. For each\\nof these nodes qiwe retrive the corresponding subterminal\\ntreeTqiand use the parser to check which of the possible\\n5' metadata={'source': './data/2403.06988.pdf', 'page': 4}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\ncontinuations are legal. The depths to which we follow these\\nterminal sequences in the prefix-tree is determined by the\\nso-called lookahead parameter k.\\nWe showcase this in Fig. 3 (e), where so far we have read the\\ninput(12. The parser thus knows that it has seen the partial\\nrule(E), in which recusively Ewas initialized with the int\\nterminal. The scanner Shas been advanced similarly and\\nhas a single active state sthat corresponds to that for an int\\nterminal , as shown in Fig. 3 (d). In the corresponding\\nprefix tree Tswe can now check each outbound edge with\\nthe parser and find that the EOS and(can not produce\\nlegal (sub)terminal sequences, but all other tokens do. After\\ntraversing one level of this tree ( k= 0) this includes all\\nnumber terminals, +and). By increasing kand checking\\nfurther paths in the tree we can also include the tokens +1\\nand1(atk= 1andk= 2respectively.\\n3.5. D OMINO\\nBased on Algorithm 1, DOMINO implements constraining\\nthat leverages subterminal trees to efficiently check for legal\\ncontinuations in V, that, by construction, line up with the\\ncurrent state, just like the pieces in a game of domino.\\nFor this, we compute the character scanner Sand corre-\\nsponding prefix trees Tqfor all q∈Sbefore inference starts\\n(offline). When update is called in Algorithm 1, we can\\nthen advance the scanner and parser state. When mask is\\ninvoked , we traverse the corresponding Tqto the desired\\ndepth, and take the union over the associated tokens to com-\\npute the current mask. Here, k= 0 is already sufficient\\nto ensure that the generated output is in LG. If we always\\ntraverse the full prefix-trees ( k=∞or sufficently large),\\nthis approach is minimally invasive, as all valid tokens will\\neventually be reached and included in the mask.\\nOverall, this enables us to handle expressive constraining\\nwith far less overhead than fully online approaches, as the\\nsize of subterminal trees is much smaller than the size of\\nthe model vocabulary, which we would need to traverse\\notherwise. Similarly DOMINO can also be extended to other\\nforms of constraining, e.g. to execute GUIDANCE programs.\\nFurther Optimizations On top of this, DOMINO also\\nsupports an optimization already present in LLAMA .CPP\\n(Gerganov & et. al.)’s fully online approach, which we term\\nopportunistic masking : Rather than computing the mask for\\nthe full logit vectors as in Algorithm 1, we can first run the\\ndecode step of the LLM and then use the parser to check the\\nmodel-proposed token first. Only if incorrect, we need to\\ncompute the rest of the mask, and thus let the LLM guide\\ndecoding.\\nTo realize this in DOMINO , rather than traversing the trees\\nTqfrom the root, we first determine the nodes linked tothe proposed token, and only then check if there exists a\\nparser-allowed path from the root to this node. This is can be\\nvery effective when an LLM already naturally adheres to a\\ngrammar, and parser transitions are expensive. In DOMINO ,\\noppurtunistic masking is enabled using a runtime flag.\\nWe finally want to note, that token healing can be impl-\\nmented in a similar way to GUIDANCE (Lundberg et al.),\\ni.e., by stripping the input back to the last token boundary\\nand changing the beginning of the grammar to force a prefix.\\nHowever, this means that the grammar needs to be recom-\\npiled for the current problem and can not neccesarily be\\nshared between multiple instance. Note however, that this is\\nof lesser concern in DOMINO , as it is only relevent for the\\nfirst boundary with the prompt, where all other boundaries\\nare embedded seamlessly into the grammar.\\n3.6. Speculative Decoding\\nSpeculative Sampling (Chen et al., 2023) is a technique to\\nspeed up the LLM sampling process, by using a smaller\\nLLM to propose multiple tokens and then only evaluate\\nthe full large LLM to confirm this token choice. This is\\nefficient, as the parallel nature of Transformer-based models\\nVaswani et al. (2017), allows to validate multiple tokens' metadata={'source': './data/2403.06988.pdf', 'page': 5}\n",
            "page_content='the full large LLM to confirm this token choice. This is\\nefficient, as the parallel nature of Transformer-based models\\nVaswani et al. (2017), allows to validate multiple tokens\\nwith a single forward pass, where rejected tokens can simply\\nbe discarded without the need to backtrack. In DOMINO ,\\nwe adopt a similar approach to further speed up inference,\\nbased on parser and scanner state. At any time, the active\\nscanner state is (largely) given by the most recently read\\nsubterminal α(or{αj}jif the NFA could be in multiple\\npossible subterminals). Similarly, we let βdenote some sub-\\nstate of the currently used parser, e.g., the currently applied\\nrule. Conditioned on α, β we can then learn a simple, count-\\nbased model for speculative next token prediction:\\nP(l|α, β) =#{LLM chose lin state (α, β)}\\n#{reached state (α, β)}.\\nAs structured languages often are very predictable and α, β\\ncan be strongly indicative of the next token, this mechanism\\ncan lead to massive speed-up during inference. Further, as\\nwe learn these counts over the parser state β, we only learn\\nto predict tokens that are legal in the language.\\nIn practice, we parameterize stokens to be predicted this\\nway at a time, if the P(l|α, β)is sufficiently large. This\\nform of speculative decoding is independent of standard\\nspeculative decoding applied to the underlying LLM (Chen\\net al., 2023), and could even be applied jointly with it.\\n6' metadata={'source': './data/2403.06988.pdf', 'page': 5}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nTable 2. Task Accuracy of different constrained decoding methods on GSM8K Cobbe et al. (2021) and CONLL2003 Tjong Kim Sang\\n(2002) datasets (400 test samples). All experiments rely on 5-shot prompting with demonstrations taken from the training split.\\nDataset Model Method Accuracy Well-Formed Perplexity Performance Impact\\nGSM8KMistral 7BUnconstrained 0.415 0.952 1.636 1.0 ×\\nGUIDANCE Lundberg et al. 0.345 0.960 1.624 0.98 ×\\nGUIDANCEWSLundberg et al. 0.403 0.976 1.737 0.54 ×\\nllama.cpp Gerganov & et. al. 0.375 0.973 1.751 0.80 ×\\nDOMINO (k=∞)0.418 0.968 1.739 1.77×\\nLlama-2 13BUnconstrained 0.262 0.904 1.650 1.0 ×\\nGUIDANCE Lundberg et al. 0.152 0.947 1.659 1.12 ×\\nGUIDANCEWSLundberg et al. 0.259 0.977 1.760 0.73 ×\\nllama.cpp Gerganov & et. al. 0.237 0.978 1.780 0.86 ×\\nDOMINO (k=∞)0.262 0.920 1.750 1.66×\\nCoNLL2003Mistral 7BUnconstrained 0.21 0.988 1.573 1.0 ×\\nGUIDANCE Lundberg et al. 0.098 0.998 1.780 2.02 ×\\nGUIDANCEWSLundberg et al. 0.19 0.998 1.896 0.82 ×\\nllama.cpp Gerganov & et. al. 0.117 0.995 1.560 0.80 ×\\nDOMINO (k=∞)0.21 0.988 1.902 2.66×\\nLlama-2 13BUnconstrained 0.09 0.897 1.579 1.0 ×\\nGUIDANCE Lundberg et al. 0.062 1.000 1.820 2.18 ×\\nGUIDANCEWSLundberg et al. 0.087 0.980 1.767 0.90 ×\\nllama.cpp Gerganov & et. al. 0.080 0.922 1.786 0.86 ×\\nDOMINO (k=∞)0.09 0.897 1.812 2.71×\\nWSGUIDANCE CFG program with flexible whitespace and formatting.\\n4. Experimental Evaluation\\nWe evaluate DOMINO in terms of downstream task accuracy,\\ncompare its performance to multiple baselines and ablate\\nkey parameters such as k.\\nSetup We evaluate on the Mistral 7B (Jiang et al., 2023)\\nand the Llama-2 13B (Touvron et al., 2023c) language mod-\\nels. As inference backends, we rely on both, transformers\\n(Wolf et al., 2019) and llama.cpp (Gerganov & et. al.) on\\nNVIDIA A100 40GB or H100 80GB GPUs. Because of its\\nnature, we explicitly evaluate DOMINO in an offline setting,\\nwhere all grammars are known ahead of time and do not\\nvary across inference requests.\\nDatasets We assess downstream accuracy of different con-\\nstraining methods with the GSM8K (Cobbe et al., 2021)\\nbenchmark for math reasoning and CoNLL-2003 (Tjong\\nKim Sang, 2002) for named-entity recognition (subset of\\n400 test samples). To examine the performance properties of\\ndifferent decoding methods, we compare their overhead over\\nunconstrained decoding for different tasks, including the\\nconstrained generation of JSON ,JSON with Schema , the C\\nProgramming Language ,XML with Schema and more static,\\nregex-based generation templates similar to GUIDANCE or\\nLMQL programs with simple structure.Baselines We consider the following baselines:\\n•Unconstrained Generation We generate output with-\\nout any form of constraints, using the same prompts\\nand inference backend.\\n•GUIDANCE Programs (Lundberg et al.) We construct\\nGUIDANCE programs to generate output in the desired\\noutput formats. We compare template-based programs\\n(standard approach) and CFG-based variants, which\\nare whitespace agnostic (comparison in App. A).\\n•LLAMA .CPP Grammars (Gerganov & et. al.) We\\nrely on LLAMA .CPP’s support for ebnf grammars as an\\nonline parsing baseline (also representative of (Poesia\\net al., 2022) and Geng et al. (2023b)).\\n4.1. Task Accuracy\\nWe first evaluate the impact of different constrained decod-\\ning methods on downstream accuracy. For this, we use the\\nGSM8K benchmark for math reasoning and CoNLL-2003\\nfor named-entity recognition.\\nSetup For both datasets, we prompt and constrain the mod-\\nels to generate a response in a given JSON format, instead of\\nfree-text reasoning (see App. D for examples). Our prompts\\nconsist of 5 few-shot demonstrations from the training split,\\nfor which we manually construct the corresponding JSON\\n7' metadata={'source': './data/2403.06988.pdf', 'page': 6}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nTable 3. Impact on throughput of constrained decoding methods with different grammars, compared to unconstrained generation. We\\nreport the change in throughput compared to generating unconstrained output with the same model and inference backend. As CFGaccel\\nwe report D OMINO opportunistic masking or speculative decoding, depending on which is more effective.\\nllama.cppop\\nGerganov & et. al.guidanceHF\\nLundberg et al.DOMINOHF\\n(Ours)\\nGrammar Model CFG Template↓CFG CFG CFGaccel\\nJSON (no schema)Mistral 7B 0.79× 1.03× 0.81× 0.88× 0.96×(opportunistic)\\nLlama 13B 0.83× 1.03× 0.86× 0.95× 1.12×(spec. s= 10 )\\nJSON (GSM8K schema, see App. D)Mistral 7B 0.80× 0.77× 0.59× 0.99× 1.77×(spec. s= 10 )\\nLlama 13B 0.86× 0.94× 0.74× 0.99× 1.66×(spec. s= 10 )\\nC Programming LanguageMistral 7B 0.74× - - 0.41 × 0.78×(opportunistic)\\nLlama 13B 0.81× - - 0.54 × 0.85×(opportunistic)\\nXML (with schema)Mistral 7B 0.80× - - 0.94 × 1.52×(spec. s= 10 )\\nLlama 13B 0.87× - - 0.98 × 1.84×(spec. s= 10 )\\nFixed TemplateMistral 7B 0.55× 1.95× 0.92× 0.97× 1.30×(spec. s= 10 )\\nLlama 13B 0.69× 2.05× 1.06× 0.99× 1.91×(spec. s= 10 )\\nopllama.cpp always runs with opportunistic masking,HFUsing the transformers library as inference backend.\\n↓GUIDANCE templates lead to significantly worse accuracy compared to CFGs (cf. Table 2), but we show them here for completeness.\\nresponse. We then compare the accuracy of the generated\\nJSON responses, considering both the validity of the JSON\\nformat and the accuracy of the final response. We also mea-\\nsure perplexity of the generated output and the overhead\\nover unconstrained generation in terms of throughput.\\nResults As documented in Table 2, DOMINO achieves the\\nbest accuracy for all tasks, while also improving through-\\nput well beyond unconstrained generation. In all cases,\\nDOMINO ’s accuracy is the same or improved compared to\\nunconstrained generation, indicating very low or no inva-\\nsiveness. In contrast, with standard GUIDANCE , we observe\\nclear artifacts of invasiveness, as accuracy drops by up to\\n11% points compared to unconstrained generation. And,\\nwhile GUIDANCE ’s inference optimizations do increase\\nthroughput (up to 2.02×),DOMINO accellerates inference\\neven further (up to 2.71×), while also maintaining high ac-\\ncuracy. Implementing minimally invasive GUIDANCEWS\\nprograms with flexible whitespace and formatting restores\\nsome accuracy, but not fully, and also lowers throughput\\nsignificantly to ∼0.8×.LLAMA .CPP’s online parsing ap-\\nproach also does not appear to be fully non-invasive, al-\\nthough accuracy seems less impaired, while throughput is\\nalso consistently reduced to ∼0.8×.\\n4.2. Parameter Study\\nNext, we investigate the key parameters of D OMINO .\\nLookahead To experiment with lookahead parameter k,\\nwe evaluate on GSM8K as before, but varying k. We reportTable 4. GSM8K task accuracy with different lookahead k.\\nConfiguration Mistral 7B Llama-2 13B\\nUnconstrained 0.415 0.155\\nDOMINO (k= 0) 0.308 0.0\\nDOMINO (k= 1) 0.1 0.036\\nDOMINO (k=∞)0.418 0.157\\nthe results in Table 4. We observe that lower kvalues impair\\nperformance significantly. Manual inspection shows that\\ndepending on k, the model is forced into different whitespac-\\ning behavior, as bridge tokens like },are unavailable, lead-\\ning to irregularities. This can affect reasoning, e.g. Llama-2\\nis unable to produce object lists of length greater than 1, if\\nrelevant bridge tokens are missing. DOMINO withk=∞,\\nhowever, recovers and even slightly exceeds unconstrained\\naccuracy, demonstrating minimal invasiveness.\\nSpeculation and Opportunistic Masking We also exper-\\niment with the number of speculative tokens swe propose\\nat each decoding step, to speed up generation. For this,\\nwe compare the generation of schema-driven JSON and\\nfree-form JSON text with Mistral 7B . We form priors on\\n10 randoms samples, and then measure mean performance\\nacross 100generated outputs per configuration, without' metadata={'source': './data/2403.06988.pdf', 'page': 7}\n",
            "page_content='free-form JSON text with Mistral 7B . We form priors on\\n10 randoms samples, and then measure mean performance\\nacross 100generated outputs per configuration, without\\nupdating counts. We report the results in Fig. 5.\\nWe find speculative decoding with s∈ {6,8,10}to be\\nparticularly effective for schema-driven JSON generation, as\\nit achieves a throughput of 1.7x compared to unconstrained\\n8' metadata={'source': './data/2403.06988.pdf', 'page': 7}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\n0 2 4 6 8 10\\nNumber of Speculative Sokens0102030405060JSON (With Schema) [tok/s]\\nspeculative\\nunconstrained\\nopportunistic\\n0 2 4 6 8 10\\nNumber of Speculative Tokens0102030405060JSON (No Schema) [tok/s]\\nFigure 5. Impact of the number of speculative tokens kon through-\\nput (tokens per second) with Mistral 7B and JSON generation with\\nand without schema, using D OMINO withtransformers LLMs.\\ngeneration. On free-form JSON output, speculation is not\\neffective, and DOMINO opportunistic masking is preferable,\\nincurring only a low 4.21% overhead.\\n4.3. Efficiency\\nNext, we examine throughput and efficiency of DOMINO .\\nFor this, we compare unconstrained generation thrrough-\\nput of each backend, and report the relative differences\\nwhen running with constrained generation. We do not in-\\nclude DOMINO ’s precomputation time as part of the reported\\nthroughputs. We note that for the tested grammars, it ranges\\nfrom 1-5s, with C being an outlier at around 20s.\\nGrammars We compare different constraining tasks, as\\nshown in Table 3. For each task, we prepare a small set of\\nrelatively general inputs, prompting the model to generate a\\nresponse in the general distribution of the given output for-\\nmat (details in App. C). Where practical, we also implement\\nGUIDANCE programs, using their custom CFG-like syntax.\\nSetup We run 100 repetitions per configuration. In each,\\nwe sample one of 5 different prompts per workload, and\\nsample output of up to 128tokens from the model, using\\na temperature value of 1.0. This way, we ensure that the\\nmodel produces in-distribution output, but that it still ex-\\nhibits diversity. Before measuring, we run 10repetitions\\nof warmup, allowing our speculative mechanisms to form a\\nprior. After that, the learned priors remain fixed.\\nResults As shown in Table 3, DOMINO is highly effective\\nand clearly reduces the compuational load of constraining\\nat inference time. DOMINO outcompetes both GUIDANCE\\nandllama.cpp ’s online parsing approach significantly. For\\ngrammars with predictable structure (e.g. schema-driven\\nformats), speculative decoding is particularly effective, lead-\\ning to up to 77% higher throughput over unconstrained\\ngeneration, while remaining minimally invasive.\\nC code generation induces the most overhead, which can\\nbe explained by the fact that the C grammar is the mostcomplex of the tested workloads. Here, speculative decod-\\ning does not bring any benefits, as the C code is to hard to\\npredict using our simple count-based model. However, by\\nrelying on DOMINO ’s opportunistic masking mode, we still\\noutcompete llama.cpp , running at 0.78×vs.0.74×.\\n5. Conclusion\\nWe have shown the need for minimally invasive, highly-\\nefficient constrained decoding methods. As first instantia-\\ntion of this for grammars, we presented DOMINO , which\\nleverages precomputation, speculative decoding and oppor-\\ntunistic masking, to implement minimally invasive con-\\nstraining (no accuracy loss), often overhead-free or even\\nfaster generation, and thus, high-throughput inference.\\nImpact Statement\\nThis paper presents work whose goal is to advance the field\\nof Machine Learning. There are many potential societal\\nconsequences of our work, none which we feel must be\\nspecifically highlighted here.\\nAcknowledgements\\nThis work has received funding from the Swiss State Secre-\\ntariat for Education, Research and Innovation (SERI). This\\nwork has been done as part of the SERI grant SAFEAI (Cer-\\ntified Safe, Fair and Robust Artificial Intelligence, contract\\nno. MB22.00088, SERI-funded ERC Consolidator Grant).\\n9' metadata={'source': './data/2403.06988.pdf', 'page': 8}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nReferences\\nAnil, R., Borgeaud, S., Wu, Y ., Alayrac, J., Yu, J., Sori-\\ncut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican,\\nK., Silver, D., Petrov, S., Johnson, M., Antonoglou, I.,\\nSchrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lilli-\\ncrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M.,\\nBarham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds,\\nM., Xu, Y ., Doherty, R., Collins, E., Meyer, C., Ruther-\\nford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G.,\\nPiqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka,\\nI., Roelofs, B., White, A., Andreassen, A., von Glehn,\\nT., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M.,\\nSygnowski, J., and et al. Gemini: A family of highly\\ncapable multimodal models. ArXiv preprint , 2023. URL\\nhttps://arxiv.org/abs/2312.11805 .\\nBeurer-Kellner, L., Fischer, M., and Vechev, M. T. Prompt-\\ning is programming: A query language for large language\\nmodels. Proc. ACM Program. Lang. , (PLDI), 2023.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\\nS., Radford, A., Sutskever, I., and Amodei, D. Language\\nmodels are few-shot learners. In Advances in Neural In-\\nformation Processing Systems 33: Annual Conference on\\nNeural Information Processing Systems 2020, NeurIPS\\n2020, December 6-12, 2020, virtual , 2020.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. ArXiv preprint , 2023.\\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,\\nH. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N.,\\nBrockman, G., Ray, A., Puri, R., Krueger, G., Petrov,\\nM., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,\\nS., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-\\nian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D.,\\nPlappert, M., Chantzis, F., Barnes, E., Herbert-V oss, A.,\\nGuss, W. H., Nichol, A., Paino, A., Tezak, N., Tang,\\nJ., Babuschkin, I., Balaji, S., Jain, S., Saunders, W.,\\nHesse, C., Carr, A. N., Leike, J., Achiam, J., Misra,\\nV ., Morikawa, E., Radford, A., Knight, M., Brundage,\\nM., Murati, M., Mayer, K., Welinder, P., McGrew, B.,\\nAmodei, D., McCandlish, S., Sutskever, I., and Zaremba,\\nW. Evaluating large language models trained on code.\\nArXiv preprint , 2021.\\nCobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\nArXiv preprint , 2021.Geng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-\\nconstrained decoding for structured NLP tasks without\\nfinetuning. In EMNLP , 2023a.\\nGeng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-\\nconstrained decoding for structured nlp tasks without\\nfinetuning. In Proc. of EMNLP , 2023b.\\nGerganov, G. and et. al. llama.cpp: Port of facebook’s\\nllama model in c/c++. URL https://github.com/\\nguidance-ai/guidance .\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. ArXiv preprint ,\\n2023.\\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\\nB., Bamford, C., Chaplot, D. S., de Las Casas, D., Hanna,\\nE. B., Bressand, F., Lengyel, G., Bour, G., Lample, G.,\\nLavaud, L. R., Saulnier, L., Lachaux, M., Stock, P., Sub-\\nramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet,\\nT., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E.\\nMixtral of experts. ArXiv preprint , 2024.\\nKudo, T. and Richardson, J. SentencePiece: A simple and\\nlanguage independent subword tokenizer and detokenizer\\nfor neural text processing. In Proc. of EMNLP , 2018.' metadata={'source': './data/2403.06988.pdf', 'page': 9}\n",
            "page_content='Kudo, T. and Richardson, J. SentencePiece: A simple and\\nlanguage independent subword tokenizer and detokenizer\\nfor neural text processing. In Proc. of EMNLP , 2018.\\nLundberg, S. and Ribeiro, M. T. A. p. The Art of Prompt\\nDesign: Prompt Boundaries and Token Healing.\\nLundberg, S., Ribeiro, M. T. A. p., and et. al. Guidance-\\nai/guidance: A guidance language for controlling\\nlarge language models. URL https://github.com/\\nguidance-ai/guidance .\\nMcNaughton, R. and Yamada, H. Regular expressions and\\nstate graphs for automata. IRE Trans. Electron. Comput. ,\\n(1), 1960.\\nOpenAI. GPT-4 technical report. ArXiv preprint , 2023.\\nPoesia, G., Polozov, A., Le, V ., Tiwari, A., Soares, G.,\\nMeek, C., and Gulwani, S. Synchromesh: Reliable code\\ngeneration from pre-trained language models. In Proc. of\\nICLR , 2022.\\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\\nParsing incrementally for constrained auto-regressive de-\\ncoding from language models. In Proc. of EMNLP , 2021.\\nSennrich, R., Haddow, B., and Birch, A. Neural machine\\ntranslation of rare words with subword units. In Proc. of\\nACL, 2016.\\nThompson, K. Regular expression search algorithm. Com-\\nmun. ACM , (6), 1968.\\n10' metadata={'source': './data/2403.06988.pdf', 'page': 9}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nTjong Kim Sang, E. F. Introduction to the CoNLL-2002\\nshared task: Language-independent named entity recog-\\nnition. In COLING-02: The 6th Conference on Natural\\nLanguage Learning 2002 (CoNLL-2002) , 2002.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels. ArXiv preprint , 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C.,\\nChen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,\\nJ., Fu, W., Fuller, B., Gao, C., Goswami, V ., Goyal, N.,\\nHartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas,\\nM., Kerkez, V ., Khabsa, M., Kloumann, I., Korenev, A.,\\nKoura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich,\\nD., Lu, Y ., Mao, Y ., Martinet, X., Mihaylov, T., Mishra, P.,\\nMolybog, I., Nie, Y ., Poulton, A., Reizenstein, J., Rungta,\\nR., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Sub-\\nramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams,\\nA., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan,\\nA., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R.,\\nEdunov, S., and Scialom, T. Llama 2: Open foundation\\nand fine-tuned chat models. ArXiv preprint , 2023b.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. ArXiv preprint , 2023c.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\\ntion is all you need. In Advances in Neural Information\\nProcessing Systems 30: Annual Conference on Neural In-\\nformation Processing Systems 2017, December 4-9, 2017,\\nLong Beach, CA, USA , 2017.\\nWillard, B. T. and Louf, R. Efficient guided generation for\\nlarge language models. ArXiv preprint , 2023.\\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,\\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\\net al. Huggingface’s transformers: State-of-the-art natural\\nlanguage processing. ArXiv preprint , 2019.\\n11' metadata={'source': './data/2403.06988.pdf', 'page': 10}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\nA. Whitespace-Flexible GUIDANCE Programs\\nIn our experiments, we differentiate standard GUIDANCE\\nprograms based on templates and whitespace-flexible GUID -\\nANCEWSprograms.\\nTo demonstrate, consider the following example of a simple\\ntemplate-based GUIDANCE program:\\n1f\"\"\"{{\\n2 \"id\": {gen(\\'id\\', regex=\\'[1-9][0-9] *\\')},\\n3 \"description\": \"A nimble fighter\",\\n4 \"name\": \"{gen(\\'name\\', stop=\\'\"\\')}\",\\n5 \"age\": {gen(\\'age\\', regex=\\'[1-9][0-9] *\\')},\\n6 \"armor\": \"{select([\\'leather\\', \\'chainmail\\', \\'plate\\'])}\",\\n7 \"weapon\": \"{select([\\'sword\\', \\'axe\\', \\'bow\\'])}\",\\n8 \"class\": \"{gen(\\'class\\', stop=\\'\"\\')}\",\\n9 \"mantra\": \"{gen(\\'mantra\\', stop=\\'\"\\')}\",\\n10 \"strength\": {gen(\\'strength\\', regex=\\'[1-9][0-9] *\\')},\\n11 \"items\": [ \"{gen(\\'item\\', stop=\\'\"\\')}\", \"{gen(\\'item\\', stop=\\'\"\\'\\n)}\", \"{gen(\\'item\\', stop=\\'\"\\')}\" ],\\n12}}\"\"\"\\nListing 1. A standard JSON GUIDANCE program.\\nHere, we provide a fixed high-level template, with respect\\nto whitespace and formatting. Only the values of the fields\\nare generated by the LLM.\\nIn contrast, a whitespace-flexible GUIDANCEWSprogram\\nfor the same task would look as follows:\\n1nl = \"\\\\n\"\\n2WS = token _limit(zero _or_more(select([\\' \\', nl])), 16)\\n3\\n4f\"\"\"{{{WS}\"id\"{WS}:{WS}{gen(\\'id\\', regex=\\'[1-9][0-9] *\\')}{WS},{WS}\"\\ndescription\":{WS}\"A nimble fighter\"{WS},{WS}\"name\"{WS}:{WS}\"\\n{gen(\\'name\\', stop=\\'\"\\')}\"{WS},{WS}\"age\"{WS}:{WS}{gen(\\'age\\',\\nregex=\\'[1-9][0-9] *\\')}{WS},{WS}\"armor\"{WS}:{WS}\"{select([\\'\\nleather\\', \\'chainmail\\', \\'plate\\'])}\"{WS},{WS}\"weapon\"{WS}:{WS}\\n\"{select([\\'sword\\', \\'axe\\', \\'bow\\'])}\"{WS},{WS}\" class\":{WS}\"{\\ngen(\\'class\\', stop=\\'\"\\')}\"{WS},{WS}\"mantra\"{WS}:{WS}\"{gen(\\'\\nmantra\\', stop=\\'\"\\')}\"{WS},{WS}\"strength\"{WS}:{WS}{gen(\\'\\nstrength\\', regex=\\'[1-9][0-9] *\\')}{WS},{WS}\"items\":{WS}[{WS}\"{\\ngen(\\'item\\', stop=\\'\"\\')}\"{WS},{WS}\"{gen(\\'item\\', stop=\\'\"\\')}\"{WS\\n},{WS}\"{gen(\\'item\\', stop=\\'\"\\')}\"{WS}]{WS},{WS}}}\"\"\"\\nListing 2. A whitespace-flexible JSON GUIDANCE program.\\nAs shown in the snippet, all explicit templated whitespace\\nis replaced by a {WS} token, using the zero_or_more operator.\\nUsing this approach, all whitespace is now also generated\\nby the LLM, allowing for more flexible formatting of the\\noutput, and less explicit constraints on the LLM.\\nOur experiments in §4 demonstrate that the whitespace-\\nflexible formulation leads to higher task accuracy but also\\nsignificantly higher inference time. This is because the pro-\\ngram leaves more freedom to the LLM on how to concretely\\ngenerate the output. At the same time however, inference\\nbecomes less efficient, as the LLM now also has to generate\\nall whitespace tokens explicitly and the GUIDANCE runtime\\ncannot skip over as many tokens as before.Algorithm 3 Model-Based Retokenization\\nInput: LLM f, Prompt x, Target Text s\\nOutput: f-preferred tokenization of s\\n1:o←[]\\n2:while s̸=∅do\\n3:v←f(x+o) // compute logits\\n4: t←arg max {v[t]|t∈ V ∧ tprefix of s}\\n5: o.append (t)\\n6: s←s[|o|:] // remove prefix tfrom s\\n7:end while\\n8:return o\\nB. Model-Based Retokenization\\nTo demonstrate differences between template-based and un-\\nconstrained generation, we consider the task of naturalizing\\na given text under a model-preferred tokenization. This is\\nthe process of converting text to the tokenization a model\\nwould have chosen to represent the same text during genera-\\ntion, when previously conditioned on some prompt. More\\nspecifically, given a target text s, a tokenized prompt x,\\nand a model fwith vocabulary V, we re-encode tusing to-\\nkens from V, such that we greedily maximize the sequence\\nlikelihood assigned by f.\\nWe refer to this process as retokenization . We provide the\\nprocedure for this in Algorithm 3. By greedily choosing the\\nhighest likelihood token that aligns with the target text, we\\nobtain a tokenization of s, that is consistent with the model’s\\npreference, when forced to generate sfrom x. This corre-\\nsponds to applying arg max decoding to f, where the token\\ndistribution of fis always masked such that it produces the\\ntarget text s. Put differently, if a model was to generate s' metadata={'source': './data/2403.06988.pdf', 'page': 11}\n",
            "page_content='sponds to applying arg max decoding to f, where the token\\ndistribution of fis always masked such that it produces the\\ntarget text s. Put differently, if a model was to generate s\\nwhen conditioned on x, it would have produced the token\\nsequence ounder arg max decoding.\\nWhile retokenization allows us to recover the model-\\npreferred tokenization of a given text, template-based con-\\nstrained generation methods cannot benefit from this, as its\\ncomputational overhead is equivalent to the cost of gener-\\nating all templated tokens from scratch, thereby negating\\nthe benefits of using a template-based approach in the first\\nplace.\\nC. Grammars And Prompts\\nBelow we include the grammars and prompts used for each\\nconstraining task from our experiments in Section 4:\\n1root ::= object\\n2value ::= object | array | string | number |\\n3 (\"true\" | \"false\" | \"null\") ws\\n4\\n5object ::=\\n6\"{\" ws (\\n7 string \":\" ws value\\n8(\",\" ws string \":\" ws value) *\\n9)? \"}\" ws\\n12' metadata={'source': './data/2403.06988.pdf', 'page': 11}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\n10\\n11array ::=\\n12\"[\" ws (\\n13 value\\n14 (\",\" ws value) *\\n15)? \"]\" ws\\n16\\n17string ::=\\n18 \"\\\\\"\" (\\n19 [^\"\\\\\\\\] |\\n20 \"\\\\ \\\\\" ([\"\\\\ \\\\/bfnrt] |\\n21 \"u\" [0-9a-fA-F]\\n22 [0-9a-fA-F]\\n23 [0-9a-fA-F]\\n24 [0-9a-fA-F]) # escapes\\n25)*\"\\\\\"\" ws\\n26\\n27number ::= (\"-\"? ([0-9] |\\n28 [1-9] [0-9] *))\\n29 (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\\n30\\n31ws ::= ([ \\\\ \\\\ t\\\\ \\\\ n] ws)?\\n32\\n33#Prompts used forgeneration\\n34\"A JSON file describing a person:\"\\n35\"A JSON file of a person John Smith:\"\\n36\"A JSON file of a person John Smith with friends\"\\n37\"JSON of a person Jane Doe with friends\"\\n38\"A JSON person:\"\\nListing 3. Basic JSON Grammar\\n1root ::= object\\n2value ::= object | array | string | number | (\"true\" | \"false\" |\\n\"null\") ws\\n3\\n4object ::=\\n5ws \"{\" ws (\\n6 \"\\\\\"thoughts\\\\\"\" \":\" ws \"[\" ws thought (ws \",\" ws thought) *\\n\"]\" ws \",\" ws\\n7 \"\\\\\"answer\\\\\"\" \":\" ws number ws\\n8) \"}\" ws\\n9\\n10thought ::=\\n11 \"{\" ws (\\n12 \"\\\\\"step\\\\\"\" \":\" ws string \",\" ws\\n13 \"\\\\\"calculation\\\\\"\" \":\" ws string \",\" ws\\n14 \"\\\\\"result\\\\\"\" \":\" ws number\\n15 ) \"}\" ws\\n16\\n17array ::=\\n18 \"[\" ws (\\n19 value\\n20 (\",\" ws value) *\\n21 )? \"]\" ws\\n22\\n23string ::=\\n24 \"\\\\\"\" (\\n25 [^\"\\\\\\\\] |\\n26 \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]\\n[0-9a-fA-F]) # escapes\\n27 )*\"\\\\\"\" ws\\n28\\n29number ::= (\"-\"? ([0-9] | [1-9] [0-9] *)) (\".\" [0-9]+)? ([eE] [-+]?\\n[0-9]+)? ws\\n30\\n31#Optional space: by convention, applied in thisgrammar after\\nliteral chars when allowed\\n32ws ::= ([ \\\\t\\\\n] ws)?\\n33\\n34#Prompts used forgeneration\\n35We use 5-shot prompts forquestions from GSM8K\\'s test split as\\nprompt (cf. Task Accuracy Experiments).\\nListing 4. Guided Math Reasoning Grammar (for GSM8K)\\n1root ::= (declaration) *\\n23declaration ::= dataType identifier ws \"(\" ws parameter? ws \")\" ws\\n\"{\" ws statement *\"}\"\\n4\\n5dataType ::= \"int\" ws | \"float\" ws | \"char\" ws\\n6identifier ::= [a-zA-Z _] [a-zA-Z _0-9]*\\n7\\n8parameter ::= dataType identifier\\n9\\n10statement ::=\\n11 ( dataType identifier ws \"=\" ws expression \";\" ws ) |\\n12 ( ( dataType identifier ws \"[\" ws expression ws \"]\" ws ( \"=\"\\nws expression )? \";\" ws ) ) |\\n13 ( identifier ws \"=\" ws expression \";\" ws ) |\\n14 ( identifier ws \"(\" argList? \")\" \";\" ws) |\\n15 ( \"return\" ws expression \";\" ws ) |\\n16 ( \"while\" \"(\" condition \")\" ws \"{\" statement *\"}\" ) |\\n17 ( \"for\" \"(\" forInit \";\" ws condition \";\" ws forUpdate \")\" \"{\"\\nstatement *\"}\" ws ) |\\n18 ( \"if\" \"(\" condition \")\" \"{\" statement *\"}\" (\"else\" \"{\"\\nstatement *\"}\")? ws ) |\\n19 ( singleLineComment ws ) |\\n20 ( multiLineComment ws )\\n21\\n22forInit ::= dataType identifier ws \"=\" ws expression | identifier\\nws \"=\" ws expression\\n23forUpdate ::= identifier ws \"=\" ws expression\\n24\\n25condition ::= expression relationOperator expression\\n26relationOperator ::= (\"<=\" | \"<\" | \"==\" | \"!=\" | \">=\" | \">\")\\n27\\n28expression ::= term ((\"+\" | \"-\") term) *\\n29term ::= factor((\" *\" | \"/\") factor) *\\n30\\n31string ::=\\n32 \"\\\\\"\" (\\n33 [^\"\\\\\\\\] |\\n34 \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]\\n[0-9a-fA-F]) # escapes\\n35 )*\"\\\\\"\" ws\\n36\\n37factor ::= identifier | number | unaryTerm | funcCall |\\nparenExpression | subscript | string\\n38unaryTerm ::= \"-\" factor\\n39funcCall ::= identifier \"(\" argList? \")\"\\n40parenExpression ::= \"(\" ws expression ws \")\"\\n41subscript ::= identifier \"[\" ws expression ws \"]\"\\n42\\n43argList ::= expression (\",\" ws expression) *\\n44\\n45number ::= [0-9]+\\n46\\n47singleLineComment ::= \"//\" [^\\\\n] *\"\\\\n\"\\n48multiLineComment ::= \"/ *\" ( [^*] | (\"*\" [^/]) ) *\"*/\"\\n49\\n50ws ::= ([ \\\\t\\\\n] *)\\n51\\n52#prompts used forgeneration\\n53\"A C program that prints \\\\\"Hello, world!\\\\\":\\\\n ```c\\\\n\"\\n54\"A C main function that iterates over an array of integers and\\nprints each one:\\\\n ```c\\\\n\"\\n55\"A C program that prints the sum of two integers:\\\\n ```c\\\\n\"\\n56\"The following is a program that finds the sum of two integers in\\nC:\\\\n```c\\\\n\"\\n57\"A C program that fills an array with the numbers 0 to 9 and\\nprints them:\\\\n ```c\\\\n\"\\n58\"A C implementation of a simple bubble sort:\\\\n ```c\\\\n\"\\nListing 5. Simple C Program Grammar and Prompts\\n1root ::= person\\n2' metadata={'source': './data/2403.06988.pdf', 'page': 12}\n",
            "page_content='prints them:\\\\n ```c\\\\n\"\\n58\"A C implementation of a simple bubble sort:\\\\n ```c\\\\n\"\\nListing 5. Simple C Program Grammar and Prompts\\n1root ::= person\\n2\\n3person ::= ( \"<person>\" ( ws personattributes ) \"</person>\" )\\n4personattributes ::= nameattribute ageattribute jobattribute\\nfriends?\\n5\\n6nameattribute ::= \"<name>\" NAME \"</name>\" ws\\n7ageattribute ::= \"<age>\" NUMBER \"</age>\" ws\\n8jobattribute ::= \"<job>\" ws jobinfo \"</job>\" ws\\n9friends ::= \"<friends>\" ws person+ ws \"</friends>\" ws\\n10\\n13' metadata={'source': './data/2403.06988.pdf', 'page': 12}\n",
            "page_content='Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\\n11jobinfo ::= jobtitle jobsalary\\n12jobtitle ::= \"<title>\" NAME \"</title>\" ws\\n13jobsalary ::= \"<salary>\" NUMBER \"</salary>\" ws\\n14\\n15NAME ::= ( [^<] )+\\n16NUMBER ::= ( [^<] )+\\n17\\n18#Optional space: by convention, applied in thisgrammar after\\nliteral chars when allowed\\n19ws ::= ([ \\\\t\\\\n] ws)?\\n20\\n21#prompts used forgeneration\\n22\"An XML file describing a person:\"\\n23\"An XML file of a person John Smith:\"\\n24\"An XML file of a person John Smith with friends\"\\n25\"XML of a person Jane Doe with friends\"\\n26\"An XML person:\"\\nListing 6. XML (with schema) Grammar and Prompts\\n1start: dict\\n2\\n3dict: \"{\" content \"}\"\\n4\\n5content: id _pair \",\" description _pair \",\" name _pair \",\" age _pair \"\\n,\" armor _pair \",\" weapon _pair \",\" class _pair \",\" mantra _pair\\n\",\" strength _pair \",\" items _pair\\n6\\n7id_pair: \"\\\\\"id\\\\\"\" \":\" NUMBER\\n8description _pair: \"\\\\\"description\\\\\"\" \":\" \"\\\\\"A nimble fighter\\\\\"\"\\n9name_pair: \"\\\\\"name\\\\\"\" \":\" STRING\\n10age_pair: \"\\\\\"age\\\\\"\" \":\" NUMBER\\n11armor_pair: \"\\\\\"armor\\\\\"\" \":\" ((\"\\\\\"leather\\\\\"\") | (\"\\\\\"chainmail\\\\\"\") |\\n(\"\\\\\"plate\\\\\"\"))\\n12weapon_pair: \"\\\\\"weapon\\\\\"\" \":\" ((\"\\\\\"sword\\\\\"\") | (\"\\\\\"axe\\\\\"\") | (\"\\\\\"\\nbow\\\\\"\"))\\n13class_pair: \"\\\\\"class\\\\\"\" \":\" STRING\\n14mantra_pair: \"\\\\\"mantra\\\\\"\" \":\" STRING\\n15strength _pair: \"\\\\\"strength\\\\\"\" \":\" NUMBER\\n16items_pair: \"\\\\\"items\\\\\"\" \":\" \"[\" item \",\" item \",\" item \"]\"\\n17\\n18item: STRING\\n19\\n20STRING: /\"[^\\\\n\\\\r\"]+\"/\\n21NUMBER: /[0-9]+/\\n22\\n23WS: /[ \\\\t\\\\n]+/\\n24\\n25# prompts used for generation\\n26\"The following is a character profile foran RPG game in JSON\\nformat.\\\\n ```json\\\\n\",\\n27\"A character profile foran RPG game:\\\\n ```json\\\\n\",\\n28\"A character profile foran RPG game in JSON format:\\\\n ```json\\\\n\",\\n29\"A character that is a level 5 human fighter with 10 strength, 10\\ndexterity, 10 constitution, 10 intelligence, 10 wisdom, and\\n10 charisma:\\\\n ```json\\\\n\",\\n30\"JSON specifying a character that is a level 5 dwarf fighter from\\na game:\\\\n ```json\\\\n\"\\nListing 7. Fixed Template Grammar and Prompts\\nD. Structured Reasoning Outputs\\nIn our experiments, we evaluate task accuracy on GSM8K\\nand CoNLL2003. For these tasks, prompted and constrained\\nmodel output looks as follows:\\n1{\\n2\"thoughts\": [\\n3 {\\n4 \"step\": \"Find the distance between the first and\\nsecond stops\",\\n5 \"calculation\": \"60 - 20 - 15\",\\n6 \"result\": 25\\n7 },\\n8 {9 \"step\": \"Find the distance between the first and\\nsecond stops\",\\n10 \"calculation\": \"25 + 15\",\\n11 \"result\": 40\\n12 }\\n13 ],\\n14 \"answer\": 40\\n15}\\nListing 8. Structured Reasoning Output for GSM8K\\n1{\\n2\"tokens\": [\\n3 {\\n4 \"token\": \"Nadim\",\\n5 \"tag\": \"B-PER\"\\n6 },\\n7 {\\n8 \"token\": \"Ladki\",\\n9 \"tag\": \"I-PER\"\\n10 }\\n11 ]\\n12}\\nListing 9. Structured Reasoning Output for CoNLL2003\\nIn practice, such outputs greatly facilitate downstream pro-\\ncessing of LLM outputs, as they are already in a structured\\nformat and can be easily parsed.\\nFew-Shot Demonstrations For few-shot demonstrations,\\nwe alternate between questions and answers using a simple\\nQ: ... \\\\n A: ... \\\\n ... format.\\n14' metadata={'source': './data/2403.06988.pdf', 'page': 13}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def find_pdfs(directory):\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".pdf\"):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "articles = find_pdfs(\".\")\n",
        "\n",
        "print(\"Saving vectors to \",env[\"COLLECTION_NAME\"] )\n",
        "\n",
        "for i, article in enumerate(articles):\n",
        "    print(\"Processing \", article)\n",
        "    loader = PyPDFLoader(article)\n",
        "    pages = loader.load_and_split()\n",
        "    s = str(await memory._embeddings_generator.generate_embeddings(\"bbbbbb\"))\n",
        "    print(s)\n",
        "    for j, page in enumerate(pages):\n",
        "        print(page)\n",
        "        await memory.save_information(\n",
        "                        id= \"{0}-{1}\".format(i,j),\n",
        "                        collection=env[\"COLLECTION_NAME\"],\n",
        "                        text=page.page_content\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr3P2IF2xjUc"
      },
      "source": [
        "#### Chat functionality from OpenAI chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c9liDhh3H-QA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwriting function \"ChatGPTFunc2\" in collection\n"
          ]
        }
      ],
      "source": [
        "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
        "from semantic_kernel.prompt_template.input_variable import InputVariable\n",
        "\n",
        "# Simple Prompt\n",
        "prompt = \"\"\"\n",
        "    You a computer scientist researcher summarizing articles recently publsihed. Use only the articles described on the article context to answer questions in a summary form\n",
        "    article context: {{$db_content}}\n",
        "    \n",
        "    Also, answer the question in Pirate English\n",
        "    \n",
        "    Chatbot:\"\"\"\n",
        "\n",
        "# configure chat settings for chat_completion model\n",
        "execution_settings = sk_oai.OpenAITextPromptExecutionSettings(\n",
        "   service_id=\"completion\",\n",
        "    ai_model_id=CHAT_COMPLETION_MODEL,\n",
        "    max_tokens=500,\n",
        "    temperature=0.0,\n",
        "    top_p=0.5\n",
        ")\n",
        "\n",
        "chat_prompt_template_config = sk.PromptTemplateConfig(\n",
        "    template=prompt,\n",
        "    name=\"grounded_response\",\n",
        "    template_format=\"semantic-kernel\",\n",
        "    input_variables=[\n",
        "        InputVariable(name=\"db_content\", description=\"database content\", is_required=True),\n",
        "        InputVariable(name=\"users_query\", description=\"user input\", is_required=True),\n",
        "    ],\n",
        "    execution_settings=execution_settings\n",
        ")\n",
        "\n",
        "chat = kernel.create_function_from_prompt(\n",
        " prompt=prompt,\n",
        " function_name= \"ChatGPTFunc2\", plugin_name=\"chatGPTPlugin2\", prompt_template_config=chat_prompt_template_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukCPNN24yUBJ"
      },
      "source": [
        "### Execute RAG flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "En_ZOyTfICKi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arrr matey! The article be discussin' a phase autoencoder for limit-cycle oscillators, which be a machine-learnin' method that can evaluate the asymptotic phase and phase sensitivity function of an oscillator only from time-series data. This autoencoder can also reconstruct the oscillator state from the phase value. It be useful for synchronizin' two oscillators without needin' their mathematical models. Spontaneous rhythmic phenomena, like heartbeats and brain waves, can be modeled as limit-cycle oscillators, and this phase autoencoder helps understand their synchronization dynamics. Aye, it be a handy tool for data-driven control of limit-cycle oscillators!\n"
          ]
        }
      ],
      "source": [
        "# query whatever you want and it respond with as per given context\n",
        "users_query = \"summarize the Phase autoencoder for limit-cycle oscillators is in simple terms\"\n",
        "\n",
        "result = await memory.search(env[\"COLLECTION_NAME\"], users_query);\n",
        "completions_result = await kernel.invoke(chat, sk.KernelArguments(users_query=users_query, db_content=result[0].text))\n",
        "print(completions_result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BaPoDGlGW4JJ",
        "5G6RH5I9pWQi",
        "YCut_zKcyvGt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
